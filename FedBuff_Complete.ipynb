{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FedBuff: Complete Self-Contained Notebook\n",
        "\n",
        "This notebook contains all code needed to run FedBuff experiments:\n",
        "- Library installation\n",
        "- Data downloading and loading\n",
        "- All FedBuff implementation\n",
        "- Logging and checkpointing\n",
        "\n",
        "**Run cells sequentially from top to bottom.**\n",
        "\n",
        "## Google Colab Setup\n",
        "This notebook is configured to save all results (logs, checkpoints, models) to Google Drive.\n",
        "Data will be downloaded locally for faster access.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Google Colab Setup (Mount Drive)\n",
        "\n",
        "**Skip this section if running locally.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colab: Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Define the target directory for saving results\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/colab/dml_project\"\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    print(f\"âœ… Google Drive mounted\")\n",
        "    print(f\"âœ… Output directory set to: {OUTPUT_DIR}\")\n",
        "    print(f\"ðŸ“ All logs, checkpoints, and results will be saved to Google Drive\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    OUTPUT_DIR = None\n",
        "    print(\"âš ï¸  Not running in Google Colab - using local paths\")\n",
        "\n",
        "# Install required packages\n",
        "%pip install torch torchvision pytorch-lightning pyyaml numpy matplotlib pandas -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Silence libraries\n",
        "import os\n",
        "import logging, warnings\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
        "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "os.environ[\"LIGHTNING_DISABLE_RICH\"] = \"1\"\n",
        "for name in [\n",
        "    \"pytorch_lightning\", \"lightning\", \"lightning.pytorch\",\n",
        "    \"lightning_fabric\", \"lightning_utilities\", \"torch\", \"torchvision\",\n",
        "]:\n",
        "    logging.getLogger(name).setLevel(logging.ERROR)\n",
        "    logging.getLogger(name).propagate = False\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Core imports\n",
        "import time\n",
        "import csv\n",
        "import threading\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup paths based on environment\n",
        "if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
        "    # Google Colab: Save everything to Drive except data\n",
        "    BASE_OUTPUT_DIR = Path(OUTPUT_DIR)\n",
        "    DATA_DIR = Path(\"./data\")  # Local for faster access\n",
        "    print(f\"âœ… Google Colab mode: Results â†’ {BASE_OUTPUT_DIR}\")\n",
        "    print(f\"âœ… Data directory: {DATA_DIR} (local)\")\n",
        "else:\n",
        "    # Local execution: Use current directory\n",
        "    BASE_OUTPUT_DIR = Path(\".\")\n",
        "    DATA_DIR = Path(\"./data\")\n",
        "    print(f\"âœ… Local mode: Results â†’ {BASE_OUTPUT_DIR}\")\n",
        "\n",
        "print(\"âœ… Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Helper Functions ==========\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"Seed all RNGs for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Return the first available computation device (CUDA/MPS/CPU).\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def _device_to_accelerator(device: torch.device) -> str:\n",
        "    \"\"\"Convert torch device to PyTorch Lightning accelerator string.\"\"\"\n",
        "    if device.type == \"cuda\":\n",
        "        return \"gpu\"\n",
        "    if device.type == \"mps\":\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "print(\"âœ… Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Model Utilities ==========\n",
        "\n",
        "def build_resnet18(num_classes: int = 10, pretrained: bool = False) -> nn.Module:\n",
        "    \"\"\"Create ResNet-18 adapted for CIFAR-10.\"\"\"\n",
        "    if pretrained:\n",
        "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        m = models.resnet18(weights=None)\n",
        "    # CIFAR-10: 32x32 -> use 3x3 conv, stride 1, no maxpool\n",
        "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    m.maxpool = nn.Identity()\n",
        "    # Replace classifier\n",
        "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "    m.num_classes = num_classes\n",
        "    return m\n",
        "\n",
        "\n",
        "def state_to_list(state: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
        "    \"\"\"Flatten a state_dict to a list of tensors on CPU.\"\"\"\n",
        "    return [t.detach().cpu().clone() for _, t in state.items()]\n",
        "\n",
        "\n",
        "def list_to_state(template: Dict[str, torch.Tensor], arrs: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Rebuild a state_dict from a list of tensors using a template.\"\"\"\n",
        "    out: Dict[str, torch.Tensor] = {}\n",
        "    for (k, v), a in zip(template.items(), arrs):\n",
        "        out[k] = a.to(v.device).type_as(v)\n",
        "    return out\n",
        "\n",
        "print(\"âœ… Model utilities defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Data Loading and Partitioning ==========\n",
        "\n",
        "class DataDistributor:\n",
        "    \"\"\"Data distributor for federated learning with Dirichlet partitioning.\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_name: str, data_dir: str = \"./data\"):\n",
        "        self.dataset_name = dataset_name.lower()\n",
        "        self.data_dir = data_dir\n",
        "        self.train_dataset, self.test_dataset, self.num_classes = self._load_dataset()\n",
        "        self.partitions = None\n",
        "\n",
        "    def _load_dataset(self) -> Tuple[Any, Any, int]:\n",
        "        \"\"\"Load CIFAR-10 dataset with augmentation.\"\"\"\n",
        "        if self.dataset_name == \"cifar10\":\n",
        "            # Strong data pipeline: augmentation for train, normalization for test\n",
        "            transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=(0.4914, 0.4822, 0.4465),\n",
        "                    std=(0.2470, 0.2435, 0.2616),\n",
        "                ),\n",
        "            ])\n",
        "            transform_test = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=(0.4914, 0.4822, 0.4465),\n",
        "                    std=(0.2470, 0.2435, 0.2616),\n",
        "                ),\n",
        "            ])\n",
        "            train = datasets.CIFAR10(self.data_dir, train=True, download=True, transform=transform_train)\n",
        "            test = datasets.CIFAR10(self.data_dir, train=False, download=True, transform=transform_test)\n",
        "            num_classes = 10\n",
        "        else:\n",
        "            raise ValueError(f\"Dataset '{self.dataset_name}' not supported. Use 'cifar10'.\")\n",
        "        return train, test, num_classes\n",
        "\n",
        "    def distribute_data(self, num_clients: int, alpha: float = 0.5, seed: int = 42):\n",
        "        \"\"\"Partition data using Dirichlet distribution.\"\"\"\n",
        "        np.random.seed(seed)\n",
        "        targets = np.array(self.train_dataset.targets)\n",
        "        self.partitions = {i: [] for i in range(num_clients)}\n",
        "\n",
        "        for cls in range(self.num_classes):\n",
        "            idxs = np.where(targets == cls)[0]\n",
        "            np.random.shuffle(idxs)\n",
        "            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n",
        "            proportions = np.array([p * len(idxs) for p in proportions]).astype(int)\n",
        "\n",
        "            start = 0\n",
        "            for client_id, size in enumerate(proportions):\n",
        "                self.partitions[client_id].extend(idxs[start:start + size])\n",
        "                start += size\n",
        "\n",
        "        for cid in self.partitions:\n",
        "            np.random.shuffle(self.partitions[cid])\n",
        "\n",
        "    def get_client_data(self, client_id: int) -> Subset:\n",
        "        \"\"\"Get data subset for a specific client.\"\"\"\n",
        "        if self.partitions is None:\n",
        "            raise ValueError(\"Data not distributed yet. Call distribute_data() first.\")\n",
        "        indices = self.partitions[client_id]\n",
        "        return Subset(self.train_dataset, indices)\n",
        "\n",
        "print(\"âœ… DataDistributor class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. FedBuff Client Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== FedBuff Client ==========\n",
        "\n",
        "def _testloader(root: str, batch_size: int = 256):\n",
        "    \"\"\"Create test dataloader.\"\"\"\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "def _evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = crit(logits, y)\n",
        "            loss_sum += float(loss.item()) * y.size(0)\n",
        "            total += y.size(0)\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "    return loss_sum / max(1, total), correct / max(1, total)\n",
        "\n",
        "\n",
        "class LitCifar(pl.LightningModule):\n",
        "    \"\"\"PyTorch Lightning module for CIFAR-10 training.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_model: nn.Module, lr: float = 1e-3, momentum: float = 0.9, weight_decay: float = 5e-4):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=[\"base_model\"])\n",
        "        self.model = base_model\n",
        "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "        # Store optimizer params directly\n",
        "        self._optimizer_lr = lr\n",
        "        self._optimizer_momentum = momentum\n",
        "        self._optimizer_weight_decay = weight_decay\n",
        "        self._train_loss_sum = 0.0\n",
        "        self._train_correct = 0\n",
        "        self._train_total = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, _batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        pred = logits.argmax(1)\n",
        "        self._train_loss_sum += float(loss.item()) * y.size(0)\n",
        "        self._train_correct += (pred == y).sum().item()\n",
        "        self._train_total += y.size(0)\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        self._train_loss_sum = 0.0\n",
        "        self._train_correct = 0\n",
        "        self._train_total = 0\n",
        "\n",
        "    def get_epoch_metrics(self) -> Tuple[float, float]:\n",
        "        if self._train_total == 0:\n",
        "            return 0.0, 0.0\n",
        "        return self._train_loss_sum / self._train_total, self._train_correct / self._train_total\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=self._optimizer_lr,\n",
        "            momentum=self._optimizer_momentum,\n",
        "            weight_decay=self._optimizer_weight_decay\n",
        "        )\n",
        "\n",
        "\n",
        "class LocalBuffClient:\n",
        "    \"\"\"FedBuff client that trains locally and submits updates to server.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        cfg: dict,\n",
        "        subset: Subset,\n",
        "        work_dir: str = \"./checkpoints/clients\",\n",
        "        base_delay: float = 0.0,\n",
        "        slow: bool = False,\n",
        "        delay_ranges: Optional[tuple] = None,\n",
        "        jitter: float = 0.0,\n",
        "        fix_delay: bool = True,\n",
        "    ):\n",
        "        self.cid = cid\n",
        "        self.cfg = cfg\n",
        "        self.device = get_device()\n",
        "\n",
        "        base = build_resnet18(num_classes=cfg[\"data\"][\"num_classes\"], pretrained=False)\n",
        "        lr = float(cfg[\"clients\"][\"lr\"])\n",
        "        momentum = float(cfg[\"clients\"].get(\"momentum\", 0.9))\n",
        "        weight_decay = float(cfg[\"clients\"].get(\"weight_decay\", 5e-4))\n",
        "        self.lit = LitCifar(base, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "        self.loader = DataLoader(subset, batch_size=int(cfg[\"clients\"][\"batch_size\"]),\n",
        "                                 shuffle=True, num_workers=0)\n",
        "\n",
        "        self.base_delay = float(base_delay)\n",
        "        self.slow = bool(slow)\n",
        "        self.delay_ranges = delay_ranges\n",
        "        self.jitter = float(jitter)\n",
        "        self.fix_delay = bool(fix_delay)\n",
        "\n",
        "        if self.fix_delay and self.delay_ranges is not None:\n",
        "            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n",
        "            if self.slow:\n",
        "                self.base_delay = random.uniform(float(a_s), float(b_s))\n",
        "            else:\n",
        "                self.base_delay = random.uniform(float(a_f), float(b_f))\n",
        "\n",
        "        self.accelerator = _device_to_accelerator(self.device)\n",
        "        self.testloader = _testloader(cfg[\"data\"][\"data_dir\"])\n",
        "\n",
        "    def _to_list(self) -> List[torch.Tensor]:\n",
        "        return state_to_list(self.lit.model.state_dict())\n",
        "\n",
        "    def _from_list(self, arrs: List[torch.Tensor]) -> None:\n",
        "        sd = self.lit.model.state_dict()\n",
        "        new_sd = list_to_state(sd, arrs)\n",
        "        self.lit.model.load_state_dict(new_sd, strict=True)\n",
        "        self.lit.to(self.device)\n",
        "\n",
        "    def _sleep_delay(self):\n",
        "        global_d = float(self.cfg.get(\"server_runtime\", {}).get(\"client_delay\", 0.0))\n",
        "        base = self.base_delay\n",
        "\n",
        "        if not self.fix_delay and self.delay_ranges is not None:\n",
        "            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n",
        "            if self.slow:\n",
        "                base = random.uniform(float(a_s), float(b_s))\n",
        "            else:\n",
        "                base = random.uniform(float(a_f), float(b_f))\n",
        "\n",
        "        jit = random.uniform(-self.jitter, self.jitter) if self.jitter > 0.0 else 0.0\n",
        "        delay = max(0.0, global_d + base + jit)\n",
        "        if delay > 0.0:\n",
        "            time.sleep(delay)\n",
        "\n",
        "    def fit_once(self, server) -> bool:\n",
        "        params, version = server.get_global()\n",
        "        self._from_list(params)\n",
        "\n",
        "        self._sleep_delay()\n",
        "\n",
        "        epochs = int(self.cfg[\"clients\"][\"local_epochs\"])\n",
        "        grad_clip = float(self.cfg[\"clients\"].get(\"grad_clip\", 1.0))\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=epochs,\n",
        "            accelerator=self.accelerator,\n",
        "            devices=1,\n",
        "            enable_checkpointing=False,\n",
        "            logger=False,\n",
        "            enable_model_summary=False,\n",
        "            num_sanity_val_steps=0,\n",
        "            enable_progress_bar=False,\n",
        "            callbacks=[],\n",
        "            gradient_clip_val=grad_clip,\n",
        "            gradient_clip_algorithm=\"norm\",\n",
        "        )\n",
        "        start = time.time()\n",
        "        trainer.fit(self.lit, train_dataloaders=self.loader)\n",
        "        duration = time.time() - start\n",
        "\n",
        "        train_loss, train_acc = self.lit.get_epoch_metrics()\n",
        "        test_loss, test_acc = _evaluate(self.lit.model, self.testloader, self.device)\n",
        "\n",
        "        new_params = self._to_list()\n",
        "        num_examples = len(self.loader.dataset)\n",
        "\n",
        "        server.submit_update(\n",
        "            client_id=self.cid,\n",
        "            base_version=version,\n",
        "            new_params=new_params,\n",
        "            num_samples=num_examples,\n",
        "            train_time_s=duration,\n",
        "            train_loss=train_loss,\n",
        "            train_acc=train_acc,\n",
        "            test_loss=test_loss,\n",
        "            test_acc=test_acc,\n",
        "        )\n",
        "        return not server.should_stop()\n",
        "\n",
        "print(\"âœ… FedBuff client classes defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== FedBuff Server ==========\n",
        "\n",
        "def _testloader_server(root: str, batch_size: int = 256):\n",
        "    \"\"\"Create test dataloader for server evaluation.\"\"\"\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "def _evaluate_server(model: torch.nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss_sum += float(loss.item()) * y.size(0)\n",
        "            total += y.size(0)\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "    return loss_sum / max(1, total), correct / max(1, total)\n",
        "\n",
        "\n",
        "class BufferedFedServer:\n",
        "    \"\"\"FedBuff server that buffers client updates and aggregates them.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        global_model: torch.nn.Module,\n",
        "        total_train_samples: int,\n",
        "        buffer_size: int = 5,\n",
        "        buffer_timeout_s: float = 10.0,\n",
        "        use_sample_weighing: bool = True,\n",
        "        target_accuracy: float = 0.70,\n",
        "        max_rounds: Optional[int] = None,\n",
        "        eval_interval_s: int = 15,\n",
        "        data_dir: str = \"./data\",\n",
        "        checkpoints_dir: str = \"./checkpoints/FedBuff\",\n",
        "        logs_dir: str = \"./logs/FedBuff\",\n",
        "        global_log_csv: Optional[str] = None,\n",
        "        client_participation_csv: Optional[str] = None,\n",
        "        final_model_path: Optional[str] = None,\n",
        "        resume: bool = True,\n",
        "        device: Optional[torch.device] = None,\n",
        "        eta: float = 0.5,\n",
        "    ):\n",
        "        self.model = global_model\n",
        "        self.template = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n",
        "        self.device = device or get_device()\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.total_train_samples = int(total_train_samples)\n",
        "        self.buffer_size = int(buffer_size)\n",
        "        self.buffer_timeout_s = float(buffer_timeout_s)\n",
        "        self.use_sample_weighing = bool(use_sample_weighing)\n",
        "        self.eta = float(eta)\n",
        "\n",
        "        self.eval_interval_s = int(eval_interval_s)\n",
        "        self.target_accuracy = float(target_accuracy)\n",
        "        self.max_rounds = int(max_rounds) if max_rounds is not None else None\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.ckpt_dir = Path(checkpoints_dir)\n",
        "        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.log_dir = Path(logs_dir)\n",
        "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.csv_path = Path(global_log_csv) if global_log_csv else (self.log_dir / \"FedBuff.csv\")\n",
        "        self.participation_csv = Path(client_participation_csv) if client_participation_csv else (self.log_dir / \"FedBuffClientParticipation.csv\")\n",
        "        self.final_model_path = Path(final_model_path) if final_model_path else Path(\"./results/FedBuffModel.pt\")\n",
        "        self.final_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if not self.csv_path.exists():\n",
        "            self.csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with self.csv_path.open(\"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\"total_agg\", \"avg_train_loss\", \"avg_train_acc\",\n",
        "                                        \"test_loss\", \"test_acc\", \"time\"])\n",
        "\n",
        "        if not self.participation_csv.exists():\n",
        "            self.participation_csv.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with self.participation_csv.open(\"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\n",
        "                    \"client_id\", \"local_train_loss\", \"local_train_acc\",\n",
        "                    \"local_test_loss\", \"local_test_acc\", \"total_agg\"\n",
        "                ])\n",
        "\n",
        "        self._lock = threading.Lock()\n",
        "        self._stop = False\n",
        "        self.t_round = 0\n",
        "        self._log_count = 0\n",
        "        self.testloader = _testloader_server(self.data_dir)\n",
        "        self._train_loss_acc_accum: List[Tuple[float, float, int]] = []\n",
        "        self._start_ts = time.time()\n",
        "\n",
        "        self._buffer: List[Dict] = []\n",
        "        self._buffer_last_flush = time.time()\n",
        "\n",
        "        if resume:\n",
        "            self._maybe_resume()\n",
        "\n",
        "    def _ckpt_file(self) -> Path:\n",
        "        return self.ckpt_dir / \"server_last.ckpt\"\n",
        "\n",
        "    def _maybe_resume(self) -> None:\n",
        "        ck = self._ckpt_file()\n",
        "        if ck.exists():\n",
        "            try:\n",
        "                blob = torch.load(ck, map_location=\"cpu\")\n",
        "                state = list_to_state(self.template, blob[\"global_params\"])\n",
        "                self.model.load_state_dict(state, strict=True)\n",
        "                self.t_round = int(blob[\"t_round\"])\n",
        "                print(f\"[resume] Loaded server checkpoint at total_agg={self.t_round}\")\n",
        "            except (RuntimeError, KeyError) as e:\n",
        "                print(f\"[resume] Checkpoint incompatible, starting fresh: {type(e).__name__}\")\n",
        "                ck.unlink()\n",
        "                self.t_round = 0\n",
        "\n",
        "    def _save_ckpt(self) -> None:\n",
        "        sd = state_to_list(self.model.state_dict())\n",
        "        torch.save({\"t_round\": self.t_round, \"global_params\": sd}, self._ckpt_file())\n",
        "\n",
        "    def _save_final_model(self) -> None:\n",
        "        torch.save(self.model.state_dict(), self.final_model_path)\n",
        "\n",
        "    def get_global(self):\n",
        "        with self._lock:\n",
        "            return state_to_list(self.model.state_dict()), self.t_round\n",
        "\n",
        "    def _flush_buffer(self) -> None:\n",
        "        if not self._buffer:\n",
        "            return\n",
        "\n",
        "        g = state_to_list(self.model.state_dict())\n",
        "        total_samples = sum(u[\"num_samples\"] for u in self._buffer)\n",
        "\n",
        "        # Aggregate client models (weighted average)\n",
        "        aggregated = []\n",
        "        for gi in g:\n",
        "            if gi.dtype.is_floating_point:\n",
        "                aggregated.append(torch.zeros_like(gi, device=gi.device, dtype=torch.float32))\n",
        "            else:\n",
        "                aggregated.append(torch.zeros(gi.shape, device=gi.device, dtype=torch.float32))\n",
        "        \n",
        "        for u in self._buffer:\n",
        "            weight = float(u[\"num_samples\"]) / float(total_samples) if self.use_sample_weighing else 1.0 / len(self._buffer)\n",
        "            for i, ci in enumerate(u[\"new_params\"]):\n",
        "                ci_tensor = ci.to(aggregated[i].device)\n",
        "                if ci_tensor.dtype.is_floating_point:\n",
        "                    ci_tensor = ci_tensor.float()\n",
        "                else:\n",
        "                    ci_tensor = ci_tensor.float()\n",
        "                aggregated[i] += weight * ci_tensor\n",
        "        \n",
        "        # Convert back to original dtypes\n",
        "        for i, gi in enumerate(g):\n",
        "            if not gi.dtype.is_floating_point:\n",
        "                aggregated[i] = aggregated[i].round().to(gi.dtype)\n",
        "            else:\n",
        "                aggregated[i] = aggregated[i].to(gi.dtype)\n",
        "\n",
        "        # Update global model: FedAvg-style (eta=1.0) or relaxed update (eta<1.0)\n",
        "        if self.eta >= 1.0:\n",
        "            merged = aggregated\n",
        "        else:\n",
        "            merged = [(1.0 - self.eta) * gi + self.eta * aggregated[i] for i, gi in enumerate(g)]\n",
        "\n",
        "        new_state = list_to_state(self.template, merged)\n",
        "        self.model.load_state_dict(new_state, strict=True)\n",
        "\n",
        "        for u in self._buffer:\n",
        "            self._train_loss_acc_accum.append((u[\"train_loss\"], u[\"train_acc\"], u[\"num_samples\"]))\n",
        "\n",
        "        self._buffer.clear()\n",
        "        self._buffer_last_flush = time.time()\n",
        "        self.t_round += 1\n",
        "        self._save_ckpt()\n",
        "\n",
        "    def submit_update(\n",
        "        self,\n",
        "        client_id: int,\n",
        "        base_version: int,\n",
        "        new_params: List[torch.Tensor],\n",
        "        num_samples: int,\n",
        "        train_time_s: float,\n",
        "        train_loss: float,\n",
        "        train_acc: float,\n",
        "        test_loss: float,\n",
        "        test_acc: float,\n",
        "    ) -> None:\n",
        "        with self._lock:\n",
        "            if self._stop:\n",
        "                return\n",
        "            if self.max_rounds is not None and self.t_round >= self.max_rounds:\n",
        "                self._stop = True\n",
        "                return\n",
        "\n",
        "            with self.participation_csv.open(\"a\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\n",
        "                    client_id, f\"{train_loss:.6f}\", f\"{train_acc:.6f}\",\n",
        "                    f\"{test_loss:.6f}\", f\"{test_acc:.6f}\", self.t_round\n",
        "                ])\n",
        "\n",
        "            self._buffer.append({\n",
        "                \"client_id\": client_id,\n",
        "                \"base_version\": base_version,\n",
        "                \"new_params\": new_params,\n",
        "                \"num_samples\": num_samples,\n",
        "                \"train_loss\": float(train_loss),\n",
        "                \"train_acc\": float(train_acc),\n",
        "                \"test_loss\": float(test_loss),\n",
        "                \"test_acc\": float(test_acc),\n",
        "            })\n",
        "\n",
        "            should_flush = False\n",
        "            if len(self._buffer) >= self.buffer_size:\n",
        "                should_flush = True\n",
        "            elif time.time() - self._buffer_last_flush >= self.buffer_timeout_s:\n",
        "                should_flush = True\n",
        "\n",
        "            if should_flush:\n",
        "                self._flush_buffer()\n",
        "\n",
        "    def should_stop(self) -> bool:\n",
        "        with self._lock:\n",
        "            return self._stop\n",
        "\n",
        "    def mark_stop(self) -> None:\n",
        "        with self._lock:\n",
        "            if self._buffer:\n",
        "                self._flush_buffer()\n",
        "            self._stop = True\n",
        "            self._save_final_model()\n",
        "            print(f\"[LOG] saved final model -> {self.final_model_path}\")\n",
        "\n",
        "    def _compute_avg_train(self) -> Tuple[float, float]:\n",
        "        if not self._train_loss_acc_accum:\n",
        "            return 0.0, 0.0\n",
        "        loss_sum, acc_sum, n_sum = 0.0, 0.0, 0\n",
        "        for l, a, n in self._train_loss_acc_accum:\n",
        "            loss_sum += l * n\n",
        "            acc_sum += a * n\n",
        "            n_sum += n\n",
        "        return loss_sum / max(1, n_sum), acc_sum / max(1, n_sum)\n",
        "\n",
        "    def _periodic_eval_and_log(self):\n",
        "        test_loss, test_acc = _evaluate_server(self.model, self.testloader, self.device)\n",
        "        avg_train_loss, avg_train_acc = self._compute_avg_train()\n",
        "        now = time.time() - self._start_ts\n",
        "        self._train_loss_acc_accum.clear()\n",
        "\n",
        "        with self.csv_path.open(\"a\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow([\n",
        "                self.t_round, f\"{avg_train_loss:.6f}\", f\"{avg_train_acc:.6f}\",\n",
        "                f\"{test_loss:.6f}\", f\"{test_acc:.6f}\", f\"{now:.3f}\"\n",
        "            ])\n",
        "        print(f\"[LOG] total_agg={self.t_round} \"\n",
        "              f\"avg_train_loss={avg_train_loss:.4f} avg_train_acc={avg_train_acc:.4f} \"\n",
        "              f\"test_loss={test_loss:.4f} test_acc={test_acc:.4f} time={now:.1f}s\")\n",
        "\n",
        "        self._log_count += 1\n",
        "        if self._log_count % 100 == 0:\n",
        "            path = self.ckpt_dir / f\"global_log{self._log_count}_t{self.t_round}.pt\"\n",
        "            torch.save(self.model.state_dict(), path)\n",
        "\n",
        "        if test_acc >= self.target_accuracy:\n",
        "            self._stop = True\n",
        "\n",
        "    def start_eval_timer(self):\n",
        "        def _loop():\n",
        "            next_ts = time.time() + self.eval_interval_s\n",
        "            while True:\n",
        "                now = time.time()\n",
        "                sleep_for = max(0.0, next_ts - now)\n",
        "                time.sleep(sleep_for)\n",
        "                with self._lock:\n",
        "                    if self._stop:\n",
        "                        break\n",
        "                    self._periodic_eval_and_log()\n",
        "                next_ts += self.eval_interval_s\n",
        "        threading.Thread(target=_loop, daemon=True).start()\n",
        "\n",
        "    def wait(self):\n",
        "        try:\n",
        "            while not self.should_stop():\n",
        "                time.sleep(0.2)\n",
        "        finally:\n",
        "            self.mark_stop()\n",
        "\n",
        "print(\"âœ… FedBuff server class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== All 6 Experiment Configurations ==========\n",
        "\n",
        "# Helper function to get paths (works for both Colab and local)\n",
        "def get_paths(exp_id: str):\n",
        "    \"\"\"Get paths for experiment, using Google Drive if in Colab.\"\"\"\n",
        "    if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
        "        base = BASE_OUTPUT_DIR\n",
        "    else:\n",
        "        base = Path(\".\")\n",
        "    \n",
        "    return {\n",
        "        \"data_dir\": str(DATA_DIR),  # Always local for faster access\n",
        "        \"checkpoints_dir\": str(base / \"checkpoints\" / \"FedBuff\" / exp_id),\n",
        "        \"logs_dir\": str(base / \"logs\" / \"FedBuff\" / exp_id),\n",
        "        \"results_dir\": str(base / \"results\" / \"FedBuff\" / exp_id),\n",
        "    }\n",
        "\n",
        "experiments = {\n",
        "    \"Exp1\": {\n",
        "        \"name\": \"IID (alpha=1000), no stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),  # Use DATA_DIR variable\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 0,\n",
        "            \"delay_slow_range\": [0.0, 0.0],\n",
        "            \"delay_fast_range\": [0.0, 0.0],\n",
        "            \"jitter_per_round\": 0.0,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"buff\": {\n",
        "            \"buffer_size\": 5,\n",
        "            \"buffer_timeout_s\": 0.0,\n",
        "            \"use_sample_weighing\": True,\n",
        "            \"eta\": 0.5\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 1000.0,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp1\")\n",
        "    },\n",
        "    \"Exp2\": {\n",
        "        \"name\": \"alpha=0.1, 10% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 10,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"buff\": {\n",
        "            \"buffer_size\": 5,\n",
        "            \"buffer_timeout_s\": 0.0,\n",
        "            \"use_sample_weighing\": True,\n",
        "            \"eta\": 0.5\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp2\")\n",
        "    },\n",
        "    \"Exp3\": {\n",
        "        \"name\": \"alpha=0.1, 20% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 20,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"buff\": {\n",
        "            \"buffer_size\": 5,\n",
        "            \"buffer_timeout_s\": 0.0,\n",
        "            \"use_sample_weighing\": True,\n",
        "            \"eta\": 0.5\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp3\")\n",
        "    },\n",
        "    \"Exp4\": {\n",
        "        \"name\": \"alpha=0.1, 30% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 30,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"buff\": {\n",
        "            \"buffer_size\": 5,\n",
        "            \"buffer_timeout_s\": 0.0,\n",
        "            \"use_sample_weighing\": True,\n",
        "            \"eta\": 0.5\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp4\")\n",
        "    },\n",
        "    \"Exp5\": {\n",
        "        \"name\": \"alpha=0.1, 40% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 40,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"buff\": {\n",
        "            \"buffer_size\": 5,\n",
        "            \"buffer_timeout_s\": 0.0,\n",
        "            \"use_sample_weighing\": True,\n",
        "            \"eta\": 0.5\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp5\")\n",
        "    },\n",
        "    \"Exp6\": {\n",
        "        \"name\": \"alpha=0.1, 50% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 50,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"buff\": {\n",
        "            \"buffer_size\": 5,\n",
        "            \"buffer_timeout_s\": 0.0,\n",
        "            \"use_sample_weighing\": True,\n",
        "            \"eta\": 0.5\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp6\")\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"âœ… All 6 experiment configurations loaded:\")\n",
        "for exp_id, exp_config in experiments.items():\n",
        "    print(f\"  {exp_id}: {exp_config['name']}\")\n",
        "    print(f\"    - Alpha: {exp_config['partition_alpha']}\")\n",
        "    print(f\"    - Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
        "    print(f\"    - Max rounds: {exp_config['train']['max_rounds']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Single Experiment (Helper Function)\n",
        "\n",
        "Use this function to run a single experiment by ID (Exp1-Exp6).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_single_experiment(exp_id: str, experiments_dict: dict):\n",
        "    \"\"\"\n",
        "    Run a single FedBuff experiment.\n",
        "    \n",
        "    Args:\n",
        "        exp_id: Experiment ID (e.g., \"Exp1\", \"Exp2\", ..., \"Exp6\")\n",
        "        experiments_dict: Dictionary containing all experiment configs\n",
        "    \"\"\"\n",
        "    if exp_id not in experiments_dict:\n",
        "        print(f\"âŒ Error: Experiment {exp_id} not found!\")\n",
        "        return None\n",
        "    \n",
        "    config = experiments_dict[exp_id]\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Running {exp_id}: {config['name']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Set random seed\n",
        "    seed = int(config.get(\"seed\", 42))\n",
        "    set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    \n",
        "    # Create timestamped run folder\n",
        "    run_dir = Path(config[\"io\"][\"logs_dir\"]) / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Write COMMIT.txt\n",
        "    commit_hash = \"notebook_run\"\n",
        "    csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n",
        "    with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n",
        "        f.write(f\"{commit_hash},{csv_header}\\n\")\n",
        "    \n",
        "    # Save config to run folder\n",
        "    with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n",
        "        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "    \n",
        "    print(f\"âœ… Run folder: {run_dir}\")\n",
        "    \n",
        "    # Load and partition data\n",
        "    dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n",
        "    dd.distribute_data(\n",
        "        num_clients=int(config[\"clients\"][\"total\"]),\n",
        "        alpha=float(config[\"partition_alpha\"]),\n",
        "        seed=seed\n",
        "    )\n",
        "    print(f\"âœ… Data partitioned: {config['clients']['total']} clients, alpha={config['partition_alpha']}\")\n",
        "    \n",
        "    # Build global model\n",
        "    global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n",
        "    \n",
        "    # Initialize server\n",
        "    server = BufferedFedServer(\n",
        "        global_model=global_model,\n",
        "        total_train_samples=len(dd.train_dataset),\n",
        "        buffer_size=int(config[\"buff\"][\"buffer_size\"]),\n",
        "        buffer_timeout_s=float(config[\"buff\"][\"buffer_timeout_s\"]),\n",
        "        use_sample_weighing=bool(config[\"buff\"][\"use_sample_weighing\"]),\n",
        "        target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n",
        "        max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n",
        "        eval_interval_s=int(config[\"eval\"][\"interval_seconds\"]),\n",
        "        data_dir=config[\"data\"][\"data_dir\"],\n",
        "        checkpoints_dir=str(run_dir / \"checkpoints\"),\n",
        "        logs_dir=str(run_dir),\n",
        "        global_log_csv=str(run_dir / \"FedBuff.csv\"),\n",
        "        client_participation_csv=str(run_dir / \"FedBuffClientParticipation.csv\"),\n",
        "        final_model_path=str(run_dir / \"FedBuffModel.pt\"),\n",
        "        resume=False,\n",
        "        device=get_device(),\n",
        "        eta=float(config[\"buff\"].get(\"eta\", 0.5)),\n",
        "    )\n",
        "    print(f\"âœ… Server initialized (device: {server.device})\")\n",
        "    \n",
        "    # Setup clients\n",
        "    n = int(config[\"clients\"][\"total\"])\n",
        "    pct = max(0, min(100, int(config[\"clients\"].get(\"struggle_percent\", 0))))\n",
        "    k_slow = (n * pct) // 100\n",
        "    slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
        "    \n",
        "    a_s, b_s = config[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
        "    a_f, b_f = config[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
        "    fix_delays = bool(config[\"clients\"].get(\"fix_delays_per_client\", True))\n",
        "    jitter = float(config[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
        "    \n",
        "    per_client_base_delay: Dict[int, float] = {}\n",
        "    if fix_delays:\n",
        "        for cid in range(n):\n",
        "            if cid in slow_ids:\n",
        "                per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
        "            else:\n",
        "                per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
        "    \n",
        "    clients = []\n",
        "    for cid in range(n):\n",
        "        subset = dd.get_client_data(cid)\n",
        "        base_delay = per_client_base_delay.get(cid, 0.0)\n",
        "        is_slow = cid in slow_ids\n",
        "        clients.append(LocalBuffClient(\n",
        "            cid=cid,\n",
        "            cfg=config,\n",
        "            subset=subset,\n",
        "            work_dir=str(run_dir / \"checkpoints\" / \"clients\"),\n",
        "            base_delay=base_delay,\n",
        "            slow=is_slow,\n",
        "            delay_ranges=((float(a_s), float(b_s)), (float(a_f), float(b_f))),\n",
        "            jitter=jitter,\n",
        "            fix_delay=fix_delays,\n",
        "        ))\n",
        "    \n",
        "    print(f\"âœ… Created {len(clients)} clients ({len(slow_ids)} slow, {n - len(slow_ids)} fast)\")\n",
        "    \n",
        "    # Start experiment\n",
        "    server.start_eval_timer()\n",
        "    sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n",
        "    \n",
        "    def client_loop(client: LocalBuffClient):\n",
        "        while True:\n",
        "            with sem:\n",
        "                cont = client.fit_once(server)\n",
        "            if not cont:\n",
        "                break\n",
        "            time.sleep(0.05)\n",
        "    \n",
        "    threads = []\n",
        "    for cl in clients:\n",
        "        t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "    \n",
        "    print(f\"âœ… Started {len(threads)} client threads\")\n",
        "    print(f\"ðŸš€ Experiment running... (max {config['train']['max_rounds']} rounds)\")\n",
        "    \n",
        "    # Wait for completion\n",
        "    server.wait()\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "    \n",
        "    print(f\"\\nâœ… {exp_id} completed!\")\n",
        "    print(f\"ðŸ“ Results: {run_dir}\")\n",
        "    \n",
        "    return run_dir\n",
        "\n",
        "print(\"âœ… Helper function `run_single_experiment()` defined\")\n",
        "print(\"   Usage: run_dir = run_single_experiment('Exp1', experiments)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run All 6 Experiments Sequentially\n",
        "\n",
        "This cell runs all experiments one by one. Each experiment saves to its own folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all 6 experiments sequentially\n",
        "experiment_results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING ALL 6 EXPERIMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
        "    exp_start = time.time()\n",
        "    try:\n",
        "        run_dir = run_single_experiment(exp_id, experiments)\n",
        "        if run_dir:\n",
        "            experiment_results[exp_id] = {\n",
        "                \"run_dir\": run_dir,\n",
        "                \"status\": \"completed\",\n",
        "                \"duration_min\": (time.time() - exp_start) / 60.0\n",
        "            }\n",
        "            print(f\"âœ… {exp_id} completed in {experiment_results[exp_id]['duration_min']:.2f} minutes\")\n",
        "        else:\n",
        "            experiment_results[exp_id] = {\"status\": \"failed\", \"duration_min\": (time.time() - exp_start) / 60.0}\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {exp_id} failed: {str(e)}\")\n",
        "        experiment_results[exp_id] = {\"status\": \"error\", \"error\": str(e), \"duration_min\": (time.time() - exp_start) / 60.0}\n",
        "    \n",
        "    print(f\"\\n{'â”€'*70}\\n\")\n",
        "\n",
        "total_duration = (time.time() - total_start) / 60.0\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ALL EXPERIMENTS COMPLETED\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total time: {total_duration:.2f} minutes ({total_duration/60:.2f} hours)\")\n",
        "print(\"\\nResults summary:\")\n",
        "for exp_id, result in experiment_results.items():\n",
        "    if result.get(\"status\") == \"completed\":\n",
        "        print(f\"  {exp_id}: âœ… {result['duration_min']:.2f} min -> {result['run_dir']}\")\n",
        "    else:\n",
        "        print(f\"  {exp_id}: âŒ {result.get('status', 'unknown')}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compare All Experiments (After Running)\n",
        "\n",
        "This section creates comparison plots across all 6 experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all experiments (load results from experiment_results)\n",
        "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
        "    # Load all CSV files\n",
        "    all_data = {}\n",
        "    for exp_id, result in experiment_results.items():\n",
        "        if result.get(\"status\") == \"completed\":\n",
        "            csv_path = result[\"run_dir\"] / \"FedBuff.csv\"\n",
        "            if csv_path.exists():\n",
        "                df = pd.read_csv(csv_path)\n",
        "                df['time_min'] = df['time'] / 60.0\n",
        "                all_data[exp_id] = df\n",
        "                print(f\"âœ… Loaded {exp_id}: {len(df)} rows\")\n",
        "    \n",
        "    if len(all_data) > 0:\n",
        "        # Create comparison plots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "        \n",
        "        # Plot 1: Accuracy vs Rounds\n",
        "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
        "            axes[0, 0].plot(df['total_agg'], df['test_acc'], \n",
        "                          label=f\"{exp_id} ({experiments[exp_id]['name']})\", \n",
        "                          linewidth=2, color=colors[i % len(colors)])\n",
        "        axes[0, 0].set_xlabel('Round', fontsize=12)\n",
        "        axes[0, 0].set_ylabel('Test Accuracy', fontsize=12)\n",
        "        axes[0, 0].set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
        "        axes[0, 0].legend(fontsize=9, loc='lower right')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        axes[0, 0].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Plot 2: Accuracy vs Wall Clock Time\n",
        "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
        "            axes[0, 1].plot(df['time_min'], df['test_acc'], \n",
        "                          label=f\"{exp_id}\", \n",
        "                          linewidth=2, color=colors[i % len(colors)])\n",
        "        axes[0, 1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "        axes[0, 1].set_ylabel('Test Accuracy', fontsize=12)\n",
        "        axes[0, 1].set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
        "        axes[0, 1].legend(fontsize=9, loc='lower right')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        axes[0, 1].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Plot 3: Best Accuracy by Experiment\n",
        "        best_accs = {exp_id: df['test_acc'].max() for exp_id, df in all_data.items()}\n",
        "        exp_names = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\" \n",
        "                    if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\" \n",
        "                    for exp_id in best_accs.keys()]\n",
        "        axes[1, 0].bar(range(len(best_accs)), list(best_accs.values()), \n",
        "                      color=colors[:len(best_accs)], alpha=0.7, edgecolor='black')\n",
        "        axes[1, 0].set_xticks(range(len(best_accs)))\n",
        "        axes[1, 0].set_xticklabels(exp_names, fontsize=9, rotation=45, ha='right')\n",
        "        axes[1, 0].set_ylabel('Best Test Accuracy', fontsize=12)\n",
        "        axes[1, 0].set_title('Best Test Accuracy by Experiment', fontsize=14, fontweight='bold')\n",
        "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "        axes[1, 0].set_ylim([0, 1.0])\n",
        "        # Add value labels\n",
        "        for i, (exp_id, acc) in enumerate(best_accs.items()):\n",
        "            axes[1, 0].text(i, acc, f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        # Plot 4: Time to reach 50% accuracy\n",
        "        times_to_50 = {}\n",
        "        for exp_id, df in all_data.items():\n",
        "            mask = df['test_acc'] >= 0.5\n",
        "            if mask.any():\n",
        "                first_idx = mask.idxmax()\n",
        "                times_to_50[exp_id] = df.loc[first_idx, 'time_min']\n",
        "            else:\n",
        "                times_to_50[exp_id] = None\n",
        "        \n",
        "        valid_times = {k: v for k, v in times_to_50.items() if v is not None}\n",
        "        if len(valid_times) > 0:\n",
        "            exp_names_50 = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\" \n",
        "                           if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\" \n",
        "                           for exp_id in valid_times.keys()]\n",
        "            axes[1, 1].bar(range(len(valid_times)), list(valid_times.values()), \n",
        "                         color=colors[:len(valid_times)], alpha=0.7, edgecolor='black')\n",
        "            axes[1, 1].set_xticks(range(len(valid_times)))\n",
        "            axes[1, 1].set_xticklabels(exp_names_50, fontsize=9, rotation=45, ha='right')\n",
        "            axes[1, 1].set_ylabel('Time to Reach 50% Accuracy (minutes)', fontsize=12)\n",
        "            axes[1, 1].set_title('Convergence Speed: Time to 50% Accuracy', fontsize=14, fontweight='bold')\n",
        "            axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "            # Add value labels\n",
        "            for i, (exp_id, t) in enumerate(valid_times.items()):\n",
        "                axes[1, 1].text(i, t, f'{t:.1f}m', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        # Use Google Drive path if in Colab, otherwise local\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
        "            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"FedBuff\" / \"comparisons\"\n",
        "        else:\n",
        "            comparison_dir = Path(\"./logs/FedBuff/comparisons\")\n",
        "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(comparison_dir / \"all_experiments_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"\\nâœ… Comparison plot saved: {comparison_dir / 'all_experiments_comparison.png'}\")\n",
        "        plt.show()\n",
        "        \n",
        "        # Print summary table\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXPERIMENT COMPARISON SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
        "        print(\"-\"*80)\n",
        "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
        "            if exp_id in all_data:\n",
        "                df = all_data[exp_id]\n",
        "                cfg = experiments[exp_id]\n",
        "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
        "                      f\"{df['test_acc'].max():<10.4f} {df['test_acc'].iloc[-1]:<11.4f} \"\n",
        "                      f\"{df['time_min'].iloc[-1]:<12.2f}\")\n",
        "        print(\"=\"*80)\n",
        "    else:\n",
        "        print(\"âš ï¸  No completed experiments found. Run experiments first.\")\n",
        "else:\n",
        "    print(\"âš ï¸  No experiment results found. Run the experiments first using Section 8.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Download Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download CIFAR-10 dataset\n",
        "# IMPORTANT: Make sure you've run Section 7 first to set the 'config' variable!\n",
        "if 'config' not in globals():\n",
        "    raise NameError(\n",
        "        \"âŒ 'config' is not defined!\\n\"\n",
        "        \"Please run Section 7 first to select an experiment (Exp1-Exp6).\\n\"\n",
        "        \"Or use Section 8 (automated runner) which handles everything automatically.\"\n",
        "    )\n",
        "\n",
        "print(\"Downloading CIFAR-10 dataset...\")\n",
        "dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n",
        "print(f\"âœ… Dataset loaded: {len(dd.train_dataset)} training samples, {len(dd.test_dataset)} test samples\")\n",
        "print(f\"âœ… Number of classes: {dd.num_classes}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select which experiment to run manually (Exp1, Exp2, Exp3, Exp4, Exp5, or Exp6)\n",
        "# This sets the 'config' variable for the manual cells below\n",
        "EXP_ID = \"Exp1\"  # Change this to Exp2, Exp3, etc. to run different experiments\n",
        "\n",
        "if EXP_ID in experiments:\n",
        "    config = experiments[EXP_ID]\n",
        "    print(f\"âœ… Selected experiment: {EXP_ID} - {config['name']}\")\n",
        "    print(f\"   Alpha: {config['partition_alpha']}\")\n",
        "    print(f\"   Stragglers: {config['clients']['struggle_percent']}%\")\n",
        "    print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
        "else:\n",
        "    raise ValueError(f\"Experiment {EXP_ID} not found! Use Exp1-Exp6.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select which experiment to run manually (Exp1, Exp2, Exp3, Exp4, Exp5, or Exp6)\n",
        "# This sets the 'config' variable for the manual cells below\n",
        "EXP_ID = \"Exp1\"  # Change this to Exp2, Exp3, etc. to run different experiments\n",
        "\n",
        "if EXP_ID in experiments:\n",
        "    config = experiments[EXP_ID]\n",
        "    print(f\"âœ… Selected experiment: {EXP_ID} - {config['name']}\")\n",
        "    print(f\"   Alpha: {config['partition_alpha']}\")\n",
        "    print(f\"   Stragglers: {config['clients']['struggle_percent']}%\")\n",
        "    print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
        "else:\n",
        "    raise ValueError(f\"Experiment {EXP_ID} not found! Use Exp1-Exp6.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Partition Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Partition data among clients\n",
        "print(f\"Partitioning data with alpha={config['partition_alpha']}...\")\n",
        "dd.distribute_data(\n",
        "    num_clients=int(config[\"clients\"][\"total\"]),\n",
        "    alpha=float(config[\"partition_alpha\"]),\n",
        "    seed=int(config[\"seed\"])\n",
        ")\n",
        "print(f\"âœ… Data partitioned among {config['clients']['total']} clients\")\n",
        "for i in range(min(3, config['clients']['total'])):\n",
        "    print(f\"   Client {i}: {len(dd.get_client_data(i))} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Setup Folders and Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create timestamped run folder\n",
        "run_dir = Path(\"logs\") / \"FedBuff\" / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "run_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Write COMMIT.txt (simulated git hash for notebook)\n",
        "commit_hash = \"notebook_run\"\n",
        "csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n",
        "with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n",
        "    f.write(f\"{commit_hash},{csv_header}\\n\")\n",
        "\n",
        "# Save config to run folder\n",
        "with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n",
        "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "\n",
        "print(f\"âœ… Run folder created: {run_dir}\")\n",
        "print(f\"âœ… COMMIT.txt and CONFIG.yaml saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Initialize Server and Clients\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seed\n",
        "seed = int(config.get(\"seed\", 42))\n",
        "set_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "# Build global model\n",
        "global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n",
        "print(f\"âœ… Global model created: ResNet-18\")\n",
        "\n",
        "# Initialize server\n",
        "server = BufferedFedServer(\n",
        "    global_model=global_model,\n",
        "    total_train_samples=len(dd.train_dataset),\n",
        "    buffer_size=int(config[\"buff\"][\"buffer_size\"]),\n",
        "    buffer_timeout_s=float(config[\"buff\"][\"buffer_timeout_s\"]),\n",
        "    use_sample_weighing=bool(config[\"buff\"][\"use_sample_weighing\"]),\n",
        "    target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n",
        "    max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n",
        "    eval_interval_s=int(config[\"eval\"][\"interval_seconds\"]),\n",
        "    data_dir=config[\"data\"][\"data_dir\"],\n",
        "    checkpoints_dir=str(run_dir / \"checkpoints\"),\n",
        "    logs_dir=str(run_dir),\n",
        "    global_log_csv=str(run_dir / \"FedBuff.csv\"),\n",
        "    client_participation_csv=str(run_dir / \"FedBuffClientParticipation.csv\"),\n",
        "    final_model_path=str(run_dir / \"FedBuffModel.pt\"),\n",
        "    resume=False,  # Start fresh in notebook\n",
        "    device=get_device(),\n",
        "    eta=float(config[\"buff\"].get(\"eta\", 0.5)),\n",
        ")\n",
        "print(f\"âœ… Server initialized\")\n",
        "print(f\"   Device: {server.device}\")\n",
        "print(f\"   Buffer size: {server.buffer_size}\")\n",
        "print(f\"   Eta: {server.eta}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup client delays (for heterogeneity simulation)\n",
        "n = int(config[\"clients\"][\"total\"])\n",
        "pct = max(0, min(100, int(config[\"clients\"].get(\"struggle_percent\", 0))))\n",
        "k_slow = (n * pct) // 100\n",
        "slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
        "\n",
        "a_s, b_s = config[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
        "a_f, b_f = config[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
        "fix_delays = bool(config[\"clients\"].get(\"fix_delays_per_client\", True))\n",
        "jitter = float(config[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
        "\n",
        "per_client_base_delay: Dict[int, float] = {}\n",
        "if fix_delays:\n",
        "    for cid in range(n):\n",
        "        if cid in slow_ids:\n",
        "            per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
        "        else:\n",
        "            per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
        "\n",
        "# Create clients\n",
        "clients = []\n",
        "for cid in range(n):\n",
        "    subset = dd.get_client_data(cid)\n",
        "    base_delay = per_client_base_delay.get(cid, 0.0)\n",
        "    is_slow = cid in slow_ids\n",
        "    clients.append(LocalBuffClient(\n",
        "        cid=cid,\n",
        "        cfg=config,\n",
        "        subset=subset,\n",
        "        work_dir=str(run_dir / \"checkpoints\" / \"clients\"),\n",
        "        base_delay=base_delay,\n",
        "        slow=is_slow,\n",
        "        delay_ranges=((float(a_s), float(b_s)), (float(a_f), float(b_f))),\n",
        "        jitter=jitter,\n",
        "        fix_delay=fix_delays,\n",
        "    ))\n",
        "\n",
        "print(f\"âœ… Created {len(clients)} clients\")\n",
        "print(f\"   Slow clients: {len(slow_ids)}\")\n",
        "print(f\"   Fast clients: {n - len(slow_ids)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display results for all 6 experiments\n",
        "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
        "    print(\"=\"*80)\n",
        "    print(\"FEDBUFF RESULTS - ALL 6 EXPERIMENTS\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    all_results = {}\n",
        "    \n",
        "    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
        "        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n",
        "            run_dir = experiment_results[exp_id][\"run_dir\"]\n",
        "            csv_path = run_dir / \"FedBuff.csv\"\n",
        "            \n",
        "            if csv_path.exists():\n",
        "                df = pd.read_csv(csv_path)\n",
        "                all_results[exp_id] = df\n",
        "                \n",
        "                exp_config = experiments[exp_id]\n",
        "                print(f\"\\n{'='*80}\")\n",
        "                print(f\"{exp_id}: {exp_config['name']}\")\n",
        "                print(f\"{'='*80}\")\n",
        "                print(f\"  Alpha: {exp_config['partition_alpha']}\")\n",
        "                print(f\"  Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
        "                print(f\"  Max rounds: {exp_config['train']['max_rounds']}\")\n",
        "                print(f\"\\n  Total rounds completed: {df['total_agg'].max()}\")\n",
        "                \n",
        "                final_row = df.iloc[-1]\n",
        "                print(f\"\\n  Final metrics:\")\n",
        "                print(f\"    - Test accuracy: {final_row['test_acc']:.4f}\")\n",
        "                print(f\"    - Train accuracy: {final_row['avg_train_acc']:.4f}\")\n",
        "                print(f\"    - Test loss: {final_row['test_loss']:.4f}\")\n",
        "                print(f\"    - Total time: {final_row['time']:.1f} seconds ({final_row['time']/60:.2f} minutes)\")\n",
        "                \n",
        "                best_acc = df['test_acc'].max()\n",
        "                best_round = df.loc[df['test_acc'].idxmax(), 'total_agg']\n",
        "                print(f\"\\n  Best test accuracy: {best_acc:.4f} (round {best_round})\")\n",
        "                \n",
        "                print(f\"\\n  Run folder: {run_dir}\")\n",
        "            else:\n",
        "                print(f\"\\n{'='*80}\")\n",
        "                print(f\"{exp_id}: Results file not found\")\n",
        "                print(f\"  Run folder: {run_dir}\")\n",
        "        else:\n",
        "            status = experiment_results.get(exp_id, {}).get(\"status\", \"not run\")\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"{exp_id}: {status.upper()}\")\n",
        "            if status == \"error\":\n",
        "                print(f\"  Error: {experiment_results[exp_id].get('error', 'Unknown error')}\")\n",
        "    \n",
        "    # Summary table\n",
        "    if len(all_results) > 0:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"SUMMARY TABLE\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Rounds':<8} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
        "        print(\"-\"*80)\n",
        "        \n",
        "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
        "            if exp_id in all_results:\n",
        "                df = all_results[exp_id]\n",
        "                cfg = experiments[exp_id]\n",
        "                final_row = df.iloc[-1]\n",
        "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
        "                      f\"{df['total_agg'].max():<8} {df['test_acc'].max():<10.4f} {final_row['test_acc']:<11.4f} \"\n",
        "                      f\"{final_row['time']/60:<12.2f}\")\n",
        "        \n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Show first and last rows for each experiment\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(\"DETAILED DATA PREVIEW\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
        "            if exp_id in all_results:\n",
        "                df = all_results[exp_id]\n",
        "                exp_config = experiments[exp_id]\n",
        "                print(f\"\\n{exp_id}: {exp_config['name']}\")\n",
        "                print(f\"  First 3 rows:\")\n",
        "                print(df.head(3).to_string(index=False))\n",
        "                print(f\"\\n  Last 3 rows:\")\n",
        "                print(df.tail(3).to_string(index=False))\n",
        "                print()\n",
        "    else:\n",
        "        print(\"\\nâš ï¸  No completed experiments found. Run experiments first using Section 8.\")\n",
        "else:\n",
        "    print(\"âš ï¸  No experiment results found. Run experiments first using Section 8.\")\n",
        "    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Run FedBuff Experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualization suite for all 6 experiments\n",
        "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
        "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "    \n",
        "    all_data = {}\n",
        "    all_participation = {}\n",
        "    \n",
        "    # Load all experiment data\n",
        "    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
        "        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n",
        "            run_dir = experiment_results[exp_id][\"run_dir\"]\n",
        "            csv_path = run_dir / \"FedBuff.csv\"\n",
        "            participation_path = run_dir / \"FedBuffClientParticipation.csv\"\n",
        "            \n",
        "            if csv_path.exists():\n",
        "                df = pd.read_csv(csv_path)\n",
        "                df['time_min'] = df['time'] / 60.0\n",
        "                all_data[exp_id] = {\n",
        "                    'df': df,\n",
        "                    'run_dir': run_dir,\n",
        "                    'config': experiments[exp_id]\n",
        "                }\n",
        "                \n",
        "                if participation_path.exists():\n",
        "                    part_df = pd.read_csv(participation_path)\n",
        "                    all_participation[exp_id] = part_df\n",
        "    \n",
        "    if len(all_data) > 0:\n",
        "        print(f\"âœ… Loaded data for {len(all_data)} experiments\")\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Generate individual plots for each experiment\n",
        "        for exp_id, exp_data in all_data.items():\n",
        "            df = exp_data['df']\n",
        "            run_dir = exp_data['run_dir']\n",
        "            exp_config = exp_data['config']\n",
        "            participation_path = run_dir / \"FedBuffClientParticipation.csv\"\n",
        "            \n",
        "            print(f\"\\nðŸ“Š Generating plots for {exp_id}: {exp_config['name']}\")\n",
        "            \n",
        "            # Plot 1: Accuracy vs Rounds\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "            \n",
        "            axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
        "            train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
        "            train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n",
        "            if len(train_acc) > 0:\n",
        "                axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
        "            axes[0].set_xlabel('Round', fontsize=12)\n",
        "            axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "            axes[0].set_title(f'{exp_id}: Accuracy vs Rounds', fontsize=14, fontweight='bold')\n",
        "            axes[0].grid(True, alpha=0.3)\n",
        "            axes[0].legend(fontsize=10)\n",
        "            axes[0].set_ylim([0, 1.0])\n",
        "            \n",
        "            axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
        "            train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
        "            train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n",
        "            if len(train_loss) > 0:\n",
        "                axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
        "            axes[1].set_xlabel('Round', fontsize=12)\n",
        "            axes[1].set_ylabel('Loss', fontsize=12)\n",
        "            axes[1].set_title(f'{exp_id}: Loss vs Rounds', fontsize=14, fontweight='bold')\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "            axes[1].legend(fontsize=10)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(run_dir / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            \n",
        "            # Plot 2: Accuracy vs Wall Clock Time\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "            \n",
        "            axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
        "            train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
        "            train_time = df[df['avg_train_acc'] > 0]['time_min']\n",
        "            if len(train_acc_time) > 0:\n",
        "                axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
        "            axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "            axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "            axes[0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
        "            axes[0].grid(True, alpha=0.3)\n",
        "            axes[0].legend(fontsize=10)\n",
        "            axes[0].set_ylim([0, 1.0])\n",
        "            \n",
        "            axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
        "            train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
        "            train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n",
        "            if len(train_loss_time) > 0:\n",
        "                axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
        "            axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "            axes[1].set_ylabel('Loss', fontsize=12)\n",
        "            axes[1].set_title(f'{exp_id}: Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "            axes[1].legend(fontsize=10)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(run_dir / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            \n",
        "            # Plot 3: Convergence Speed Analysis\n",
        "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "            \n",
        "            thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "            times_to_threshold = []\n",
        "            rounds_to_threshold = []\n",
        "            reached_thresholds = []\n",
        "            \n",
        "            for thresh in thresholds:\n",
        "                mask = df['test_acc'] >= thresh\n",
        "                if mask.any():\n",
        "                    first_idx = mask.idxmax()\n",
        "                    times_to_threshold.append(df.loc[first_idx, 'time_min'])\n",
        "                    rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n",
        "                    reached_thresholds.append(thresh)\n",
        "                else:\n",
        "                    break\n",
        "            \n",
        "            if len(reached_thresholds) > 0:\n",
        "                axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n",
        "                axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n",
        "                axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n",
        "                axes[0].set_title(f'{exp_id}: Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
        "                axes[0].set_xticks(range(len(reached_thresholds)))\n",
        "                axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
        "                axes[0].grid(True, alpha=0.3, axis='y')\n",
        "                \n",
        "                for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n",
        "                    axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
        "                \n",
        "                axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
        "                axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n",
        "                axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n",
        "                axes[1].set_title(f'{exp_id}: Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
        "                axes[1].set_xticks(range(len(reached_thresholds)))\n",
        "                axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
        "                axes[1].grid(True, alpha=0.3, axis='y')\n",
        "                \n",
        "                for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n",
        "                    axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(run_dir / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            \n",
        "            # Plot 4: Training Efficiency\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "            \n",
        "            if len(df) > 1:\n",
        "                df_sorted = df.sort_values('time_min')\n",
        "                efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n",
        "                efficiency = efficiency.fillna(0)\n",
        "                \n",
        "                ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n",
        "                ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n",
        "                ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "                ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n",
        "                ax.set_title(f'{exp_id}: Training Efficiency', fontsize=14, fontweight='bold')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "                \n",
        "                max_eff_idx = efficiency.idxmax()\n",
        "                max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n",
        "                max_eff_val = efficiency.loc[max_eff_idx]\n",
        "                ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n",
        "                ax.legend(fontsize=10)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(run_dir / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            \n",
        "            # Plot 5: Client Participation Analysis\n",
        "            if participation_path.exists():\n",
        "                part_df = pd.read_csv(participation_path)\n",
        "                \n",
        "                fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "                \n",
        "                client_counts = part_df['client_id'].value_counts().sort_index()\n",
        "                axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n",
        "                axes[0, 0].set_xlabel('Client ID', fontsize=11)\n",
        "                axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n",
        "                axes[0, 0].set_title(f'{exp_id}: Client Participation Frequency', fontsize=12, fontweight='bold')\n",
        "                axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "                \n",
        "                participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n",
        "                axes[0, 1].plot(participation_by_round.index, participation_by_round.values, \n",
        "                               marker='o', markersize=4, linewidth=2, color='coral')\n",
        "                axes[0, 1].set_xlabel('Round', fontsize=11)\n",
        "                axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n",
        "                axes[0, 1].set_title(f'{exp_id}: Client Participation per Round', fontsize=12, fontweight='bold')\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "                \n",
        "                client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n",
        "                axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values, \n",
        "                                marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n",
        "                axes[1, 0].set_xlabel('Round', fontsize=11)\n",
        "                axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n",
        "                axes[1, 0].set_title(f'{exp_id}: Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n",
        "                axes[1, 0].grid(True, alpha=0.3)\n",
        "                axes[1, 0].set_ylim([0, 1.0])\n",
        "                \n",
        "                if len(part_df) > 0:\n",
        "                    client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values \n",
        "                                  for cid in sorted(part_df['client_id'].unique())[:10]]\n",
        "                    if len(client_accs) > 0:\n",
        "                        axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n",
        "                        axes[1, 1].set_xlabel('Client ID', fontsize=11)\n",
        "                        axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
        "                        axes[1, 1].set_title(f'{exp_id}: Client Accuracy Distribution', fontsize=12, fontweight='bold')\n",
        "                        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "                        axes[1, 1].set_ylim([0, 1.0])\n",
        "                \n",
        "                plt.tight_layout()\n",
        "                plt.savefig(run_dir / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n",
        "                plt.close()\n",
        "            \n",
        "            # Plot 6: Training Progress Summary\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "            \n",
        "            axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
        "            train_acc_plot = df[df['avg_train_acc'] > 0]\n",
        "            if len(train_acc_plot) > 0:\n",
        "                axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'], \n",
        "                               'r--', linewidth=2, label='Train')\n",
        "            axes[0, 0].set_xlabel('Round', fontsize=11)\n",
        "            axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
        "            axes[0, 0].set_title(f'{exp_id}: Accuracy Trajectory', fontsize=12, fontweight='bold')\n",
        "            axes[0, 0].legend(fontsize=10)\n",
        "            axes[0, 0].grid(True, alpha=0.3)\n",
        "            axes[0, 0].set_ylim([0, 1.0])\n",
        "            \n",
        "            axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n",
        "            train_loss_plot = df[df['avg_train_loss'] > 0]\n",
        "            if len(train_loss_plot) > 0:\n",
        "                axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'], \n",
        "                               'r--', linewidth=2, label='Train')\n",
        "            axes[0, 1].set_xlabel('Round', fontsize=11)\n",
        "            axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
        "            axes[0, 1].set_title(f'{exp_id}: Loss Trajectory', fontsize=12, fontweight='bold')\n",
        "            axes[0, 1].legend(fontsize=10)\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "            \n",
        "            axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
        "            if len(train_acc_plot) > 0:\n",
        "                train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n",
        "                axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'], \n",
        "                                'r--', linewidth=2, label='Train')\n",
        "            axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n",
        "            axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
        "            axes[1, 0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n",
        "            axes[1, 0].legend(fontsize=10)\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "            axes[1, 0].set_ylim([0, 1.0])\n",
        "            \n",
        "            axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n",
        "            axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n",
        "            axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n",
        "            axes[1, 1].set_title(f'{exp_id}: Round Progression Over Time', fontsize=12, fontweight='bold')\n",
        "            axes[1, 1].grid(True, alpha=0.3)\n",
        "            \n",
        "            if len(df) > 1:\n",
        "                total_time = df['time_min'].iloc[-1]\n",
        "                total_rounds = df['total_agg'].iloc[-1]\n",
        "                rate = total_rounds / total_time if total_time > 0 else 0\n",
        "                axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min', \n",
        "                               transform=axes[1, 1].transAxes, fontsize=10,\n",
        "                               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(run_dir / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            \n",
        "            print(f\"  âœ… All plots saved to {run_dir}\")\n",
        "        \n",
        "        # Create comparison plots across all experiments\n",
        "        print(f\"\\nðŸ“Š Creating comparison plots across all experiments...\")\n",
        "        \n",
        "        # Comparison Plot 1: Test Accuracy vs Rounds (all experiments)\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n",
        "            df = exp_data['df']\n",
        "            exp_config = exp_data['config']\n",
        "            label = f\"{exp_id} (Î±={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n",
        "            ax.plot(df['total_agg'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n",
        "        ax.set_xlabel('Round', fontsize=12)\n",
        "        ax.set_ylabel('Test Accuracy', fontsize=12)\n",
        "        ax.set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
        "        ax.legend(fontsize=9, loc='lower right')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim([0, 1.0])\n",
        "        \n",
        "        # Save comparison plot\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
        "            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"FedBuff\" / \"comparisons\"\n",
        "        else:\n",
        "            comparison_dir = Path(\"./logs/FedBuff/comparisons\")\n",
        "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(comparison_dir / \"comparison_accuracy_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"  âœ… Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_rounds.png'}\")\n",
        "        \n",
        "        # Comparison Plot 2: Test Accuracy vs Time (all experiments)\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n",
        "            df = exp_data['df']\n",
        "            exp_config = exp_data['config']\n",
        "            label = f\"{exp_id} (Î±={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n",
        "            ax.plot(df['time_min'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n",
        "        ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "        ax.set_ylabel('Test Accuracy', fontsize=12)\n",
        "        ax.set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
        "        ax.legend(fontsize=9, loc='lower right')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim([0, 1.0])\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(comparison_dir / \"comparison_accuracy_vs_time.png\", dpi=150, bbox_inches='tight')\n",
        "        plt.close()\n",
        "        print(f\"  âœ… Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_time.png'}\")\n",
        "        \n",
        "        print(f\"\\nâœ… All visualizations completed!\")\n",
        "        print(f\"   Individual plots saved to each experiment's run folder\")\n",
        "        print(f\"   Comparison plots saved to: {comparison_dir}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"âš ï¸  No completed experiments found. Run experiments first using Section 8.\")\n",
        "else:\n",
        "    print(\"âš ï¸  No experiment results found. Run experiments first using Section 8.\")\n",
        "    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start evaluation timer\n",
        "server.start_eval_timer()\n",
        "print(\"âœ… Evaluation timer started\")\n",
        "\n",
        "# Concurrency gate\n",
        "sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n",
        "\n",
        "def client_loop(client: LocalBuffClient):\n",
        "    \"\"\"Client training loop.\"\"\"\n",
        "    while True:\n",
        "        with sem:\n",
        "            cont = client.fit_once(server)\n",
        "        if not cont:\n",
        "            break\n",
        "        time.sleep(0.05)\n",
        "\n",
        "# Launch client threads\n",
        "threads = []\n",
        "for cl in clients:\n",
        "    t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n",
        "    t.start()\n",
        "    threads.append(t)\n",
        "\n",
        "print(f\"âœ… Started {len(threads)} client threads\")\n",
        "print(f\"\\nðŸš€ FedBuff experiment running...\")\n",
        "print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
        "print(f\"   Target accuracy: {config['eval']['target_accuracy']}\")\n",
        "print(f\"\\nResults will be saved to: {run_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wait for completion\n",
        "server.wait()\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"\\nâœ… Experiment completed!\")\n",
        "print(f\"\\nðŸ“ Results saved to: {run_dir}\")\n",
        "print(f\"   - FedBuff.csv: Global metrics\")\n",
        "print(f\"   - FedBuffClientParticipation.csv: Client participation\")\n",
        "print(f\"   - FedBuffModel.pt: Final model\")\n",
        "print(f\"   - CONFIG.yaml: Configuration used\")\n",
        "print(f\"   - COMMIT.txt: Run metadata\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. View Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and display results\n",
        "csv_path = run_dir / \"FedBuff.csv\"\n",
        "if csv_path.exists():\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(\"=== FedBuff Results ===\")\n",
        "    print(f\"\\nTotal rounds: {df['total_agg'].max()}\")\n",
        "    print(f\"\\nFinal metrics:\")\n",
        "    final_row = df.iloc[-1]\n",
        "    print(f\"  - Test accuracy: {final_row['test_acc']:.4f}\")\n",
        "    print(f\"  - Train accuracy: {final_row['avg_train_acc']:.4f}\")\n",
        "    print(f\"  - Test loss: {final_row['test_loss']:.4f}\")\n",
        "    print(f\"  - Total time: {final_row['time']:.1f} seconds\")\n",
        "    \n",
        "    print(f\"\\nBest test accuracy: {df['test_acc'].max():.4f} (round {df.loc[df['test_acc'].idxmax(), 'total_agg']})\")\n",
        "    \n",
        "    print(f\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "    \n",
        "    print(f\"\\nLast 5 rows:\")\n",
        "    print(df.tail())\n",
        "else:\n",
        "    print(\"Results file not found yet.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Comprehensive Results Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualization suite\n",
        "csv_path = run_dir / \"FedBuff.csv\"\n",
        "participation_path = run_dir / \"FedBuffClientParticipation.csv\"\n",
        "\n",
        "if csv_path.exists():\n",
        "    df = pd.read_csv(csv_path)\n",
        "    \n",
        "    # Convert time to minutes for better readability\n",
        "    df['time_min'] = df['time'] / 60.0\n",
        "    \n",
        "    # ========== Plot 1: Accuracy vs Rounds ==========\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Test accuracy over rounds\n",
        "    axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
        "    train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
        "    train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n",
        "    if len(train_acc) > 0:\n",
        "        axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
        "    axes[0].set_xlabel('Round', fontsize=12)\n",
        "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[0].set_title('Accuracy vs Rounds', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].set_ylim([0, 1.0])\n",
        "    \n",
        "    # Loss over rounds\n",
        "    axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
        "    train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
        "    train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n",
        "    if len(train_loss) > 0:\n",
        "        axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
        "    axes[1].set_xlabel('Round', fontsize=12)\n",
        "    axes[1].set_ylabel('Loss', fontsize=12)\n",
        "    axes[1].set_title('Loss vs Rounds', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend(fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(run_dir / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
        "    print(f\"âœ… Plot 1 saved: 1_accuracy_loss_vs_rounds.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    # ========== Plot 2: Accuracy vs Wall Clock Time ==========\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Test accuracy over time\n",
        "    axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
        "    train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
        "    train_time = df[df['avg_train_acc'] > 0]['time_min']\n",
        "    if len(train_acc_time) > 0:\n",
        "        axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
        "    axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "    axes[0].set_title('Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].set_ylim([0, 1.0])\n",
        "    \n",
        "    # Loss over time\n",
        "    axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
        "    train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
        "    train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n",
        "    if len(train_loss_time) > 0:\n",
        "        axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
        "    axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "    axes[1].set_ylabel('Loss', fontsize=12)\n",
        "    axes[1].set_title('Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend(fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(run_dir / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n",
        "    print(f\"âœ… Plot 2 saved: 2_accuracy_loss_vs_time.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    # ========== Plot 3: Convergence Speed Analysis ==========\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Time to reach accuracy thresholds\n",
        "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "    times_to_threshold = []\n",
        "    rounds_to_threshold = []\n",
        "    reached_thresholds = []\n",
        "    \n",
        "    for thresh in thresholds:\n",
        "        mask = df['test_acc'] >= thresh\n",
        "        if mask.any():\n",
        "            first_idx = mask.idxmax()\n",
        "            times_to_threshold.append(df.loc[first_idx, 'time_min'])\n",
        "            rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n",
        "            reached_thresholds.append(thresh)\n",
        "        else:\n",
        "            break\n",
        "    \n",
        "    if len(reached_thresholds) > 0:\n",
        "        axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n",
        "        axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n",
        "        axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n",
        "        axes[0].set_title('Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
        "        axes[0].set_xticks(range(len(reached_thresholds)))\n",
        "        axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
        "        axes[0].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n",
        "            axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
        "        axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n",
        "        axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n",
        "        axes[1].set_title('Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
        "        axes[1].set_xticks(range(len(reached_thresholds)))\n",
        "        axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
        "        axes[1].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n",
        "            axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(run_dir / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n",
        "    print(f\"âœ… Plot 3 saved: 3_convergence_speed.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    # ========== Plot 4: Training Efficiency (Accuracy per Time) ==========\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    \n",
        "    # Calculate efficiency: accuracy gain per minute\n",
        "    if len(df) > 1:\n",
        "        df_sorted = df.sort_values('time_min')\n",
        "        efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n",
        "        efficiency = efficiency.fillna(0)\n",
        "        \n",
        "        ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n",
        "        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n",
        "        ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "        ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n",
        "        ax.set_title('Training Efficiency: Accuracy Gain Rate Over Time', fontsize=14, fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Highlight peak efficiency\n",
        "        max_eff_idx = efficiency.idxmax()\n",
        "        max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n",
        "        max_eff_val = efficiency.loc[max_eff_idx]\n",
        "        ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n",
        "        ax.legend(fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(run_dir / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n",
        "    print(f\"âœ… Plot 4 saved: 4_training_efficiency.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    # ========== Plot 5: Client Participation Analysis ==========\n",
        "    if participation_path.exists():\n",
        "        part_df = pd.read_csv(participation_path)\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        \n",
        "        # Client participation frequency\n",
        "        client_counts = part_df['client_id'].value_counts().sort_index()\n",
        "        axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n",
        "        axes[0, 0].set_xlabel('Client ID', fontsize=11)\n",
        "        axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n",
        "        axes[0, 0].set_title('Client Participation Frequency', fontsize=12, fontweight='bold')\n",
        "        axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        # Client participation over rounds\n",
        "        participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n",
        "        axes[0, 1].plot(participation_by_round.index, participation_by_round.values, \n",
        "                       marker='o', markersize=4, linewidth=2, color='coral')\n",
        "        axes[0, 1].set_xlabel('Round', fontsize=11)\n",
        "        axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n",
        "        axes[0, 1].set_title('Client Participation per Round', fontsize=12, fontweight='bold')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Average client accuracy over time\n",
        "        client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n",
        "        axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values, \n",
        "                        marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n",
        "        axes[1, 0].set_xlabel('Round', fontsize=11)\n",
        "        axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n",
        "        axes[1, 0].set_title('Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Client accuracy distribution (box plot)\n",
        "        if len(part_df) > 0:\n",
        "            client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values \n",
        "                          for cid in sorted(part_df['client_id'].unique())[:10]]  # Limit to first 10 clients for readability\n",
        "            if len(client_accs) > 0:\n",
        "                axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n",
        "                axes[1, 1].set_xlabel('Client ID', fontsize=11)\n",
        "                axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
        "                axes[1, 1].set_title('Client Accuracy Distribution', fontsize=12, fontweight='bold')\n",
        "                axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "                axes[1, 1].set_ylim([0, 1.0])\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(run_dir / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"âœ… Plot 5 saved: 5_client_participation.png\")\n",
        "        plt.show()\n",
        "    \n",
        "    # ========== Plot 6: Training Progress Summary ==========\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Accuracy trajectory\n",
        "    axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
        "    train_acc_plot = df[df['avg_train_acc'] > 0]\n",
        "    if len(train_acc_plot) > 0:\n",
        "        axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'], \n",
        "                       'r--', linewidth=2, label='Train')\n",
        "    axes[0, 0].set_xlabel('Round', fontsize=11)\n",
        "    axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
        "    axes[0, 0].set_title('Accuracy Trajectory', fontsize=12, fontweight='bold')\n",
        "    axes[0, 0].legend(fontsize=10)\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].set_ylim([0, 1.0])\n",
        "    \n",
        "    # Loss trajectory\n",
        "    axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n",
        "    train_loss_plot = df[df['avg_train_loss'] > 0]\n",
        "    if len(train_loss_plot) > 0:\n",
        "        axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'], \n",
        "                       'r--', linewidth=2, label='Train')\n",
        "    axes[0, 1].set_xlabel('Round', fontsize=11)\n",
        "    axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
        "    axes[0, 1].set_title('Loss Trajectory', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].legend(fontsize=10)\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Accuracy vs time\n",
        "    axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
        "    if len(train_acc_plot) > 0:\n",
        "        train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n",
        "        axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'], \n",
        "                        'r--', linewidth=2, label='Train')\n",
        "    axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n",
        "    axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
        "    axes[1, 0].set_title('Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].legend(fontsize=10)\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    axes[1, 0].set_ylim([0, 1.0])\n",
        "    \n",
        "    # Round progression over time\n",
        "    axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n",
        "    axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n",
        "    axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n",
        "    axes[1, 1].set_title('Round Progression Over Time', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add rate annotation\n",
        "    if len(df) > 1:\n",
        "        total_time = df['time_min'].iloc[-1]\n",
        "        total_rounds = df['total_agg'].iloc[-1]\n",
        "        rate = total_rounds / total_time if total_time > 0 else 0\n",
        "        axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min', \n",
        "                       transform=axes[1, 1].transAxes, fontsize=10,\n",
        "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(run_dir / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n",
        "    print(f\"âœ… Plot 6 saved: 6_training_summary.png\")\n",
        "    plt.show()\n",
        "    \n",
        "    # ========== Print Summary Statistics ==========\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXPERIMENT SUMMARY STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Total Rounds: {df['total_agg'].max()}\")\n",
        "    print(f\"Total Time: {df['time_min'].iloc[-1]:.2f} minutes ({df['time'].iloc[-1]:.2f} seconds)\")\n",
        "    print(f\"Final Test Accuracy: {df['test_acc'].iloc[-1]:.4f}\")\n",
        "    print(f\"Best Test Accuracy: {df['test_acc'].max():.4f} (Round {df.loc[df['test_acc'].idxmax(), 'total_agg']})\")\n",
        "    if len(df) > 1:\n",
        "        print(f\"Average Round Time: {(df['time'].iloc[-1] / df['total_agg'].iloc[-1]):.2f} seconds\")\n",
        "        print(f\"Rounds per Minute: {(df['total_agg'].iloc[-1] / df['time_min'].iloc[-1]):.2f}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "else:\n",
        "    print(\"Results file not found yet.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
