{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d189811",
   "metadata": {
    "id": "9d189811"
   },
   "source": [
    "# FedAsync\n",
    "\n",
    "This notebook is organized with one cell per original source file,\n",
    "and is optimized for running on **Google Colab** with all results\n",
    "stored on **Google Drive**.\n",
    "\n",
    "**How to use on Colab:**\n",
    "\n",
    "1. Upload this notebook to Colab.\n",
    "2. Run the first cell to mount Google Drive and set your base path.\n",
    "3. All subsequent paths (data, logs, models, results) will be created\n",
    "   relative to that base directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P6_6oPCNeefj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4861,
     "status": "ok",
     "timestamp": 1764519236346,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "P6_6oPCNeefj",
    "outputId": "ce49bcf2-03cb-4981-ce7b-a5b01705ede8"
   },
   "outputs": [],
   "source": [
    "! pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9788e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1764519236752,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "1ce9788e",
    "outputId": "5f0df291-df6a-4568-d2e9-479dd6b42cbd"
   },
   "outputs": [],
   "source": [
    "\n",
    "# @title Mount Google Drive and set base project directory\n",
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Base directory INSIDE your Drive where everything will be stored.\n",
    "# You can change this to any folder you like.\n",
    "BASE_DIR = \"/content/drive/MyDrive/Arya/FedAsync\"  # @param {type:\"string\"}\n",
    "\n",
    "BASE_PATH = Path(BASE_DIR).expanduser()\n",
    "BASE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "os.chdir(BASE_PATH)\n",
    "os.environ[\"FEDASYNC_BASE_DIR\"] = str(BASE_PATH)\n",
    "print(\"Working directory set to:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80024cb2",
   "metadata": {
    "id": "80024cb2"
   },
   "source": [
    "## config.yaml\n",
    "\n",
    "Below is the YAML configuration used by the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "config_path = Path(BASE_DIR) / \"config.yaml\"\n",
    "config_text = f\"\"\"\n",
    "\n",
    "# ---- Data ----\n",
    "data:\n",
    "  dataset: cifar10\n",
    "  data_dir: ./data\n",
    "  num_classes: 10\n",
    "\n",
    "# ---- Clients ----\n",
    "clients:\n",
    "  total: 20\n",
    "  concurrent: 10\n",
    "  local_epochs: 2\n",
    "  batch_size: 128\n",
    "  lr: 0.005\n",
    "\n",
    "  # --- heterogeneity controls ---\n",
    "  struggle_percent: 20          # % of all clients that are slow\n",
    "  delay_slow_range: [0.8, 2.0]  # seconds, Uniform[a, b] for slow clients\n",
    "  delay_fast_range: [0.0, 0.2]  # seconds, Uniform[a, b] for fast/normal clients\n",
    "  jitter_per_round: 0.1         # extra +/- seconds each local fit; 0 disables\n",
    "  fix_delays_per_client: true   # true: sample once per client; false: resample every fit\n",
    "\n",
    "# ---- Async FedAsync ----\n",
    "async:\n",
    "  alpha: 0.5\n",
    "\n",
    "# ---- Evaluation / stopping ----\n",
    "eval:\n",
    "  # Aggregation-based eval cadence\n",
    "  eval_every_aggs: 5\n",
    "  target_accuracy: 0.8     # stop when global test_acc >= this value\n",
    "\n",
    "# ---- Safety cap on merges (optional) ----\n",
    "train:\n",
    "  max_rounds: 1000\n",
    "\n",
    "# ---- Partitioning ----\n",
    "partition_alpha: 0.1\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "seed: 42\n",
    "\n",
    "# ---- Runtime / I/O ----\n",
    "server_runtime:\n",
    "  client_delay: 0.0   # extra global delay added on every client before training\n",
    "\n",
    "io:\n",
    "  checkpoints_dir: \"{(Path(BASE_DIR) / 'checkpoints' / 'FedAsync').as_posix()}\"\n",
    "  logs_dir: \"{(Path(BASE_DIR) / 'logs').as_posix()}\"\n",
    "  results_dir: \"{(Path(BASE_DIR) / 'results').as_posix()}\"\n",
    "  global_log_csv: \"{(Path(BASE_DIR) / 'logs' / 'FedAsync.csv').as_posix()}\"\n",
    "  client_participation_csv: \"{(Path(BASE_DIR) / 'logs' / 'FedAsyncClientParticipation.csv').as_posix()}\"\n",
    "  final_model_path: \"{(Path(BASE_DIR) / 'results' / 'FedAsyncModel.pt').as_posix()}\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "config_path.write_text(textwrap.dedent(config_text).lstrip())\n",
    "print(f\"config.yaml written to {config_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d6618",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "\n",
    "config_path = Path(BASE_DIR) / \"config_run.yaml\"\n",
    "config_text = f\"\"\"\n",
    "\n",
    "# ---- Data ----\n",
    "data:\n",
    "  dataset: cifar10\n",
    "  data_dir: ./data\n",
    "  num_classes: 10\n",
    "\n",
    "# ---- Clients ----\n",
    "clients:\n",
    "  total: 20\n",
    "  concurrent: 10\n",
    "  local_epochs: 2\n",
    "  batch_size: 128\n",
    "  lr: 0.005\n",
    "\n",
    "  # --- heterogeneity controls ---\n",
    "  struggle_percent: 20          # % of all clients that are slow\n",
    "  delay_slow_range: [0.8, 2.0]  # seconds, Uniform[a, b] for slow clients\n",
    "  delay_fast_range: [0.0, 0.2]  # seconds, Uniform[a, b] for fast/normal clients\n",
    "  jitter_per_round: 0.1         # extra +/- seconds each local fit; 0 disables\n",
    "  fix_delays_per_client: true   # true: sample once per client; false: resample every fit\n",
    "\n",
    "# ---- Async FedAsync ----\n",
    "async:\n",
    "  alpha: 0.5\n",
    "\n",
    "# ---- Evaluation / stopping ----\n",
    "eval:\n",
    "  # Aggregation-based eval cadence\n",
    "  eval_every_aggs: 5\n",
    "  target_accuracy: 0.8     # stop when global test_acc >= this value\n",
    "\n",
    "# ---- Safety cap on merges (optional) ----\n",
    "train:\n",
    "  max_rounds: 1000\n",
    "\n",
    "# ---- Partitioning ----\n",
    "partition_alpha: 1000\n",
    "\n",
    "# ---- Reproducibility ----\n",
    "seed: 42\n",
    "\n",
    "# ---- Runtime / I/O ----\n",
    "server_runtime:\n",
    "  client_delay: 0.0   # extra global delay added on every client before training\n",
    "\n",
    "io:\n",
    "  checkpoints_dir: \"{(Path(BASE_DIR) / 'checkpoints' / 'FedAsync').as_posix()}\"\n",
    "  logs_dir: \"{(Path(BASE_DIR) / 'logs').as_posix()}\"\n",
    "  results_dir: \"{(Path(BASE_DIR) / 'results').as_posix()}\"\n",
    "  global_log_csv: \"{(Path(BASE_DIR) / 'logs' / 'FedAsync.csv').as_posix()}\"\n",
    "  client_participation_csv: \"{(Path(BASE_DIR) / 'logs' / 'FedAsyncClientParticipation.csv').as_posix()}\"\n",
    "  final_model_path: \"{(Path(BASE_DIR) / 'results' / 'FedAsyncModel.pt').as_posix()}\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "config_path.write_text(textwrap.dedent(config_text).lstrip())\n",
    "print(f\"config.yaml written to {config_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fed7c69",
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1764519236792,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "0fed7c69"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d829789e",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1764519236795,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "d829789e"
   },
   "outputs": [],
   "source": [
    "# ===== helper.py =====\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed all RNGs used in this project.\n",
    "\n",
    "    Setting a global seed helps to ensure reproducible results.  This\n",
    "    function touches Python's builtâ€‘in random module, NumPy, and\n",
    "    PyTorch's CPU and GPU RNGs.  Deterministic behaviour in cuDNN\n",
    "    kernels is also enabled.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    seed:\n",
    "        The random seed to use.  Defaults to ``42``.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    # When running on GPUs you may have more than one device\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Deterministic behaviour comes at a performance cost but\n",
    "    # reproducibility is more important for experimentation.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Return the first available computation device.\n",
    "\n",
    "    Tries to use CUDA if available, otherwise falls back to MPS\n",
    "    (Apple Silicon) and finally the CPU.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    device:\n",
    "        A PyTorch ``torch.device`` object indicating where tensors\n",
    "        should be allocated.\n",
    "    \"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    # MPS stands for Metal Performance Shaders.  It is the backend\n",
    "    # available on Apple Silicon systems for GPU acceleration.\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d19aa",
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1764519236824,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "e47d19aa"
   },
   "outputs": [],
   "source": [
    "# ===== model.py =====\n",
    "\n",
    "# Utilities for building models and converting parameters\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def build_resnet18(num_classes: int = 10, pretrained: bool = False) -> nn.Module:\n",
    "    \"\"\"Create a ResNet-18 tailored for CIFAR-size inputs.\"\"\"\n",
    "    m = models.resnet18(weights=None)\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "\n",
    "    in_features = m.fc.in_features\n",
    "    m.fc = nn.Linear(in_features, num_classes)\n",
    "    m.num_classes = num_classes\n",
    "    return m\n",
    "\n",
    "\n",
    "def state_to_list(state: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
    "    \"\"\"Flatten a state_dict to a list of tensors on CPU.\"\"\"\n",
    "    return [t.detach().cpu().clone() for _, t in state.items()]\n",
    "\n",
    "\n",
    "def list_to_state(template: Dict[str, torch.Tensor], arrs: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Rebuild a state_dict from a list of tensors using a template for keys/dtypes/devices.\"\"\"\n",
    "    out: Dict[str, torch.Tensor] = {}\n",
    "    for (k, v), a in zip(template.items(), arrs):\n",
    "        out[k] = a.to(v.device).type_as(v)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7492f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2764,
     "status": "ok",
     "timestamp": 1764519239589,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "aed7492f",
    "outputId": "b177bb85-13b8-4847-b1c0-d30ead4cb92a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ===== partitioning.py =====\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "\n",
    "BASE_PATH = Path(os.environ.get(\"FEDASYNC_BASE_DIR\", \".\")).expanduser()\n",
    "DEFAULT_DATA_DIR = str(BASE_PATH / \"data\")\n",
    "DEFAULT_RESULTS_DIR = BASE_PATH / \"results\"\n",
    "\n",
    "\n",
    "class DataDistributor:\n",
    "    def __init__(self, dataset_name: str, data_dir: str = DEFAULT_DATA_DIR):\n",
    "        \"\"\"\n",
    "        Flexible data distributor for federated learning experiments.\n",
    "\n",
    "        Args:\n",
    "            dataset_name (str): Name of dataset ('CIFAR10', 'MNIST', etc.)\n",
    "            data_dir (str): Directory to store data.\n",
    "        \"\"\"\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        data_root = Path(data_dir)\n",
    "        if not data_root.is_absolute():\n",
    "            data_root = BASE_PATH / data_root\n",
    "        self.data_dir = str(data_root)\n",
    "        self.train_dataset, self.test_dataset, self.num_classes = self._load_dataset()\n",
    "        self.partitions = None\n",
    "\n",
    "    def _load_dataset(self) -> Tuple[Any, Any, int]:\n",
    "        \"\"\"Load supported torchvision datasets.\"\"\"\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            train = datasets.CIFAR10(self.data_dir, train=True, download=True, transform=transform)\n",
    "            test = datasets.CIFAR10(self.data_dir, train=False, download=True, transform=transform)\n",
    "            num_classes = 10\n",
    "\n",
    "        elif self.dataset_name == \"mnist\":\n",
    "            train = datasets.MNIST(self.data_dir, train=True, download=True, transform=transform)\n",
    "            test = datasets.MNIST(self.data_dir, train=False, download=True, transform=transform)\n",
    "            num_classes = 10\n",
    "\n",
    "        elif self.dataset_name == \"fashionmnist\":\n",
    "            train = datasets.FashionMNIST(self.data_dir, train=True, download=True, transform=transform)\n",
    "            test = datasets.FashionMNIST(self.data_dir, train=False, download=True, transform=transform)\n",
    "            num_classes = 10\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset '{self.dataset_name}' is not supported yet.\")\n",
    "\n",
    "        return train, test, num_classes\n",
    "\n",
    "    def distribute_data(self, num_clients: int, alpha: float = 0.5, seed: int = 42) -> Dict[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Perform Dirichlet-based data partitioning across clients (Non-IID).\n",
    "\n",
    "        Args:\n",
    "            num_clients (int): Number of clients.\n",
    "            alpha (float): Dirichlet distribution parameter (smaller = more non-IID).\n",
    "            seed (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        targets = np.array(self.train_dataset.targets)\n",
    "        self.partitions = {i: [] for i in range(num_clients)}\n",
    "\n",
    "        for cls in range(self.num_classes):\n",
    "            idxs = np.where(targets == cls)[0]\n",
    "            # Shuffle indices for this class\n",
    "            np.random.shuffle(idxs)\n",
    "            # Sample proportions from a Dirichlet distribution\n",
    "            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n",
    "            # Convert proportions to integer counts (floor) for each client\n",
    "            int_props = np.floor(proportions * len(idxs)).astype(int)\n",
    "            # Assign counts to clients\n",
    "            start = 0\n",
    "            for client_id, size in enumerate(int_props):\n",
    "                self.partitions[client_id].extend(idxs[start:start + size])\n",
    "                start += size\n",
    "            # If any samples are left over due to floor truncation, assign them\n",
    "            # to clients with the largest initial share (or random if equal).  This\n",
    "            # ensures that the union of partitions covers the full dataset.\n",
    "            remaining = len(idxs) - start\n",
    "            if remaining > 0:\n",
    "                # Rank clients by proportion (descending); break ties randomly\n",
    "                ranked_clients = np.argsort(-proportions)\n",
    "                # Distribute leftover samples in roundâ€‘robin order among ranked clients\n",
    "                for i in range(remaining):\n",
    "                    cid = ranked_clients[i % len(ranked_clients)]\n",
    "                    self.partitions[int(cid)].append(idxs[start + i])\n",
    "\n",
    "        for cid in self.partitions:\n",
    "            np.random.shuffle(self.partitions[cid])\n",
    "\n",
    "        return self.partitions\n",
    "\n",
    "    # ... rest of partitioning.py remains unchanged ...\n",
    "\n",
    "\n",
    "    def get_client_data(self, client_id: int) -> Subset:\n",
    "        \"\"\"\n",
    "        Retrieve dataset subset for a specific client.\n",
    "\n",
    "        Args:\n",
    "            client_id (int): Client identifier.\n",
    "        \"\"\"\n",
    "        if self.partitions is None:\n",
    "            raise ValueError(\"Please run distribute_data() before accessing client data.\")\n",
    "        indices = self.partitions[client_id]\n",
    "        return Subset(self.train_dataset, indices)\n",
    "\n",
    "    def visualize_distribution(self, save_path: str | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Create IEEE-style stacked bar chart of sample counts per client.\n",
    "\n",
    "        Args:\n",
    "            save_path (str | None): File path to save the visualization.\n",
    "        \"\"\"\n",
    "        if self.partitions is None:\n",
    "            raise ValueError(\"Run distribute_data() before visualization.\")\n",
    "\n",
    "        path_obj = Path(save_path) if save_path is not None else DEFAULT_RESULTS_DIR / \"data_distribution_ieee.png\"\n",
    "        if not path_obj.is_absolute():\n",
    "            path_obj = BASE_PATH / path_obj\n",
    "        path_obj.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        targets = np.array(self.train_dataset.targets)\n",
    "        client_counts = np.zeros((len(self.partitions), self.num_classes), dtype=int)\n",
    "\n",
    "        for cid, idxs in self.partitions.items():\n",
    "            class_counts = np.bincount(targets[idxs], minlength=self.num_classes)\n",
    "            client_counts[cid, :] = class_counts\n",
    "\n",
    "        # IEEE single-column figure size (~3.5in wide)\n",
    "        fig, ax = plt.subplots(figsize=(1.8, 1.2), dpi=300)\n",
    "        bottom = np.zeros(len(self.partitions))\n",
    "        colors = plt.get_cmap(\"tab20\").colors\n",
    "\n",
    "        for cls in range(self.num_classes):\n",
    "            ax.bar(\n",
    "                x=np.arange(len(self.partitions)),\n",
    "                height=client_counts[:, cls],\n",
    "                bottom=bottom,\n",
    "                color=colors[cls % len(colors)],\n",
    "                linewidth=0.1,\n",
    "                edgecolor=\"white\",\n",
    "            )\n",
    "            bottom += client_counts[:, cls]\n",
    "\n",
    "        ax.set_xlabel(\"Client ID\", fontsize=8)\n",
    "        ax.set_ylabel(\"Samples\", fontsize=8)\n",
    "        ax.set_title(f\"{self.dataset_name.upper()} Data Distribution Among Clients\", fontsize=9)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=7)\n",
    "\n",
    "        fig.savefig(path_obj, dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        print(f\"ðŸ“Š Data distribution plot saved at: {path_obj}\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Example Usage (for testing)\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    distributor = DataDistributor(dataset_name=\"CIFAR10\", data_dir=DEFAULT_DATA_DIR)\n",
    "    distributor.distribute_data(num_clients=5, alpha=0.3, seed=42)\n",
    "    distributor.visualize_distribution(DEFAULT_RESULTS_DIR / \"cifar10_distribution_ieee.png\")\n",
    "\n",
    "    # Retrieve client dataset subset\n",
    "    client_data = distributor.get_client_data(0)\n",
    "    print(f\"âœ… Client 0 has {len(client_data)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15dbb2e",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1764519239592,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "d15dbb2e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2393f504",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1764519239607,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "2393f504"
   },
   "outputs": [],
   "source": [
    "# ==== server.py ====\n",
    "\n",
    "# Async FedAsync server with periodic evaluation/logging and accuracy-based stopping\n",
    "import csv\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, Future\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "import io\n",
    "from utils.model import state_to_list, list_to_state, build_resnet18\n",
    "from utils.helper import get_device\n",
    "\n",
    "\n",
    "\n",
    "def _testloader(root: str, batch_size: int = 256):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    buf = io.StringIO()\n",
    "    # Silence torchvision download/cache prints\n",
    "    with redirect_stdout(buf), redirect_stderr(buf):\n",
    "        ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "def _evaluate(model: torch.nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            loss_sum += float(loss.item()) * y.size(0)\n",
    "            total += y.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "    return loss_sum / max(1, total), correct / max(1, total)\n",
    "\n",
    "\n",
    "def _async_eval_worker(\n",
    "    state_dict: dict,\n",
    "    data_dir: str,\n",
    "    num_classes: int,\n",
    "    log_path: str,\n",
    "    agg_id: int,\n",
    "    avg_train_loss: float,\n",
    "    avg_train_acc: float,\n",
    "    device_str: str,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate a model copy in a background thread to avoid blocking the server.\"\"\"\n",
    "    device = torch.device(device_str)\n",
    "    model = build_resnet18(num_classes=num_classes)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device)\n",
    "    loader = _testloader(root=data_dir, batch_size=256)\n",
    "    test_loss, test_acc = _evaluate(model, loader, device)\n",
    "\n",
    "    log_path = Path(log_path)\n",
    "    log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    write_header = not log_path.exists()\n",
    "    with log_path.open(\"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow(\n",
    "                [\"total_agg\", \"avg_train_loss\", \"avg_train_acc\", \"test_loss\", \"test_acc\", \"time\"]\n",
    "            )\n",
    "        writer.writerow(\n",
    "            [\n",
    "                int(agg_id),\n",
    "                float(avg_train_loss),\n",
    "                float(avg_train_acc),\n",
    "                float(test_loss),\n",
    "                float(test_acc),\n",
    "                time.time(),\n",
    "            ]\n",
    "        )\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "class AsyncFedServer:\n",
    "    \"\"\"FedAsync: fixed alpha mix (1 - alpha) * w_global + alpha * w_client. Logs every `eval_every_aggs` aggregations.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        global_model: torch.nn.Module,\n",
    "        alpha: float = 0.5,\n",
    "        target_accuracy: float = 0.70,\n",
    "        max_rounds: Optional[int] = None,\n",
    "        eval_every_aggs: int = 5,\n",
    "        data_dir: str = \"./data\",\n",
    "        logs_dir: str = \"./logs\",\n",
    "        global_log_csv: Optional[str] = None,\n",
    "        client_participation_csv: Optional[str] = None,\n",
    "        final_model_path: Optional[str] = None,\n",
    "        num_classes: int = 10,\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        self.model = global_model\n",
    "        self.template = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n",
    "        self.device = device or get_device()\n",
    "        self.model.to(self.device)\n",
    "        self.num_classes = int(num_classes)\n",
    "\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "        self.eval_every_aggs = int(eval_every_aggs)\n",
    "        self.target_accuracy = float(target_accuracy)\n",
    "        self.max_rounds = int(max_rounds) if max_rounds is not None else None\n",
    "\n",
    "        # I/O\n",
    "        self.data_dir = data_dir\n",
    "        self.log_dir = Path(logs_dir); self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Paths supplied by config (with defaults)\n",
    "        self.csv_path = Path(global_log_csv) if global_log_csv else (self.log_dir / \"FedAsync.csv\")\n",
    "        self.participation_csv = Path(client_participation_csv) if client_participation_csv else (self.log_dir / \"FedAsyncClientParticipation.csv\")\n",
    "        self.final_model_path = Path(final_model_path) if final_model_path else Path(\"./results/FedAsyncModel.pt\")\n",
    "        self.final_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Init CSV headers if files don't exist\n",
    "        if not self.csv_path.exists():\n",
    "            self.csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with self.csv_path.open(\"w\", newline=\"\") as f:\n",
    "                csv.writer(f).writerow(\n",
    "                    [\"total_agg\", \"avg_train_loss\", \"avg_train_acc\", \"test_loss\", \"test_acc\", \"time\"]\n",
    "                )\n",
    "\n",
    "        if not self.participation_csv.exists():\n",
    "            self.participation_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with self.participation_csv.open(\"w\", newline=\"\") as f:\n",
    "                csv.writer(f).writerow(\n",
    "                    [\n",
    "                        \"client_id\",\n",
    "                        \"local_train_loss\",\n",
    "                        \"local_train_acc\",\n",
    "                        \"local_test_loss\",\n",
    "                        \"local_test_acc\",\n",
    "                        \"total_agg\",\n",
    "                        \"staleness\",\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "        self._lock = threading.Lock()\n",
    "        self._stop = False\n",
    "        self.t_round = 0  # increments on every merge\n",
    "        self.testloader = _testloader(self.data_dir)\n",
    "        self._train_loss_acc_accum: List[Tuple[float, float, int]] = []  # (loss, acc, n) since last eval\n",
    "        # Async evaluation state\n",
    "        self._eval_executor: Optional[ThreadPoolExecutor] = ThreadPoolExecutor(max_workers=1)\n",
    "        self._async_eval_future: Optional[Future] = None\n",
    "        self._init_async_eval_log()\n",
    "        self._last_eval: Tuple[float, float] = (0.0, 0.0)\n",
    "\n",
    "    def _save_final_model(self) -> None:\n",
    "        torch.save(self.model.state_dict(), self.final_model_path)\n",
    "\n",
    "    def _shutdown_eval_executor(self) -> None:\n",
    "        \"\"\"Release async eval thread quickly when stopping.\"\"\"\n",
    "        if self._eval_executor is None:\n",
    "            return\n",
    "        try:\n",
    "            self._eval_executor.shutdown(wait=False, cancel_futures=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self._eval_executor = None\n",
    "\n",
    "    # ---------- client/server API ----------\n",
    "    def get_global(self):\n",
    "        with self._lock:\n",
    "            return state_to_list(self.model.state_dict()), self.t_round\n",
    "\n",
    "    def submit_update(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        base_version: int,\n",
    "        new_params: List[torch.Tensor],\n",
    "        num_samples: int,\n",
    "        train_time_s: float,\n",
    "        train_loss: float,\n",
    "        train_acc: float,\n",
    "        test_loss: float,\n",
    "        test_acc: float,\n",
    "    ) -> None:\n",
    "        cleanup_requested = False\n",
    "        with self._lock:\n",
    "            if self._stop:\n",
    "                return\n",
    "            if self.max_rounds is not None and self.t_round >= self.max_rounds:\n",
    "                self._stop = True\n",
    "                cleanup_requested = True\n",
    "            else:\n",
    "                # FedAsync merge (fixed alpha, still logs staleness for visibility)\n",
    "                staleness = max(0, self.t_round - base_version)\n",
    "                eff = self.alpha\n",
    "\n",
    "                g = state_to_list(self.model.state_dict())\n",
    "                merged = [(1.0 - eff) * gi + eff * ci for gi, ci in zip(g, new_params)]\n",
    "                new_state = list_to_state(self.template, merged)\n",
    "                self.model.load_state_dict(new_state, strict=True)\n",
    "\n",
    "                self.t_round += 1\n",
    "\n",
    "                # Client participation CSV (append staleness like TrustWeight)\n",
    "                with self.participation_csv.open(\"a\", newline=\"\") as f:\n",
    "                    csv.writer(f).writerow(\n",
    "                        [\n",
    "                            client_id,\n",
    "                            f\"{train_loss:.6f}\",\n",
    "                            f\"{train_acc:.6f}\",\n",
    "                            f\"{test_loss:.6f}\",\n",
    "                            f\"{test_acc:.6f}\",\n",
    "                            self.t_round,\n",
    "                            float(staleness),\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                # accumulate metrics since last eval tick (used by optional timer)\n",
    "                self._train_loss_acc_accum.append((float(train_loss), float(train_acc), int(num_samples)))\n",
    "                # Kick off async global eval every eval_every_aggs\n",
    "                if self.t_round % self.eval_every_aggs == 0:\n",
    "                    avg_loss, avg_acc = self._compute_avg_train()\n",
    "                    self._train_loss_acc_accum.clear()\n",
    "                    self._launch_async_eval_if_needed(self.t_round, avg_loss, avg_acc)\n",
    "\n",
    "                # only print aggregation number to console\n",
    "                print(self.t_round)\n",
    "        if cleanup_requested:\n",
    "            self._save_final_model()\n",
    "            self._shutdown_eval_executor()\n",
    "\n",
    "    def should_stop(self) -> bool:\n",
    "        with self._lock:\n",
    "            return self._stop\n",
    "\n",
    "    def mark_stop(self) -> None:\n",
    "        with self._lock:\n",
    "            self._stop = True\n",
    "            # store final model when marking stop\n",
    "            self._save_final_model()\n",
    "        self._shutdown_eval_executor()\n",
    "\n",
    "    # ---------- evaluation / logging ----------\n",
    "    def _compute_avg_train(self) -> Tuple[float, float]:\n",
    "        if not self._train_loss_acc_accum:\n",
    "            return 0.0, 0.0\n",
    "        loss_sum, acc_sum, n_sum = 0.0, 0.0, 0\n",
    "        for l, a, n in self._train_loss_acc_accum:\n",
    "            loss_sum += l * n\n",
    "            acc_sum += a * n\n",
    "            n_sum += n\n",
    "        return loss_sum / max(1, n_sum), acc_sum / max(1, n_sum)\n",
    "\n",
    "    def _init_async_eval_log(self) -> None:\n",
    "        \"\"\"Ensure the async evaluation CSV exists with header.\"\"\"\n",
    "        if self.csv_path.exists():\n",
    "            return\n",
    "        self.csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with self.csv_path.open(\"w\", newline=\"\") as f:\n",
    "            csv.writer(f).writerow(\n",
    "                [\"total_agg\", \"avg_train_loss\", \"avg_train_acc\", \"test_loss\", \"test_acc\", \"time\"]\n",
    "            )\n",
    "\n",
    "    def _launch_async_eval_if_needed(self, total_agg: int, avg_train_loss: float, avg_train_acc: float) -> None:\n",
    "        \"\"\"Schedule a non-blocking global eval every eval_every_aggs aggregations.\"\"\"\n",
    "        if self._eval_executor is None:\n",
    "            return\n",
    "        if total_agg % self.eval_every_aggs != 0:\n",
    "            return\n",
    "        if self._async_eval_future is not None and not self._async_eval_future.done():\n",
    "            return\n",
    "\n",
    "        # snapshot state on current device\n",
    "        state_copy = {k: v.clone() for k, v in self.model.state_dict().items()}\n",
    "        self._async_eval_future = self._eval_executor.submit(\n",
    "            _async_eval_worker,\n",
    "            state_copy,\n",
    "            self.data_dir,\n",
    "            self.num_classes,\n",
    "            str(self.csv_path),\n",
    "            total_agg,\n",
    "            avg_train_loss,\n",
    "            avg_train_acc,\n",
    "            str(self.device),\n",
    "        )\n",
    "        self._async_eval_future.add_done_callback(self._handle_eval_result)\n",
    "\n",
    "    def _handle_eval_result(self, fut: Future) -> None:\n",
    "        try:\n",
    "            test_loss, test_acc = fut.result()\n",
    "        except Exception:\n",
    "            return\n",
    "        with self._lock:\n",
    "            self._last_eval = (float(test_loss), float(test_acc))\n",
    "            if test_acc >= self.target_accuracy:\n",
    "                self._stop = True\n",
    "                self._save_final_model()\n",
    "                self._shutdown_eval_executor()\n",
    "\n",
    "    def wait(self):\n",
    "        try:\n",
    "            while not self.should_stop():\n",
    "                time.sleep(0.2)\n",
    "        finally:\n",
    "            self.mark_stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bfbfa2",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1764519239639,
     "user": {
      "displayName": "Arya Patel",
      "userId": "07088500315875920175"
     },
     "user_tz": 360
    },
    "id": "a4bfbfa2"
   },
   "outputs": [],
   "source": [
    "# ===== client.py =====\n",
    "\n",
    "# Lightning-based local client, no Flower deps\n",
    "import time\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "import io\n",
    "import pytorch_lightning as pl\n",
    "from utils.model import build_resnet18, state_to_list, list_to_state\n",
    "from utils.helper import get_device\n",
    "import random\n",
    "\n",
    "\n",
    "def _device_to_accelerator(device: torch.device) -> str:\n",
    "    if device.type == \"cuda\":\n",
    "        return \"gpu\"\n",
    "    if device.type == \"mps\":\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "\n",
    "def _testloader(root: str, batch_size: int = 256):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    buf = io.StringIO()\n",
    "    # Silence torchvision download/cache prints\n",
    "    with redirect_stdout(buf), redirect_stderr(buf):\n",
    "        ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "def _evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = crit(logits, y)\n",
    "            loss_sum += float(loss.item()) * y.size(0)\n",
    "            total += y.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "    return loss_sum / max(1, total), correct / max(1, total)\n",
    "\n",
    "\n",
    "class LitCifar(pl.LightningModule):\n",
    "    def __init__(self, base_model: nn.Module, lr: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"base_model\"])\n",
    "        self.model = base_model\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self._train_loss_sum = 0.0\n",
    "        self._train_correct = 0\n",
    "        self._train_total = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, _batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        pred = logits.argmax(1)\n",
    "        self._train_loss_sum += float(loss.item()) * y.size(0)\n",
    "        self._train_correct += (pred == y).sum().item()\n",
    "        self._train_total += y.size(0)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self._train_loss_sum = 0.0\n",
    "        self._train_correct = 0\n",
    "        self._train_total = 0\n",
    "\n",
    "    def get_epoch_metrics(self) -> Tuple[float, float]:\n",
    "        if self._train_total == 0:\n",
    "            return 0.0, 0.0\n",
    "        return self._train_loss_sum / self._train_total, self._train_correct / self._train_total\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "\n",
    "class LocalAsyncClient:\n",
    "    \"\"\"Pull global -> Lightning local fit -> push update with averaged metrics.\n",
    "       Supports per-client slow/fast delays with optional per-round jitter.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: int,\n",
    "        cfg: dict,\n",
    "        subset: Subset,\n",
    "        base_delay: float = 0.0,\n",
    "        slow: bool = False,\n",
    "        delay_ranges: Optional[tuple] = None,   # ((a_s, b_s), (a_f, b_f))\n",
    "        jitter: float = 0.0,\n",
    "        fix_delay: bool = True,\n",
    "    ):\n",
    "        self.cid = cid\n",
    "        self.cfg = cfg\n",
    "        self.device = get_device()\n",
    "\n",
    "        base = build_resnet18(num_classes=cfg[\"data\"][\"num_classes\"], pretrained=False)\n",
    "        self.lit = LitCifar(base, lr=float(cfg[\"clients\"][\"lr\"]))\n",
    "\n",
    "        # Rebuild a training subset with CIFAR-style augmentation (keeps partition indices, avoids touching partitioner)\n",
    "        indices = subset.indices if hasattr(subset, \"indices\") else list(range(len(subset)))\n",
    "        train_tfm = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        ])\n",
    "        train_ds = datasets.CIFAR10(cfg[\"data\"][\"data_dir\"], train=True, download=False, transform=train_tfm)\n",
    "        aug_subset = Subset(train_ds, indices)\n",
    "        self.loader = DataLoader(aug_subset, batch_size=int(cfg[\"clients\"][\"batch_size\"]),\n",
    "                                 shuffle=True, num_workers=0)\n",
    "\n",
    "        # delay controls\n",
    "        self.base_delay = float(base_delay)\n",
    "        self.slow = bool(slow)\n",
    "        self.delay_ranges = delay_ranges\n",
    "        self.jitter = float(jitter)\n",
    "        self.fix_delay = bool(fix_delay)\n",
    "\n",
    "        # pre-sample fixed delay if requested\n",
    "        if self.fix_delay and self.delay_ranges is not None:\n",
    "            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n",
    "            if self.slow:\n",
    "                self.base_delay = random.uniform(float(a_s), float(b_s))\n",
    "            else:\n",
    "                self.base_delay = random.uniform(float(a_f), float(b_f))\n",
    "\n",
    "        self.accelerator = _device_to_accelerator(self.device)\n",
    "\n",
    "        # local test loader to compute per-client test metrics\n",
    "        self.testloader = _testloader(cfg[\"data\"][\"data_dir\"])\n",
    "\n",
    "    def _to_list(self) -> List[torch.Tensor]:\n",
    "        return state_to_list(self.lit.model.state_dict())\n",
    "\n",
    "    def _from_list(self, arrs: List[torch.Tensor]) -> None:\n",
    "        sd = self.lit.model.state_dict()\n",
    "        new_sd = list_to_state(sd, arrs)\n",
    "        self.lit.model.load_state_dict(new_sd, strict=True)\n",
    "        self.lit.to(self.device)\n",
    "\n",
    "    def _sleep_delay(self):\n",
    "        # global delay from config (kept for backward compat)\n",
    "        global_d = float(self.cfg.get(\"server_runtime\", {}).get(\"client_delay\", 0.0))\n",
    "\n",
    "        # per-client base delay\n",
    "        base = self.base_delay\n",
    "\n",
    "        # if not fixed, resample each fit\n",
    "        if not self.fix_delay and self.delay_ranges is not None:\n",
    "            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n",
    "            if self.slow:\n",
    "                base = random.uniform(float(a_s), float(b_s))\n",
    "            else:\n",
    "                base = random.uniform(float(a_f), float(b_f))\n",
    "\n",
    "        # add +/- jitter\n",
    "        jit = random.uniform(-self.jitter, self.jitter) if self.jitter > 0.0 else 0.0\n",
    "\n",
    "        delay = max(0.0, global_d + base + jit)\n",
    "        if delay > 0.0:\n",
    "            time.sleep(delay)\n",
    "\n",
    "    def fit_once(self, server) -> bool:\n",
    "        # pull global\n",
    "        params, version = server.get_global()\n",
    "        self._from_list(params)\n",
    "\n",
    "        # emulate heterogeneous device speed\n",
    "        self._sleep_delay()\n",
    "\n",
    "        # train for local_epochs; checkpoints disabled for async runs\n",
    "        epochs = int(self.cfg[\"clients\"][\"local_epochs\"])\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=epochs,\n",
    "            accelerator=self.accelerator,\n",
    "            devices=1,\n",
    "            enable_checkpointing=False,\n",
    "            logger=False,\n",
    "            enable_model_summary=False,\n",
    "            num_sanity_val_steps=0,\n",
    "            enable_progress_bar=False,\n",
    "            callbacks=[],\n",
    "        )\n",
    "        start = time.time()\n",
    "        trainer.fit(self.lit, train_dataloaders=self.loader)\n",
    "        duration = time.time() - start\n",
    "\n",
    "        # local metrics\n",
    "        train_loss, train_acc = self.lit.get_epoch_metrics()\n",
    "        test_loss, test_acc = _evaluate(self.lit.model, self.testloader, self.device)\n",
    "\n",
    "        new_params = self._to_list()\n",
    "        num_examples = len(self.loader.dataset)\n",
    "\n",
    "        server.submit_update(\n",
    "            client_id=self.cid,\n",
    "            base_version=version,\n",
    "            new_params=new_params,\n",
    "            num_samples=num_examples,\n",
    "            train_time_s=duration,\n",
    "            train_loss=train_loss,\n",
    "            train_acc=train_acc,\n",
    "            test_loss=test_loss,\n",
    "            test_acc=test_acc,\n",
    "        )\n",
    "        return not server.should_stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d9871a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85d9871a",
    "outputId": "af20d477-34d0-4090-bd08-540b2742e1bb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== run.py ====\n",
    "\n",
    "# Orchestrator: partitions data, starts server, runs Lightning clients\n",
    "import os\n",
    "# ---- silence libraries before anything else ----\n",
    "import logging, warnings\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "os.environ[\"LIGHTNING_DISABLE_RICH\"] = \"1\"\n",
    "for name in [\n",
    "    \"pytorch_lightning\", \"lightning\", \"lightning.pytorch\",\n",
    "    \"lightning_fabric\", \"lightning_utilities\", \"torch\", \"torchvision\",\n",
    "]:\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "    logging.getLogger(name).propagate = False\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import time\n",
    "from typing import Dict, Any, List\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "CFG_PATH = os.environ.get(\"FEDASYNC_CONFIG\", str(Path(BASE_DIR) / \"config_run.yaml\"))\n",
    "\n",
    "\n",
    "def load_cfg(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = load_cfg(CFG_PATH)\n",
    "\n",
    "    # Reproducibility\n",
    "    seed = int(cfg.get(\"seed\", 42))\n",
    "    set_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Partition dataset\n",
    "    dd = DataDistributor(dataset_name=cfg[\"data\"][\"dataset\"], data_dir=cfg[\"data\"][\"data_dir\"])\n",
    "    dd.distribute_data(\n",
    "        num_clients=int(cfg[\"clients\"][\"total\"]),\n",
    "        alpha=float(cfg.get(\"partition_alpha\", 0.5)),\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Build server with periodic eval/log and accuracy-based stopping\n",
    "    global_model = build_resnet18(num_classes=cfg[\"data\"][\"num_classes\"], pretrained=False)\n",
    "    server = AsyncFedServer(\n",
    "        global_model=global_model,\n",
    "        alpha=float(cfg[\"async\"][\"alpha\"]),\n",
    "        target_accuracy=float(cfg[\"eval\"][\"target_accuracy\"]),\n",
    "        max_rounds=int(cfg[\"train\"][\"max_rounds\"]) if \"max_rounds\" in cfg[\"train\"] else None,\n",
    "        eval_every_aggs=int(cfg[\"eval\"].get(\"eval_every_aggs\", 5)),\n",
    "        data_dir=cfg[\"data\"][\"data_dir\"],\n",
    "        logs_dir=cfg[\"io\"][\"logs_dir\"],\n",
    "        global_log_csv=cfg[\"io\"].get(\"global_log_csv\"),\n",
    "        client_participation_csv=cfg[\"io\"].get(\"client_participation_csv\"),\n",
    "        final_model_path=cfg[\"io\"].get(\"final_model_path\"),\n",
    "        num_classes=int(cfg[\"data\"][\"num_classes\"]),\n",
    "        device=get_device(),\n",
    "    )\n",
    "\n",
    "    # ---- derive per-client delays to simulate heterogeneity ----\n",
    "    n = int(cfg[\"clients\"][\"total\"])\n",
    "    pct = max(0, min(100, int(cfg[\"clients\"].get(\"struggle_percent\", 0))))\n",
    "    k_slow = (n * pct) // 100\n",
    "    slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
    "\n",
    "    a_s, b_s = cfg[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
    "    a_f, b_f = cfg[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
    "    fix_delays = bool(cfg[\"clients\"].get(\"fix_delays_per_client\", True))\n",
    "    jitter = float(cfg[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
    "\n",
    "    per_client_base_delay: Dict[int, float] = {}\n",
    "    if fix_delays:\n",
    "        for cid in range(n):\n",
    "            if cid in slow_ids:\n",
    "                per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
    "            else:\n",
    "                per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
    "\n",
    "    # Create clients\n",
    "    clients: List[LocalAsyncClient] = []\n",
    "    for cid in range(n):\n",
    "        subset = dd.get_client_data(cid)\n",
    "        base_delay = per_client_base_delay.get(cid, 0.0)\n",
    "        is_slow = cid in slow_ids\n",
    "        clients.append(LocalAsyncClient(\n",
    "            cid=cid,\n",
    "            cfg=cfg,\n",
    "            subset=subset,\n",
    "            base_delay=base_delay,\n",
    "            slow=is_slow,\n",
    "            delay_ranges=((float(a_s), float(b_s)), (float(a_f), float(b_f))),\n",
    "            jitter=jitter,\n",
    "            fix_delay=fix_delays,\n",
    "        ))\n",
    "\n",
    "    # Concurrency gate via thread pool\n",
    "    def client_loop(client: LocalAsyncClient):\n",
    "        try:\n",
    "            while not server.should_stop():\n",
    "                cont = client.fit_once(server)\n",
    "                if not cont:\n",
    "                    break\n",
    "                time.sleep(0.05)\n",
    "        except Exception:\n",
    "            # ensure the orchestrator stops if any client fails\n",
    "            server.mark_stop()\n",
    "            raise\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=int(cfg[\"clients\"][\"concurrent\"])) as executor:\n",
    "        futures = [executor.submit(client_loop, cl) for cl in clients]\n",
    "        try:\n",
    "            while not server.should_stop():\n",
    "                if all(f.done() for f in futures):\n",
    "                    # all clients exited (success or error) -> stop server loop\n",
    "                    server.mark_stop()\n",
    "                    break\n",
    "                time.sleep(0.2)\n",
    "        finally:\n",
    "            server.mark_stop()\n",
    "            for f in futures:\n",
    "                f.result()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea7b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== experiment.py =====\n",
    "\n",
    "# Straggler sweep runner for FedAsync\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Iterable, List\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import yaml\n",
    "\n",
    "\n",
    "def load_cfg(path: str) -> Dict[str, Any]:\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "\n",
    "def _override_io(cfg: Dict[str, Any], exp_dir: Path) -> Dict[str, Any]:\n",
    "    cfg = deepcopy(cfg)\n",
    "    exp_dir = exp_dir.resolve()\n",
    "    cfg[\"io\"][\"logs_dir\"] = str(exp_dir)\n",
    "    cfg[\"io\"][\"checkpoints_dir\"] = str(exp_dir / \"checkpoints\")\n",
    "    cfg[\"io\"][\"results_dir\"] = str(exp_dir / \"results\")\n",
    "    cfg[\"io\"][\"global_log_csv\"] = str(exp_dir / \"FedAsync.csv\")\n",
    "    cfg[\"io\"][\"client_participation_csv\"] = str(exp_dir / \"FedAsyncClientParticipation.csv\")\n",
    "    cfg[\"io\"][\"final_model_path\"] = str(exp_dir / \"results\" / \"FedAsyncModel.pt\")\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def run_once(cfg: Dict[str, Any]) -> None:\n",
    "    # Silence noisy logs\n",
    "    os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "    os.environ[\"LIGHTNING_DISABLE_RICH\"] = \"1\"\n",
    "    for name in [\n",
    "        \"pytorch_lightning\", \"lightning\", \"lightning.pytorch\",\n",
    "        \"lightning_fabric\", \"lightning_utilities\", \"torch\", \"torchvision\",\n",
    "    ]:\n",
    "        logging.getLogger(name).setLevel(logging.ERROR)\n",
    "        logging.getLogger(name).propagate = False\n",
    "    logging.getLogger().setLevel(logging.WARNING)\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "    # Reproducibility\n",
    "    seed = int(cfg.get(\"seed\", 42))\n",
    "    set_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Partition dataset\n",
    "    dd = DataDistributor(dataset_name=cfg[\"data\"][\"dataset\"], data_dir=cfg[\"data\"][\"data_dir\"])\n",
    "    dd.distribute_data(\n",
    "        num_clients=int(cfg[\"clients\"][\"total\"]),\n",
    "        alpha=float(cfg.get(\"partition_alpha\", 0.5)),\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    # Build server\n",
    "    global_model = build_resnet18(num_classes=cfg[\"data\"][\"num_classes\"], pretrained=False)\n",
    "    server = AsyncFedServer(\n",
    "        global_model=global_model,\n",
    "        alpha=float(cfg[\"async\"][\"alpha\"]),\n",
    "        target_accuracy=float(cfg[\"eval\"][\"target_accuracy\"]),\n",
    "        max_rounds=int(cfg[\"train\"][\"max_rounds\"]) if \"max_rounds\" in cfg[\"train\"] else None,\n",
    "        eval_every_aggs=int(cfg[\"eval\"].get(\"eval_every_aggs\", 5)),\n",
    "        data_dir=cfg[\"data\"][\"data_dir\"],\n",
    "        logs_dir=cfg[\"io\"][\"logs_dir\"],\n",
    "        global_log_csv=cfg[\"io\"].get(\"global_log_csv\"),\n",
    "        client_participation_csv=cfg[\"io\"].get(\"client_participation_csv\"),\n",
    "        final_model_path=cfg[\"io\"].get(\"final_model_path\"),\n",
    "        num_classes=int(cfg[\"data\"][\"num_classes\"]),\n",
    "        device=get_device(),\n",
    "    )\n",
    "\n",
    "    # Straggler sampling\n",
    "    n = int(cfg[\"clients\"][\"total\"])\n",
    "    pct = max(0, min(100, int(cfg[\"clients\"].get(\"struggle_percent\", 0))))\n",
    "    k_slow = (n * pct) // 100\n",
    "    slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
    "\n",
    "    a_s, b_s = cfg[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
    "    a_f, b_f = cfg[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
    "    fix_delays = bool(cfg[\"clients\"].get(\"fix_delays_per_client\", True))\n",
    "    jitter = float(cfg[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
    "\n",
    "    per_client_base_delay: Dict[int, float] = {}\n",
    "    if fix_delays:\n",
    "        for cid in range(n):\n",
    "            if cid in slow_ids:\n",
    "                per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
    "            else:\n",
    "                per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
    "\n",
    "    # Clients\n",
    "    clients: List[LocalAsyncClient] = []\n",
    "    for cid in range(n):\n",
    "        subset = dd.get_client_data(cid)\n",
    "        base_delay = per_client_base_delay.get(cid, 0.0)\n",
    "        is_slow = cid in slow_ids\n",
    "        clients.append(LocalAsyncClient(\n",
    "            cid=cid,\n",
    "            cfg=cfg,\n",
    "            subset=subset,\n",
    "            base_delay=base_delay,\n",
    "            slow=is_slow,\n",
    "            delay_ranges=((float(a_s), float(b_s)), (float(a_f), float(b_f))),\n",
    "            jitter=jitter,\n",
    "            fix_delay=fix_delays,\n",
    "        ))\n",
    "\n",
    "    def client_loop(client: LocalAsyncClient):\n",
    "        try:\n",
    "            while not server.should_stop():\n",
    "                cont = client.fit_once(server)\n",
    "                if not cont:\n",
    "                    break\n",
    "                time.sleep(0.05)\n",
    "        except Exception:\n",
    "            server.mark_stop()\n",
    "            raise\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=int(cfg[\"clients\"][\"concurrent\"])) as executor:\n",
    "        futures = [executor.submit(client_loop, cl) for cl in clients]\n",
    "        try:\n",
    "            while not server.should_stop():\n",
    "                if all(f.done() for f in futures):\n",
    "                    server.mark_stop()\n",
    "                    break\n",
    "                time.sleep(0.2)\n",
    "        finally:\n",
    "            server.mark_stop()\n",
    "            for f in futures:\n",
    "                f.result()\n",
    "\n",
    "\n",
    "def straggler_sweep(\n",
    "    base_cfg: Dict[str, Any],\n",
    "    percents: Iterable[int],\n",
    "    out_root: Path,\n",
    ") -> None:\n",
    "    for pct in percents:\n",
    "        exp_dir = out_root / f\"straggle_{pct}pct\"\n",
    "        cfg = deepcopy(base_cfg)\n",
    "        cfg[\"clients\"][\"struggle_percent\"] = int(pct)\n",
    "        cfg = _override_io(cfg, exp_dir)\n",
    "        exp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"[straggler_sweep] percent={pct}% -> logs at {exp_dir}\")\n",
    "        run_once(cfg)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cfg_path = os.environ.get(\"FEDASYNC_CONFIG\", str(Path(BASE_DIR) / \"config.yaml\"))\n",
    "    base = load_cfg(cfg_path)\n",
    "    out_root = Path(base[\"io\"][\"logs_dir\"]) / \"FedAsyncStragglerExp\"\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "    straggler_sweep(base, percents=[10, 20, 30, 40, 50], out_root=out_root)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}