{"cells":[{"cell_type":"markdown","id":"9d189811","metadata":{"id":"9d189811"},"source":["# TrustWeight Async Federated Learning â€” Google Colab Version\n","\n","This notebook is organized with one cell per original source file,\n","and is optimized for running on **Google Colab** with all results\n","stored on **Google Drive**.\n","\n","**How to use on Colab:**\n","\n","1. Upload this notebook to Colab.\n","2. Run the first cell to mount Google Drive and set your base path.\n","3. All subsequent paths (data, logs, models, results) will be created\n","   relative to that base directory.\n"]},{"cell_type":"code","source":["! pip install pytorch_lightning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6_6oPCNeefj","executionInfo":{"status":"ok","timestamp":1764519236346,"user_tz":360,"elapsed":4861,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}},"outputId":"ce49bcf2-03cb-4981-ce7b-a5b01705ede8"},"id":"P6_6oPCNeefj","execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.12/dist-packages (2.6.0)\n","Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (2.9.0+cu126)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (4.67.1)\n","Requirement already satisfied: PyYAML>5.4 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (6.0.3)\n","Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2025.3.0)\n","Requirement already satisfied: torchmetrics>0.7.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (1.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (25.0)\n","Requirement already satisfied: typing-extensions>4.5.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (4.15.0)\n","Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from pytorch_lightning) (0.15.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.13.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (75.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.20.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1.0->pytorch_lightning) (3.5.0)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics>0.7.0->pytorch_lightning) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.22.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1.0->pytorch_lightning) (3.0.3)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (3.11)\n"]}]},{"cell_type":"code","execution_count":52,"id":"1ce9788e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ce9788e","executionInfo":{"status":"ok","timestamp":1764519236752,"user_tz":360,"elapsed":405,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}},"outputId":"5f0df291-df6a-4568-d2e9-479dd6b42cbd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Working directory set to: /content/drive/MyDrive/TrustWeightColab\n"]}],"source":["\n","# @title Mount Google Drive and set base project directory\n","from google.colab import drive\n","import os\n","from pathlib import Path\n","\n","# Mount your Google Drive\n","drive.mount('/content/drive')\n","\n","# Base directory INSIDE your Drive where everything will be stored.\n","# You can change this to any folder you like.\n","BASE_DIR = \"/content/drive/MyDrive/TrustWeightColab\"  # @param {type:\"string\"}\n","\n","BASE_PATH = Path(BASE_DIR).expanduser()\n","BASE_PATH.mkdir(parents=True, exist_ok=True)\n","os.chdir(BASE_PATH)\n","os.environ[\"TRUSTWEIGHT_BASE_DIR\"] = str(BASE_PATH)\n","print(\"Working directory set to:\", os.getcwd())\n"]},{"cell_type":"markdown","id":"80024cb2","metadata":{"id":"80024cb2"},"source":["## config.yaml\n","\n","Below is the YAML configuration used by the code.\n"]},{"cell_type":"code","execution_count":53,"id":"e0704549","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0704549","executionInfo":{"status":"ok","timestamp":1764519236763,"user_tz":360,"elapsed":9,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}},"outputId":"9c3bc099-b738-4389-91ca-58d565b45332"},"outputs":[{"output_type":"stream","name":"stdout","text":["config.yaml written to /content/drive/MyDrive/TrustWeightColab/config.yaml\n"]}],"source":["\n","from pathlib import Path\n","import textwrap\n","\n","config_path = Path(BASE_DIR) / \"config.yaml\"\n","config_text = f\"\"\"\n","data:\n","  dataset: \"cifar10\"\n","  data_dir: \"{(Path(BASE_DIR) / 'data').as_posix()}\"\n","  num_classes: 10\n","\n","clients:\n","  total: 20\n","  concurrent: 10\n","  local_epochs: 1\n","  batch_size: 128\n","  lr: 0.005\n","  weight_decay: 0.001\n","  grad_clip: 5.0\n","\n","  # Straggler simulation\n","  struggle_percent: 0         # % of clients treated as slow\n","  delay_slow_range: [0.8, 2.0]  # seconds\n","  delay_fast_range: [0.0, 0.2]\n","  jitter_per_round: 0.05\n","  fix_delays_per_client: true   # keep each client's slow/fast identity\n","\n","eval:\n","  interval_seconds: 15          # (not used for logging now but kept for compatibility)\n","  target_accuracy: 0.80         # stop training when achieved\n","\n","train:\n","  max_rounds: 500              # stop after this many aggregations\n","  update_clip_norm: 5.0          # clip oversized update vectors before aggregation\n","\n","partition_alpha: 1000            # Dirichlet Î± for non-IID partitioning\n","seed: 42\n","\n","server_runtime:\n","  client_delay: 0.0             # fixed delay added to each client\n","\n","io:\n","  checkpoints_dir: \"{(Path(BASE_DIR) / 'checkpoints' / 'TrustWeight').as_posix()}\"\n","  logs: \"{(Path(BASE_DIR) / 'logs').as_posix()}\"\n","  results: \"{(Path(BASE_DIR) / 'results').as_posix()}\"\n","  # Global server-level training log (one row per aggregation)\n","  global_log_csv: \"{(Path(BASE_DIR) / 'logs' / 'TrustWeight.csv').as_posix()}\"\n","  # Per-client participation log (one row per client per aggregation)\n","  client_participation_csv: \"{(Path(BASE_DIR) / 'logs' / 'TrustWeightClientParticipation.csv').as_posix()}\"\n","  final_model_path: \"{(Path(BASE_DIR) / 'results' / 'TrustWeightModel.pt').as_posix()}\"\n","\"\"\"\n","\n","config_path.write_text(textwrap.dedent(config_text).lstrip())\n","print(f\"config.yaml written to {config_path}\")\n"]},{"cell_type":"code","execution_count":54,"id":"0fed7c69","metadata":{"id":"0fed7c69","executionInfo":{"status":"ok","timestamp":1764519236792,"user_tz":360,"elapsed":27,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}}},"outputs":[],"source":["\n","# ===== config.py =====\n","\n","from __future__ import annotations\n","\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Any, Dict, Tuple\n","import os\n","\n","import yaml\n","\n","\n","# ----------------------------- base paths ------------------------------------\n","BASE_PATH = Path(\n","    os.environ.get(\"TRUSTWEIGHT_BASE_DIR\", globals().get(\"BASE_DIR\", \".\"))\n",").expanduser()\n","BASE_PATH.mkdir(parents=True, exist_ok=True)\n","\n","\n","def _resolve_to_base(path_like: str | Path) -> str:\n","    \"\"\"Return absolute path under BASE_PATH unless already absolute.\"\"\"\n","    p = Path(path_like)\n","    if p.is_absolute():\n","        return str(p)\n","    return str(BASE_PATH / p)\n","\n","\n","# ----------------------------- dataclasses -----------------------------------\n","\n","\n","@dataclass\n","class DataConfig:\n","    dataset: str\n","    data_dir: str\n","    num_classes: int\n","\n","\n","@dataclass\n","class ClientsConfig:\n","    total: int\n","    concurrent: int\n","    local_epochs: int\n","    batch_size: int\n","    lr: float\n","    weight_decay: float\n","    grad_clip: float\n","    struggle_percent: int\n","    delay_slow_range: Tuple[float, float]\n","    delay_fast_range: Tuple[float, float]\n","    jitter_per_round: float\n","    fix_delays_per_client: bool\n","\n","\n","@dataclass\n","class EvalConfig:\n","    interval_seconds: int\n","    target_accuracy: float\n","\n","\n","@dataclass\n","class TrainConfig:\n","    max_rounds: int\n","    update_clip_norm: float\n","\n","\n","@dataclass\n","class ServerRuntimeConfig:\n","    client_delay: float\n","\n","\n","@dataclass\n","class IOConfig:\n","    checkpoints_dir: str\n","    logs_dir: str\n","    results_dir: str\n","    global_log_csv: str\n","    client_participation_csv: str\n","    final_model_path: str\n","\n","\n","@dataclass\n","class GlobalConfig:\n","    data: DataConfig\n","    clients: ClientsConfig\n","    eval: EvalConfig\n","    train: TrainConfig\n","    partition_alpha: float\n","    seed: int\n","    server_runtime: ServerRuntimeConfig\n","    io: IOConfig\n","\n","\n","# ----------------------------- loader ----------------------------------------\n","\n","\n","def _as_tuple(x: Any) -> Tuple[float, float]:\n","    if isinstance(x, (list, tuple)) and len(x) == 2:\n","        return float(x[0]), float(x[1])\n","    raise ValueError(f\"Expected length-2 sequence, got {x!r}\")\n","\n","\n","def load_config(path: str | Path | None = None) -> GlobalConfig:\n","    cfg_path = Path(path) if path is not None else BASE_PATH / \"config.yaml\"\n","    with cfg_path.open(\"r\") as f:\n","        raw: Dict[str, Any] = yaml.safe_load(f)\n","\n","    data_cfg = DataConfig(\n","        dataset=raw[\"data\"][\"dataset\"],\n","        data_dir=_resolve_to_base(raw[\"data\"][\"data_dir\"]),\n","        num_classes=int(raw[\"data\"][\"num_classes\"]),\n","    )\n","\n","    clients_section = raw[\"clients\"]\n","    clients_cfg = ClientsConfig(\n","        total=int(clients_section[\"total\"]),\n","        concurrent=int(clients_section[\"concurrent\"]),\n","        local_epochs=int(clients_section[\"local_epochs\"]),\n","        batch_size=int(clients_section[\"batch_size\"]),\n","        lr=float(clients_section[\"lr\"]),\n","        weight_decay=float(clients_section.get(\"weight_decay\", 5e-4)),\n","        grad_clip=float(clients_section.get(\"grad_clip\", 5.0)),\n","        struggle_percent=int(clients_section.get(\"struggle_percent\", 0)),\n","        delay_slow_range=_as_tuple(clients_section.get(\"delay_slow_range\", [0.8, 2.0])),\n","        delay_fast_range=_as_tuple(clients_section.get(\"delay_fast_range\", [0.0, 0.2])),\n","        jitter_per_round=float(clients_section.get(\"jitter_per_round\", 0.0)),\n","        fix_delays_per_client=bool(clients_section.get(\"fix_delays_per_client\", True)),\n","    )\n","\n","    eval_cfg = EvalConfig(\n","        interval_seconds=int(raw[\"eval\"][\"interval_seconds\"]),\n","        target_accuracy=float(raw[\"eval\"][\"target_accuracy\"]),\n","    )\n","\n","    train_cfg = TrainConfig(\n","        max_rounds=int(raw[\"train\"][\"max_rounds\"]),\n","        update_clip_norm=float(raw[\"train\"].get(\"update_clip_norm\", 10.0)),\n","    )\n","\n","    server_runtime_cfg = ServerRuntimeConfig(\n","        client_delay=float(raw[\"server_runtime\"].get(\"client_delay\", 0.0))\n","    )\n","\n","    io_section = raw[\"io\"]\n","    io_cfg = IOConfig(\n","        checkpoints_dir=_resolve_to_base(io_section[\"checkpoints_dir\"]),\n","        logs_dir=_resolve_to_base(io_section[\"logs\"]),\n","        results_dir=_resolve_to_base(io_section[\"results\"]),\n","        global_log_csv=_resolve_to_base(io_section[\"global_log_csv\"]),\n","        client_participation_csv=_resolve_to_base(io_section[\"client_participation_csv\"]),\n","        final_model_path=_resolve_to_base(io_section[\"final_model_path\"]),\n","    )\n","\n","    cfg = GlobalConfig(\n","        data=data_cfg,\n","        clients=clients_cfg,\n","        eval=eval_cfg,\n","        train=train_cfg,\n","        partition_alpha=float(raw.get(\"partition_alpha\", 0.5)),\n","        seed=int(raw.get(\"seed\", 42)),\n","        server_runtime=server_runtime_cfg,\n","        io=io_cfg,\n","    )\n","    return cfg\n"]},{"cell_type":"code","execution_count":55,"id":"d829789e","metadata":{"id":"d829789e","executionInfo":{"status":"ok","timestamp":1764519236795,"user_tz":360,"elapsed":1,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}}},"outputs":[],"source":["# ===== helper.py =====\n","\n","from __future__ import annotations\n","\n","import random\n","import numpy as np\n","import torch\n","\n","\n","def set_seed(seed: int = 42) -> None:\n","    \"\"\"Seed all RNGs used in this project.\n","\n","    Setting a global seed helps to ensure reproducible results.  This\n","    function touches Python's builtâ€‘in random module, NumPy, and\n","    PyTorch's CPU and GPU RNGs.  Deterministic behaviour in cuDNN\n","    kernels is also enabled.\n","\n","    Parameters\n","    ----------\n","    seed:\n","        The random seed to use.  Defaults to ``42``.\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    # When running on GPUs you may have more than one device\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    # Deterministic behaviour comes at a performance cost but\n","    # reproducibility is more important for experimentation.\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def get_device() -> torch.device:\n","    \"\"\"Return the first available computation device.\n","\n","    Tries to use CUDA if available, otherwise falls back to MPS\n","    (Apple Silicon) and finally the CPU.\n","\n","    Returns\n","    -------\n","    device:\n","        A PyTorch ``torch.device`` object indicating where tensors\n","        should be allocated.\n","    \"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device(\"cuda\")\n","    # MPS stands for Metal Performance Shaders.  It is the backend\n","    # available on Apple Silicon systems for GPU acceleration.\n","    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","        return torch.device(\"mps\")\n","    return torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":56,"id":"e47d19aa","metadata":{"id":"e47d19aa","executionInfo":{"status":"ok","timestamp":1764519236824,"user_tz":360,"elapsed":1,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}}},"outputs":[],"source":["# ===== model.py =====\n","\n","# Utilities for building models and converting parameters\n","from typing import Dict, List\n","import torch\n","import torch.nn as nn\n","from torchvision import models\n","\n","\n","def build_resnet18(num_classes: int = 10, pretrained: bool = False) -> nn.Module:\n","    \"\"\"Create a ResNet-18 tailored for CIFAR-size inputs.\"\"\"\n","    m = models.resnet18(weights=None)\n","    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","    m.maxpool = nn.Identity()\n","\n","    in_features = m.fc.in_features\n","    m.fc = nn.Linear(in_features, num_classes)\n","    m.num_classes = num_classes\n","    return m\n","\n","\n","def state_to_list(state: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n","    \"\"\"Flatten a state_dict to a list of tensors on CPU.\"\"\"\n","    return [t.detach().cpu().clone() for _, t in state.items()]\n","\n","\n","def list_to_state(template: Dict[str, torch.Tensor], arrs: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n","    \"\"\"Rebuild a state_dict from a list of tensors using a template for keys/dtypes/devices.\"\"\"\n","    out: Dict[str, torch.Tensor] = {}\n","    for (k, v), a in zip(template.items(), arrs):\n","        out[k] = a.to(v.device).type_as(v)\n","    return out\n"]},{"cell_type":"code","execution_count":57,"id":"aed7492f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aed7492f","executionInfo":{"status":"ok","timestamp":1764519239589,"user_tz":360,"elapsed":2764,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}},"outputId":"b177bb85-13b8-4847-b1c0-d30ead4cb92a"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ“Š Data distribution plot saved at: /content/drive/MyDrive/TrustWeightColab/results/cifar10_distribution_ieee.png\n","âœ… Client 0 has 13080 samples.\n"]}],"source":["\n","# ===== partitioning.py =====\n","\n","import os\n","from pathlib import Path\n","import numpy as np\n","import torch\n","from torch.utils.data import Subset\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","from typing import Dict, List, Tuple, Any\n","\n","\n","BASE_PATH = Path(os.environ.get(\"TRUSTWEIGHT_BASE_DIR\", \".\")).expanduser()\n","DEFAULT_DATA_DIR = str(BASE_PATH / \"data\")\n","DEFAULT_RESULTS_DIR = BASE_PATH / \"results\"\n","\n","\n","class DataDistributor:\n","    def __init__(self, dataset_name: str, data_dir: str = DEFAULT_DATA_DIR):\n","        \"\"\"\n","        Flexible data distributor for federated learning experiments.\n","\n","        Args:\n","            dataset_name (str): Name of dataset ('CIFAR10', 'MNIST', etc.)\n","            data_dir (str): Directory to store data.\n","        \"\"\"\n","        self.dataset_name = dataset_name.lower()\n","        data_root = Path(data_dir)\n","        if not data_root.is_absolute():\n","            data_root = BASE_PATH / data_root\n","        self.data_dir = str(data_root)\n","        self.train_dataset, self.test_dataset, self.num_classes = self._load_dataset()\n","        self.partitions = None\n","\n","    def _load_dataset(self) -> Tuple[Any, Any, int]:\n","        \"\"\"Load supported torchvision datasets.\"\"\"\n","        transform = transforms.Compose([transforms.ToTensor()])\n","\n","        if self.dataset_name == \"cifar10\":\n","            train = datasets.CIFAR10(self.data_dir, train=True, download=True, transform=transform)\n","            test = datasets.CIFAR10(self.data_dir, train=False, download=True, transform=transform)\n","            num_classes = 10\n","\n","        elif self.dataset_name == \"mnist\":\n","            train = datasets.MNIST(self.data_dir, train=True, download=True, transform=transform)\n","            test = datasets.MNIST(self.data_dir, train=False, download=True, transform=transform)\n","            num_classes = 10\n","\n","        elif self.dataset_name == \"fashionmnist\":\n","            train = datasets.FashionMNIST(self.data_dir, train=True, download=True, transform=transform)\n","            test = datasets.FashionMNIST(self.data_dir, train=False, download=True, transform=transform)\n","            num_classes = 10\n","\n","        else:\n","            raise ValueError(f\"Dataset '{self.dataset_name}' is not supported yet.\")\n","\n","        return train, test, num_classes\n","\n","    def distribute_data(self, num_clients: int, alpha: float = 0.5, seed: int = 42) -> Dict[int, List[int]]:\n","        \"\"\"\n","        Perform Dirichlet-based data partitioning across clients (Non-IID).\n","\n","        Args:\n","            num_clients (int): Number of clients.\n","            alpha (float): Dirichlet distribution parameter (smaller = more non-IID).\n","            seed (int): Random seed for reproducibility.\n","        \"\"\"\n","        np.random.seed(seed)\n","        targets = np.array(self.train_dataset.targets)\n","        self.partitions = {i: [] for i in range(num_clients)}\n","\n","        for cls in range(self.num_classes):\n","            idxs = np.where(targets == cls)[0]\n","            # Shuffle indices for this class\n","            np.random.shuffle(idxs)\n","            # Sample proportions from a Dirichlet distribution\n","            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n","            # Convert proportions to integer counts (floor) for each client\n","            int_props = np.floor(proportions * len(idxs)).astype(int)\n","            # Assign counts to clients\n","            start = 0\n","            for client_id, size in enumerate(int_props):\n","                self.partitions[client_id].extend(idxs[start:start + size])\n","                start += size\n","            # If any samples are left over due to floor truncation, assign them\n","            # to clients with the largest initial share (or random if equal).  This\n","            # ensures that the union of partitions covers the full dataset.\n","            remaining = len(idxs) - start\n","            if remaining > 0:\n","                # Rank clients by proportion (descending); break ties randomly\n","                ranked_clients = np.argsort(-proportions)\n","                # Distribute leftover samples in roundâ€‘robin order among ranked clients\n","                for i in range(remaining):\n","                    cid = ranked_clients[i % len(ranked_clients)]\n","                    self.partitions[int(cid)].append(idxs[start + i])\n","\n","        for cid in self.partitions:\n","            np.random.shuffle(self.partitions[cid])\n","\n","        return self.partitions\n","\n","    # ... rest of partitioning.py remains unchanged ...\n","\n","\n","    def get_client_data(self, client_id: int) -> Subset:\n","        \"\"\"\n","        Retrieve dataset subset for a specific client.\n","\n","        Args:\n","            client_id (int): Client identifier.\n","        \"\"\"\n","        if self.partitions is None:\n","            raise ValueError(\"Please run distribute_data() before accessing client data.\")\n","        indices = self.partitions[client_id]\n","        return Subset(self.train_dataset, indices)\n","\n","    def visualize_distribution(self, save_path: str | None = None) -> None:\n","        \"\"\"\n","        Create IEEE-style stacked bar chart of sample counts per client.\n","\n","        Args:\n","            save_path (str | None): File path to save the visualization.\n","        \"\"\"\n","        if self.partitions is None:\n","            raise ValueError(\"Run distribute_data() before visualization.\")\n","\n","        path_obj = Path(save_path) if save_path is not None else DEFAULT_RESULTS_DIR / \"data_distribution_ieee.png\"\n","        if not path_obj.is_absolute():\n","            path_obj = BASE_PATH / path_obj\n","        path_obj.parent.mkdir(parents=True, exist_ok=True)\n","\n","        targets = np.array(self.train_dataset.targets)\n","        client_counts = np.zeros((len(self.partitions), self.num_classes), dtype=int)\n","\n","        for cid, idxs in self.partitions.items():\n","            class_counts = np.bincount(targets[idxs], minlength=self.num_classes)\n","            client_counts[cid, :] = class_counts\n","\n","        # IEEE single-column figure size (~3.5in wide)\n","        fig, ax = plt.subplots(figsize=(1.8, 1.2), dpi=300)\n","        bottom = np.zeros(len(self.partitions))\n","        colors = plt.get_cmap(\"tab20\").colors\n","\n","        for cls in range(self.num_classes):\n","            ax.bar(\n","                x=np.arange(len(self.partitions)),\n","                height=client_counts[:, cls],\n","                bottom=bottom,\n","                color=colors[cls % len(colors)],\n","                linewidth=0.1,\n","                edgecolor=\"white\",\n","            )\n","            bottom += client_counts[:, cls]\n","\n","        ax.set_xlabel(\"Client ID\", fontsize=8)\n","        ax.set_ylabel(\"Samples\", fontsize=8)\n","        ax.set_title(f\"{self.dataset_name.upper()} Data Distribution Among Clients\", fontsize=9)\n","        ax.tick_params(axis=\"both\", which=\"major\", labelsize=7)\n","\n","        fig.savefig(path_obj, dpi=300, bbox_inches=\"tight\")\n","        plt.close(fig)\n","        print(f\"ðŸ“Š Data distribution plot saved at: {path_obj}\")\n","\n","\n","# -------------------------------\n","# Example Usage (for testing)\n","# -------------------------------\n","if __name__ == \"__main__\":\n","    distributor = DataDistributor(dataset_name=\"CIFAR10\", data_dir=DEFAULT_DATA_DIR)\n","    distributor.distribute_data(num_clients=5, alpha=0.3, seed=42)\n","    distributor.visualize_distribution(DEFAULT_RESULTS_DIR / \"cifar10_distribution_ieee.png\")\n","\n","    # Retrieve client dataset subset\n","    client_data = distributor.get_client_data(0)\n","    print(f\"âœ… Client 0 has {len(client_data)} samples.\")\n"]},{"cell_type":"code","execution_count":58,"id":"d15dbb2e","metadata":{"id":"d15dbb2e","executionInfo":{"status":"ok","timestamp":1764519239592,"user_tz":360,"elapsed":11,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}}},"outputs":[],"source":["# ===== strategy.py =====\n","\n","# Trust-weighted asynchronous aggregation strategy implementing the PDF math\n","from __future__ import annotations\n","\n","from dataclasses import dataclass\n","from typing import Dict, List, Tuple\n","\n","import torch\n","\n","\n","@dataclass\n","class TrustWeightedConfig:\n","    eta: float = 1.0              # server learning rate Î·\n","    eps: float = 1e-8             # numerical stability Îµ\n","    freshness_alpha: float = 0.1  # Î± in s(Ï„) = exp(-Î± Ï„)\n","    beta1: float = 0.0            # Guard term coefficient on staleness\n","    beta2: float = 0.0            # Guard term coefficient on ||u||\n","    momentum_gamma: float = 0.9   # update factor for m_t\n","    theta: Tuple[float, float, float] = (0.0, 0.0, 0.0)  # quality weights\n","\n","\n","class TrustWeightedAsyncStrategy:\n","    \"\"\"Implements the aggregation rule described in the DML solution PDF.\n","\n","    Core formula:\n","\n","        w_{t+1} = w_t + Î· * Î£_i Weight_i *\n","            [ Proj_m_t(u_i) + Guard_i * (u_i - Proj_m_t(u_i)) ]\n","\n","    with:\n","\n","        Proj_m_t(u_i) = <u_i, m_t> / (||m_t||^2 + eps) * m_t\n","        Guard_i = 1 / (1 + Î²1 * Ï„_i + Î²2 * ||u_i||)\n","        Weight_i âˆ s(Ï„_i) * exp(Î¸áµ€ [Î”LÌƒ_i, ||u_i||, cos(u_i, m_t)]) * (n_i / Î£_j n_j)\n","    \"\"\"\n","\n","    def __init__(self, dim: int, cfg: TrustWeightedConfig | None = None) -> None:\n","        self.dim = int(dim)\n","        self.cfg = cfg or TrustWeightedConfig()\n","        self.m = torch.zeros(self.dim, dtype=torch.float32)  # m_t, server momentum\n","        self.step: int = 0\n","\n","        self.theta = torch.tensor(self.cfg.theta, dtype=torch.float32)\n","\n","    # ------------------------------------------------------------------ helpers\n","\n","    def _proj_m(self, u: torch.Tensor) -> torch.Tensor:\n","        # Proj_m(u) = <u, m> / (||m||^2 + eps) * m\n","        num = torch.dot(u, self.m)\n","        denom = torch.dot(self.m, self.m) + self.cfg.eps\n","        coef = num / denom\n","        return coef * self.m\n","\n","    def _guard(self, tau: torch.Tensor, norm_u: torch.Tensor) -> torch.Tensor:\n","        # Guard_i = 1 / (1 + Î²1 Ï„_i + Î²2 ||u_i||)\n","        return 1.0 / (1.0 + self.cfg.beta1 * tau + self.cfg.beta2 * norm_u)\n","\n","    def _freshness(self, tau: torch.Tensor) -> torch.Tensor:\n","        # s(Ï„) = exp(-Î± Ï„)\n","        return torch.exp(-self.cfg.freshness_alpha * tau)\n","\n","    # ---------------------------------------------------------------- aggregate\n","\n","    def aggregate(\n","        self,\n","        w_t: torch.Tensor,\n","        updates: List[Dict[str, torch.Tensor]],\n","    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n","        \"\"\"Aggregate a buffer of updates.\n","\n","        Args:\n","            w_t: Flattened current global model.\n","            updates: List of dicts, each containing:\n","                {\n","                    \"u\": update vector (1D tensor),\n","                    \"tau\": scalar tensor Ï„_i,\n","                    \"num_samples\": scalar tensor n_i,\n","                    \"delta_loss\": scalar tensor Î”LÌƒ_i,\n","                }\n","\n","        Returns:\n","            new_w: updated global model vector.\n","            metrics: small dict with aggregation statistics.\n","        \"\"\"\n","        if not updates:\n","            return w_t, {\"avg_tau\": 0.0, \"buffer_size\": 0.0}\n","\n","        self.step += 1\n","        device = w_t.device\n","        self.m = self.m.to(device)\n","\n","        # Collect basic statistics\n","        taus = torch.stack([u[\"tau\"].to(device) for u in updates])  # [B]\n","        ns = torch.stack([u[\"num_samples\"].to(device) for u in updates])  # [B]\n","        delta_losses = torch.stack([u[\"delta_loss\"].to(device) for u in updates])  # [B]\n","        total_n = ns.sum().clamp_min(1.0)\n","\n","        # Precompute norms, projections, sideways components, cosines\n","        proj_list: List[torch.Tensor] = []\n","        side_list: List[torch.Tensor] = []\n","        norm_u_list: List[torch.Tensor] = []\n","        cos_list: List[torch.Tensor] = []\n","\n","        for u_rec in updates:\n","            u = u_rec[\"u\"].to(device)\n","            norm_u = torch.norm(u).clamp_min(self.cfg.eps)\n","            norm_u_list.append(norm_u)\n","\n","            proj = self._proj_m(u)\n","            side = u - proj\n","            proj_list.append(proj)\n","            side_list.append(side)\n","\n","            # cos(u, m) = <u, m> / (||u|| ||m|| + eps)\n","            norm_m = torch.norm(self.m)\n","            if norm_m.item() > 0.0:\n","                cos_val = torch.dot(u, self.m) / (norm_u * norm_m + self.cfg.eps)\n","            else:\n","                cos_val = torch.tensor(0.0, device=device)\n","            cos_list.append(cos_val)\n","\n","        norm_u_tensor = torch.stack(norm_u_list)  # [B]\n","        cos_tensor = torch.stack(cos_list)  # [B]\n","\n","        # Guard factors per update\n","        guards = self._guard(taus, norm_u_tensor)  # [B]\n","\n","        # Freshness\n","        freshness = self._freshness(taus)  # [B]\n","\n","        # Quality term: exp(Î¸áµ€ [Î”LÌƒ_i, ||u_i||, cos(u_i, m_t)])\n","        feats = torch.stack(\n","            [delta_losses, norm_u_tensor, cos_tensor],\n","            dim=1,\n","        )  # [B, 3]\n","        quality_logits = feats @ self.theta.to(device)\n","        quality = torch.exp(quality_logits)\n","\n","        # Data share term: n_i / Î£_j n_j\n","        data_share = ns / total_n  # [B]\n","\n","        # Unnormalized weights, then normalization over buffer\n","        raw_weights = freshness * quality * data_share  # [B]\n","        sum_raw = raw_weights.sum()\n","        if sum_raw.item() <= 0.0:\n","            weights = torch.full_like(raw_weights, 1.0 / len(updates))\n","        else:\n","            weights = raw_weights / sum_raw\n","\n","        # Combine projection and guarded sideways components\n","        agg_update = torch.zeros_like(w_t)\n","        for i in range(len(updates)):\n","            comp = proj_list[i] + guards[i] * side_list[i]\n","            agg_update = agg_update + weights[i] * comp\n","\n","        # Final aggregation step:\n","        # w_{t+1} = w_t + Î· * Î£_i Weight_i * [Proj_m(u_i) + Guard_i (u_i - Proj_m(u_i))]\n","        new_w = w_t + self.cfg.eta * agg_update\n","\n","        # Update momentum m_t as a running average of aggregated updates\n","        self.m = (1.0 - self.cfg.momentum_gamma) * self.m + self.cfg.momentum_gamma * agg_update\n","\n","        metrics = {\n","            \"avg_tau\": float(taus.mean().item()),\n","            \"avg_norm_u\": float(norm_u_tensor.mean().item()),\n","            \"avg_delta_loss\": float(delta_losses.mean().item()),\n","            \"buffer_size\": float(len(updates)),\n","        }\n","        return new_w, metrics\n"]},{"cell_type":"code","execution_count":59,"id":"2393f504","metadata":{"id":"2393f504","executionInfo":{"status":"ok","timestamp":1764519239607,"user_tz":360,"elapsed":12,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}}},"outputs":[],"source":["# ===== server.py =====\n","\n","# Asynchronous federated server implementing the Trust-Weighted projection rule\n","import csv\n","import time\n","import threading\n","from dataclasses import dataclass\n","from pathlib import Path\n","from typing import Dict, List, Tuple, Optional, OrderedDict as ODType\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","\n","\n","# ---------------------------------------------------------------------------\n","\n","def _testloader(root: str, batch_size: int = 256) -> DataLoader:\n","    transform = transforms.Compose(\n","        [\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n","        ]\n","    )\n","    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=transform)\n","    # num_workers=0 to avoid multiprocessing / SemLock issues\n","    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","\n","def _flatten_state(state: \"ODType[str, torch.Tensor]\") -> torch.Tensor:\n","    \"\"\"Flatten a state_dict into a 1D tensor.\n","\n","    The default implementation simply concatenates parameters in whatever\n","    iteration order the dictionary exposes.  While this works for\n","    ``OrderedDict`` instances created from the same model template, it can\n","    become brittle when ``state`` is a plain Python ``dict`` because the\n","    insertion order may differ from the server's canonical template.  An\n","    inconsistent ordering corrupts the mapping between parameter slices and\n","    model layers, which in turn leads to meaningless update vectors and\n","    prevents the global model from learning.\n","\n","    To avoid such subtle bugs, prefer ``_flatten_state_by_template`` when\n","    flattening a state that may not share the exact ordering with the\n","    server's template (see ``_flatten_state_by_template`` below).\n","    \"\"\"\n","    return torch.cat([p.reshape(-1) for p in state.values()])\n","\n","\n","def _flatten_state_by_template(\n","    state: Dict[str, torch.Tensor], template: \"ODType[str, torch.Tensor]\"\n",") -> torch.Tensor:\n","    \"\"\"Flatten ``state`` according to the key ordering of ``template``.\n","\n","    Many downstream computations (e.g. computing parameter deltas) assume\n","    that all 1D parameter vectors follow the same ordering.  When\n","    ``state`` is a plain ``dict`` created from a model state_dict,\n","    Python preserves insertion order, but nothing guarantees that this\n","    ordering matches that of the server's ``template``.  This helper\n","    constructs a flattened tensor by explicitly iterating over the keys of\n","    ``template``, thereby aligning the parameter ordering regardless of\n","    how ``state`` was constructed.\n","\n","    Args:\n","        state: Mapping from parameter names to tensors.  Can be a\n","            standard ``dict`` or ``OrderedDict``.\n","        template: The canonical parameter ordering to follow.\n","\n","    Returns:\n","        A 1D tensor containing all parameters from ``state`` in the order\n","        specified by ``template``.\n","    \"\"\"\n","    return torch.cat([state[k].reshape(-1) for k in template.keys()])\n","\n","\n","def _vector_to_state(\n","    vec: torch.Tensor, template: \"ODType[str, torch.Tensor]\"\n",") -> \"ODType[str, torch.Tensor]\":\n","    new_state: \"ODType[str, torch.Tensor]\" = type(template)()\n","    offset = 0\n","    for k, t in template.items():\n","        numel = t.numel()\n","        new_state[k] = vec[offset : offset + numel].view_as(t).clone()\n","        offset += numel\n","    assert offset == vec.numel()\n","    return new_state\n","\n","\n","@dataclass\n","class ClientUpdateState:\n","    client_id: int\n","    base_version: int\n","    new_params: \"ODType[str, torch.Tensor]\"\n","    num_samples: int\n","    train_time_s: float\n","    delta_loss: float\n","    loss_before: float\n","    loss_after: float\n","    train_acc: float\n","    test_loss: float\n","    test_acc: float\n","    arrival_ts: float\n","\n","\n","class AsyncServer:\n","    \"\"\"Central server maintaining global model and asynchronous buffer.\"\"\"\n","\n","    def __init__(self, cfg: Optional[GlobalConfig] = None) -> None:\n","        if cfg is None:\n","            cfg = load_config()\n","        self.cfg = cfg\n","        self.device = get_device()\n","\n","        # data / evaluation\n","        self.testloader = _testloader(cfg.data.data_dir, batch_size=256)\n","\n","        # global model and version history\n","        model = build_resnet18(num_classes=cfg.data.num_classes)\n","        self._template_state: ODType[str, torch.Tensor] = ODType(\n","            (k, v.detach().cpu().clone()) for k, v in model.state_dict().items()\n","        )\n","        self._global_state: ODType[str, torch.Tensor] = ODType(\n","            (k, v.clone()) for k, v in self._template_state.items()\n","        )\n","\n","        self._model_versions: List[ODType[str, torch.Tensor]] = [\n","            ODType((k, v.clone()) for k, v in self._global_state.items())\n","        ]\n","        self._version: int = 0\n","\n","        # strategy encapsulating all math from the PDF\n","        dim = _flatten_state(self._global_state).numel()\n","        self.strategy = TrustWeightedAsyncStrategy(dim=dim)\n","\n","        # async buffer\n","        self.buffer: List[ClientUpdateState] = []\n","        self.buffer_size: int = 5  # K\n","        self.buffer_timeout_s: float = 5.0  # Î”t in seconds\n","        self._last_flush_ts: float = time.time()\n","\n","        # logging / control\n","        self.io_root = Path(cfg.io.logs_dir)\n","        self.io_root.mkdir(parents=True, exist_ok=True)\n","\n","        # Global training log CSV\n","        self.global_log_path = Path(cfg.io.global_log_csv)\n","        self.global_log_path.parent.mkdir(parents=True, exist_ok=True)\n","        self._init_global_log()\n","\n","        # Client participation log CSV\n","        self.client_log_path = Path(cfg.io.client_participation_csv)\n","        self.client_log_path.parent.mkdir(parents=True, exist_ok=True)\n","        self._init_client_log()\n","\n","        self.eval_interval_s: float = cfg.eval.interval_seconds\n","        self.target_accuracy: float = cfg.eval.target_accuracy\n","        self.max_rounds: int = cfg.train.max_rounds\n","        self.update_clip_norm: float = float(cfg.train.update_clip_norm)\n","\n","        self._num_aggregations: int = 0  # total_agg counter\n","        self._stop: bool = False\n","        self._stop_reason: str = \"\"\n","        self._lock = threading.Lock()\n","\n","        # Ensures only one aggregation is in-flight at a time.  Without\n","        # this lock, multiple threads could concurrently call\n","        # ``_aggregate`` on separate buffers, causing race conditions in\n","        # versioning and inconsistent staleness calculations.  All calls\n","        # to ``_aggregate`` must acquire this lock.\n","        self._agg_lock = threading.Lock()\n","\n","    # ----------------------------------------------------------------- logging\n","\n","    def _init_global_log(self) -> None:\n","        \"\"\"Initialize the global training CSV if it does not exist.\n","\n","        Columns (per aggregation):\n","            total_agg, avg_train_loss, avg_train_acc, test_loss, test_acc, time\n","        \"\"\"\n","        if self.global_log_path.exists():\n","            return\n","        with self.global_log_path.open(\"w\", newline=\"\") as f:\n","            writer = csv.writer(f)\n","            writer.writerow(\n","                [\n","                    \"total_agg\",\n","                    \"avg_train_loss\",\n","                    \"avg_train_acc\",\n","                    \"test_loss\",\n","                    \"test_acc\",\n","                    \"time\",\n","                ]\n","            )\n","\n","    def _append_global_log(\n","        self,\n","        total_agg: int,\n","        avg_train_loss: float,\n","        avg_train_acc: float,\n","        test_loss: float,\n","        test_acc: float,\n","    ) -> None:\n","        \"\"\"Append a single aggregation row to the global CSV.\"\"\"\n","        ts = time.time()\n","        with self.global_log_path.open(\"a\", newline=\"\") as f:\n","            writer = csv.writer(f)\n","            writer.writerow(\n","                [\n","                    int(total_agg),\n","                    float(avg_train_loss),\n","                    float(avg_train_acc),\n","                    float(test_loss),\n","                    float(test_acc),\n","                    ts,\n","                ]\n","            )\n","\n","    def _init_client_log(self) -> None:\n","        \"\"\"Initialize the client participation CSV.\n","\n","        Columns:\n","            client_id, local_train_loss, local_train_acc,\n","            local_test_loss, local_test_acc, total_agg, staleness\n","        \"\"\"\n","        if self.client_log_path.exists():\n","            return\n","        with self.client_log_path.open(\"w\", newline=\"\") as f:\n","            writer = csv.writer(f)\n","            writer.writerow(\n","                [\n","                    \"client_id\",\n","                    \"local_train_loss\",\n","                    \"local_train_acc\",\n","                    \"local_test_loss\",\n","                    \"local_test_acc\",\n","                    \"total_agg\",\n","                    \"staleness\",\n","                ]\n","            )\n","\n","    def _append_client_participation_log(\n","        self,\n","        total_agg: int,\n","        updates: List[ClientUpdateState],\n","        staleness_list: List[float],\n","    ) -> None:\n","        \"\"\"Append one row per client update participating in this aggregation.\n","\n","        staleness_list[i] is the Ï„_i for updates[i].\n","        \"\"\"\n","        with self.client_log_path.open(\"a\", newline=\"\") as f:\n","            writer = csv.writer(f)\n","            for u, tau_i in zip(updates, staleness_list):\n","                writer.writerow(\n","                    [\n","                        int(u.client_id),\n","                        float(u.loss_after),\n","                        float(u.train_acc),\n","                        float(u.test_loss),\n","                        float(u.test_acc),\n","                        int(total_agg),\n","                        float(tau_i),\n","                    ]\n","                )\n","\n","    # ------------------------------------------------------------------- public\n","\n","    def should_stop(self) -> bool:\n","        with self._lock:\n","            return self._stop\n","\n","    def mark_stop(self, reason: str = \"\") -> None:\n","        with self._lock:\n","            if not self._stop:\n","                self._stop = True\n","                if reason:\n","                    self._stop_reason = reason\n","                print(f\"[Server] Stopping: {self._stop_reason}\")\n","\n","    # --------------------------------------------------------------- model I/O\n","\n","    def get_global_model(self) -> Tuple[int, Dict[str, torch.Tensor]]:\n","        \"\"\"Return (version, state_dict) of the current global model.\"\"\"\n","        with self._lock:\n","            version = self._version\n","            # Preserve the parameter ordering when sending to clients by\n","            # constructing an OrderedDict.  A plain dict may reorder keys\n","            # unexpectedly on some Python versions or implementations,\n","            # breaking downstream flatten/unflatten assumptions.  Clones\n","            # ensure the server's tensors remain unmodified.\n","            state = ODType((k, v.clone()) for k, v in self._global_state.items())\n","        return version, state\n","\n","    # --------------------------------------------------------------- evaluation\n","\n","    def _make_model_from_state(self, state: Dict[str, torch.Tensor]) -> torch.nn.Module:\n","        model = build_resnet18(num_classes=self.cfg.data.num_classes)\n","        model.load_state_dict(state)\n","        return model.to(self.device)\n","\n","    def _evaluate_global(self) -> Tuple[float, float]:\n","        \"\"\"Evaluate the current global model on the test set.\"\"\"\n","        model = self._make_model_from_state(self._global_state)\n","        model.eval()\n","        criterion = torch.nn.CrossEntropyLoss()\n","        total_loss = 0.0\n","        total_correct = 0\n","        total_examples = 0\n","        with torch.no_grad():\n","            for xb, yb in self.testloader:\n","                xb, yb = xb.to(self.device), yb.to(self.device)\n","                logits = model(xb)\n","                loss = criterion(logits, yb)\n","                total_loss += loss.item() * xb.size(0)\n","                preds = logits.argmax(dim=1)\n","                total_correct += (preds == yb).sum().item()\n","                total_examples += xb.size(0)\n","        if total_examples == 0:\n","            return 0.0, 0.0\n","        return total_loss / total_examples, total_correct / total_examples\n","\n","    # --------------------------------------------------------------- aggregation\n","\n","    def _flush_buffer_if_needed(self) -> None:\n","        now = time.time()\n","        should_flush = False\n","        if len(self.buffer) >= self.buffer_size:\n","            should_flush = True\n","        elif (now - self._last_flush_ts) >= self.buffer_timeout_s and self.buffer:\n","            should_flush = True\n","\n","        if not should_flush:\n","            return\n","\n","        # copy buffer locally under lock then release for heavy work\n","        with self._lock:\n","            buffer_copy = list(self.buffer)\n","            self.buffer.clear()\n","            self._last_flush_ts = now\n","\n","        # Serialize aggregations to avoid version races\n","        with self._agg_lock:\n","            self._aggregate(buffer_copy)\n","\n","    def _aggregate(self, updates: List[ClientUpdateState]) -> None:\n","        \"\"\"Aggregate a batch of client updates and log to CSVs.\"\"\"\n","        if not updates:\n","            return\n","\n","        # Snapshot of current global parameters and version history\n","        with self._lock:\n","            # Always flatten the global state according to the template order\n","            global_vec = _flatten_state_by_template(self._global_state, self._template_state)\n","            version_now = self._version\n","            model_versions = list(self._model_versions)\n","\n","        # Construct per-update vectors and metadata for the strategy,\n","        # and collect staleness Ï„_i for logging.\n","        update_vectors: List[Dict[str, torch.Tensor]] = []\n","        staleness_list: List[float] = []\n","        valid_updates: List[ClientUpdateState] = []\n","\n","        for u in updates:\n","            base_state = model_versions[u.base_version]\n","            # Flatten base_state and new_params using the canonical template order\n","            base_vec = _flatten_state_by_template(base_state, self._template_state)\n","            new_vec = _flatten_state_by_template(u.new_params, self._template_state)\n","            ui = new_vec - base_vec\n","\n","            # Skip obviously bad updates (NaN/Inf) to avoid corrupting the global model\n","            if not torch.isfinite(ui).all():\n","                print(f\"[Server] Dropping client {u.client_id} update due to NaN/Inf values\")\n","                continue\n","            if self.update_clip_norm > 0:\n","                norm = torch.norm(ui)\n","                if torch.isfinite(norm) and norm.item() > self.update_clip_norm:\n","                    ui = ui * (self.update_clip_norm / (norm + 1e-12))\n","\n","            # Ï„_i = current-server-version - base_version (same Ï„_i used in strategy)\n","            tau_i = float(max(0, version_now - u.base_version))\n","            staleness_list.append(tau_i)\n","\n","            delta_loss = float(u.delta_loss)  # Î”LÌƒ_i\n","            update_vectors.append(\n","                {\n","                    \"u\": ui,\n","                    \"tau\": torch.tensor(tau_i, dtype=torch.float32),\n","                    \"num_samples\": torch.tensor(float(u.num_samples), dtype=torch.float32),\n","                    \"delta_loss\": torch.tensor(delta_loss, dtype=torch.float32),\n","                }\n","            )\n","            valid_updates.append(u)\n","\n","        if not update_vectors:\n","            print(\"[Server] Buffer flush skipped: no valid updates after filtering.\")\n","            return\n","\n","        # Run the trust-weighted aggregation strategy (unchanged algorithm)\n","        new_global_vec, agg_metrics = self.strategy.aggregate(global_vec, update_vectors)\n","\n","        # Map back into parameter state_dict form\n","        new_state = _vector_to_state(new_global_vec, self._template_state)\n","\n","        # Compute average local train metrics for this aggregation\n","        avg_train_loss = sum(u.loss_after for u in valid_updates) / len(valid_updates)\n","        avg_train_acc = sum(u.train_acc for u in valid_updates) / len(valid_updates)\n","\n","        # Commit the new global model and update aggregation counter\n","        with self._lock:\n","            self._global_state = ODType((k, v.clone()) for k, v in new_state.items())\n","            self._model_versions.append(\n","                ODType((k, v.clone()) for k, v in self._global_state.items())\n","            )\n","            self._version = len(self._model_versions) - 1\n","            self._num_aggregations += 1\n","            total_agg = self._num_aggregations\n","\n","        # Evaluate updated global model on test data\n","        test_loss, test_acc = self._evaluate_global()\n","\n","        # Log global metrics and per-client participation (now with staleness)\n","        self._append_global_log(\n","            total_agg=total_agg,\n","            avg_train_loss=avg_train_loss,\n","            avg_train_acc=avg_train_acc,\n","            test_loss=test_loss,\n","            test_acc=test_acc,\n","        )\n","        self._append_client_participation_log(\n","            total_agg=total_agg,\n","            updates=valid_updates,\n","            staleness_list=staleness_list,\n","        )\n","\n","        print(\n","            f\"[Server] Aggregated {len(valid_updates)} updates -> agg={total_agg} \"\n","            f\"(avg_tau={agg_metrics.get('avg_tau', 0.0):.3f}, \"\n","            f\"test_loss={test_loss:.4f}, test_acc={test_acc:.4f})\"\n","        )\n","\n","        # Stopping conditions based on global performance and max rounds\n","        if test_acc >= self.target_accuracy:\n","            self.mark_stop(f\"target accuracy {test_acc:.4f} reached\")\n","        if total_agg >= self.max_rounds:\n","            self.mark_stop(\"max aggregation rounds reached\")\n","\n","    # ------------------------------------------------------------ client entry\n","\n","    def submit_update(\n","        self,\n","        client_id: int,\n","        base_version: int,\n","        new_params: Dict[str, torch.Tensor],\n","        num_samples: int,\n","        train_time_s: float,\n","        delta_loss: float,\n","        loss_before: float,\n","        loss_after: float,\n","        train_acc: float,\n","        test_loss: float,\n","        test_acc: float,\n","    ) -> None:\n","        \"\"\"Entry point called by clients after local training.\n","\n","        This method only enqueues the update and triggers buffer flushing.\n","        All CSV logging tied to a particular aggregation happens inside\n","        `_aggregate` so that `total_agg` and `staleness` are consistent.\n","        \"\"\"\n","        cu = ClientUpdateState(\n","            client_id=client_id,\n","            base_version=base_version,\n","            new_params=new_params,\n","            num_samples=num_samples,\n","            train_time_s=float(train_time_s),\n","            delta_loss=float(delta_loss),\n","            loss_before=float(loss_before),\n","            loss_after=float(loss_after),\n","            train_acc=float(train_acc),\n","            test_loss=float(test_loss),\n","            test_acc=float(test_acc),\n","            arrival_ts=time.time(),\n","        )\n","        with self._lock:\n","            self.buffer.append(cu)\n","        self._flush_buffer_if_needed()\n","\n","    # --------------------------------------------------------------- lifecycle\n","\n","    def wait(self) -> None:\n","        \"\"\"Block until training is finished (stopping condition reached).\"\"\"\n","        try:\n","            while not self.should_stop():\n","                time.sleep(0.2)\n","        finally:\n","            self.mark_stop(self._stop_reason or \"training finished\")\n"]},{"cell_type":"code","execution_count":60,"id":"a4bfbfa2","metadata":{"id":"a4bfbfa2","executionInfo":{"status":"ok","timestamp":1764519239639,"user_tz":360,"elapsed":14,"user":{"displayName":"Arya Patel","userId":"07088500315875920175"}}},"outputs":[],"source":["# ===== client.py =====\n","\n","# Lightning-free async federated client for CIFAR-10\n","import time\n","import random\n","from typing import Sequence, Tuple\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import datasets, transforms\n","from torch.nn.utils import clip_grad_norm_\n","\n","import pytorch_lightning as pl  # imported but not required; kept for compatibility\n","from pytorch_lightning.callbacks import ModelCheckpoint  # unused, kept for compatibility\n","\n","\n","\n","def _build_transform() -> transforms.Compose:\n","    # CIFAR-10 standard augmentation to reduce overfitting\n","    return transforms.Compose(\n","        [\n","            transforms.RandomCrop(32, padding=4),\n","            transforms.RandomHorizontalFlip(),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n","        ]\n","    )\n","\n","\n","def _make_dataloader(\n","    data_dir: str,\n","    indices: Sequence[int],\n","    batch_size: int,\n",") -> DataLoader:\n","    dataset = datasets.CIFAR10(\n","        root=data_dir,\n","        train=True,\n","        download=True,\n","        transform=_build_transform(),\n","    )\n","    subset = Subset(dataset, indices)\n","\n","    # Safety: in case partitioning ever returns an empty list for a client\n","    if len(subset) == 0:\n","        return DataLoader(subset, batch_size=1, shuffle=False, num_workers=0)\n","\n","    # num_workers=0 to avoid multiprocessing issues on macOS / Python 3.13\n","    return DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=0)\n","\n","\n","class AsyncClient:\n","    \"\"\"Asynchronous client performing local training on its partition.\n","\n","    The client fetches the latest global model from the server, trains for a few\n","    local epochs, and submits the updated parameters plus basic telemetry.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        cid: int,\n","        indices: Sequence[int],\n","        cfg: GlobalConfig,\n","    ) -> None:\n","        self.cid = cid\n","        self.cfg = cfg\n","        self.device = get_device()\n","        self.loader = _make_dataloader(\n","            cfg.data.data_dir,\n","            indices,\n","            cfg.clients.batch_size,\n","        )\n","        self.num_classes = cfg.data.num_classes\n","        self.local_epochs = cfg.clients.local_epochs\n","        self.lr = cfg.clients.lr\n","        self.weight_decay = cfg.clients.weight_decay\n","        self.grad_clip = cfg.clients.grad_clip\n","\n","        # --- straggler behaviour ---\n","        num_clients = cfg.clients.total\n","        slow_fraction = cfg.clients.struggle_percent / 100.0\n","        num_slow = int(round(num_clients * slow_fraction))\n","        self.is_slow = cid < num_slow  # deterministic but good enough\n","\n","        self.delay_slow_range = tuple(cfg.clients.delay_slow_range)\n","        self.delay_fast_range = tuple(cfg.clients.delay_fast_range)\n","        self.jitter_per_round = float(cfg.clients.jitter_per_round)\n","        self.client_delay = float(cfg.server_runtime.client_delay)\n","\n","    # ------------------------------------------------------------------ utils\n","\n","    def _sample_delay(self) -> float:\n","        if self.is_slow:\n","            base = random.uniform(*self.delay_slow_range)\n","        else:\n","            base = random.uniform(*self.delay_fast_range)\n","        jitter = random.uniform(-self.jitter_per_round, self.jitter_per_round)\n","        return max(0.0, base + jitter + self.client_delay)\n","\n","    def _build_model(self) -> nn.Module:\n","        model = build_resnet18(num_classes=self.num_classes)\n","        return model.to(self.device)\n","\n","    # ---------------------------------------------------------------- training\n","\n","    def _evaluate_on_loader(self, model: nn.Module) -> Tuple[float, float]:\n","        \"\"\"Return (loss, accuracy) on the client's local data.\"\"\"\n","        model.eval()\n","        criterion = nn.CrossEntropyLoss()\n","        total_loss = 0.0\n","        total_correct = 0\n","        total_examples = 0\n","        with torch.no_grad():\n","            for xb, yb in self.loader:\n","                xb, yb = xb.to(self.device), yb.to(self.device)\n","                logits = model(xb)\n","                loss = criterion(logits, yb)\n","                total_loss += loss.item() * xb.size(0)\n","                preds = logits.argmax(dim=1)\n","                total_correct += (preds == yb).sum().item()\n","                total_examples += xb.size(0)\n","        if total_examples == 0:\n","            return 0.0, 0.0\n","        return total_loss / total_examples, total_correct / total_examples\n","\n","    def _train_local(self, model: nn.Module) -> Tuple[float, float]:\n","        \"\"\"Train for `local_epochs` and return (loss_after, acc_after).\"\"\"\n","        model.train()\n","        criterion = nn.CrossEntropyLoss()\n","        optim = torch.optim.SGD(\n","            model.parameters(),\n","            lr=self.lr,\n","            momentum=0.9,\n","            weight_decay=self.weight_decay,\n","        )\n","\n","        for _ in range(self.local_epochs):\n","            for xb, yb in self.loader:\n","                xb, yb = xb.to(self.device), yb.to(self.device)\n","                optim.zero_grad(set_to_none=True)\n","                logits = model(xb)\n","                loss = criterion(logits, yb)\n","                loss.backward()\n","                if self.grad_clip > 0:\n","                    clip_grad_norm_(model.parameters(), self.grad_clip)\n","                optim.step()\n","\n","        # reuse evaluation code for final metrics\n","        return self._evaluate_on_loader(model)\n","\n","    # ------------------------------------------------------------- main loop\n","\n","    def run_once(self, server) -> bool:\n","        \"\"\"Perform a single async round with the server.\n","\n","        Returns False when the server indicates global stopping, True otherwise.\n","        \"\"\"\n","        # Simulated network / computation delay heterogeneity\n","        delay = self._sample_delay()\n","        if delay > 0:\n","            time.sleep(delay)\n","\n","        if server.should_stop():\n","            return False\n","\n","        # Get the latest global model snapshot\n","        version, global_state = server.get_global_model()\n","        model = self._build_model()\n","        model.load_state_dict(global_state)\n","\n","        # Evaluate before local training to compute loss drop Î”LÌƒ_i\n","        loss_before, _ = self._evaluate_on_loader(model)\n","\n","        start_time = time.time()\n","        loss_after, train_acc = self._train_local(model)\n","        train_time_s = time.time() - start_time\n","\n","        # Local \"test\" is just another pass over the client's data\n","        test_loss, test_acc = self._evaluate_on_loader(model)\n","\n","        # Move params to CPU tensors so they are cheap to share with server\n","        # Build an OrderedDict for `new_params` using the model's own\n","        # parameter ordering.  Although Python's plain dicts preserve\n","        # insertion order, explicitly constructing an `OrderedDict` makes\n","        # the intent clear and avoids any surprises if the language\n","        # specification changes.  The server relies on matching key\n","        # ordering to correctly flatten parameter tensors.\n","        from collections import OrderedDict\n","        new_params = OrderedDict()\n","        for k, v in model.state_dict().items():\n","            new_params[k] = v.detach().cpu().clone()\n","        num_examples = len(self.loader.dataset)\n","\n","        delta_loss = loss_before - loss_after  # Î”LÌƒ_i\n","\n","        # All local metrics are computed here and passed to the server\n","        server.submit_update(\n","            client_id=self.cid,\n","            base_version=version,\n","            new_params=new_params,\n","            num_samples=num_examples,\n","            train_time_s=train_time_s,\n","            delta_loss=delta_loss,\n","            loss_before=loss_before,\n","            loss_after=loss_after,\n","            train_acc=train_acc,\n","            test_loss=test_loss,\n","            test_acc=test_acc,\n","        )\n","        return not server.should_stop()"]},{"cell_type":"code","execution_count":null,"id":"85d9871a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"85d9871a","outputId":"af20d477-34d0-4090-bd08-540b2742e1bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["[exp] clients=20 -> logs at /content/drive/MyDrive/TrustWeightColab/logs/TrustWeightClientCountExp/clients_20\n"]},{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 170M/170M [00:01<00:00, 103MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["[Server] Aggregated 1 updates -> agg=1 (avg_tau=0.000, test_loss=2.4572, test_acc=0.1000)\n","[Server] Aggregated 5 updates -> agg=2 (avg_tau=1.000, test_loss=2.3844, test_acc=0.1004)\n","[Server] Aggregated 5 updates -> agg=3 (avg_tau=2.000, test_loss=2.3421, test_acc=0.1014)\n","[Server] Aggregated 5 updates -> agg=4 (avg_tau=2.400, test_loss=2.3167, test_acc=0.1145)\n","[Server] Aggregated 3 updates -> agg=5 (avg_tau=3.000, test_loss=2.2997, test_acc=0.1325)\n","[Server] Aggregated 1 updates -> agg=6 (avg_tau=3.000, test_loss=2.2899, test_acc=0.1383)\n","[Server] Aggregated 3 updates -> agg=7 (avg_tau=4.000, test_loss=2.2819, test_acc=0.1422)\n","[Server] Aggregated 3 updates -> agg=8 (avg_tau=5.000, test_loss=2.2748, test_acc=0.1441)\n","[Server] Aggregated 1 updates -> agg=9 (avg_tau=5.000, test_loss=2.2693, test_acc=0.1435)\n","[Server] Aggregated 1 updates -> agg=10 (avg_tau=5.000, test_loss=2.2650, test_acc=0.1438)\n","[Server] Aggregated 4 updates -> agg=11 (avg_tau=6.000, test_loss=2.2619, test_acc=0.1422)\n","[Server] Aggregated 1 updates -> agg=12 (avg_tau=6.000, test_loss=2.2585, test_acc=0.1447)\n","[Server] Aggregated 3 updates -> agg=13 (avg_tau=7.000, test_loss=2.2560, test_acc=0.1445)\n","[Server] Aggregated 1 updates -> agg=14 (avg_tau=7.000, test_loss=2.2542, test_acc=0.1447)\n","[Server] Aggregated 1 updates -> agg=15 (avg_tau=7.000, test_loss=2.2510, test_acc=0.1472)\n","[Server] Aggregated 2 updates -> agg=16 (avg_tau=8.000, test_loss=2.2483, test_acc=0.1485)\n","[Server] Aggregated 1 updates -> agg=17 (avg_tau=8.000, test_loss=2.2420, test_acc=0.1546)\n","[Server] Aggregated 2 updates -> agg=18 (avg_tau=8.000, test_loss=2.2367, test_acc=0.1580)\n","[Server] Aggregated 1 updates -> agg=19 (avg_tau=8.000, test_loss=2.2308, test_acc=0.1627)\n","[Server] Aggregated 1 updates -> agg=20 (avg_tau=9.000, test_loss=2.2243, test_acc=0.1636)\n","[Server] Aggregated 1 updates -> agg=21 (avg_tau=9.000, test_loss=2.2149, test_acc=0.1712)\n","[Server] Aggregated 1 updates -> agg=22 (avg_tau=9.000, test_loss=2.2049, test_acc=0.1755)\n","[Server] Aggregated 1 updates -> agg=23 (avg_tau=9.000, test_loss=2.1922, test_acc=0.1820)\n","[Server] Aggregated 1 updates -> agg=24 (avg_tau=9.000, test_loss=2.1788, test_acc=0.1878)\n","[Server] Aggregated 1 updates -> agg=25 (avg_tau=9.000, test_loss=2.1646, test_acc=0.1949)\n","[Server] Aggregated 1 updates -> agg=26 (avg_tau=9.000, test_loss=2.1458, test_acc=0.2034)\n","[Server] Aggregated 1 updates -> agg=27 (avg_tau=9.000, test_loss=2.1258, test_acc=0.2086)\n","[Server] Aggregated 1 updates -> agg=28 (avg_tau=9.000, test_loss=2.1076, test_acc=0.2167)\n","[Server] Aggregated 1 updates -> agg=29 (avg_tau=9.000, test_loss=2.0906, test_acc=0.2235)\n","[Server] Aggregated 1 updates -> agg=30 (avg_tau=9.000, test_loss=2.0727, test_acc=0.2345)\n","[Server] Aggregated 1 updates -> agg=31 (avg_tau=9.000, test_loss=2.0561, test_acc=0.2393)\n","[Server] Aggregated 1 updates -> agg=32 (avg_tau=8.000, test_loss=2.0404, test_acc=0.2465)\n","[Server] Aggregated 1 updates -> agg=33 (avg_tau=9.000, test_loss=2.0242, test_acc=0.2544)\n","[Server] Aggregated 1 updates -> agg=34 (avg_tau=9.000, test_loss=2.0106, test_acc=0.2580)\n","[Server] Aggregated 1 updates -> agg=35 (avg_tau=9.000, test_loss=1.9998, test_acc=0.2609)\n","[Server] Aggregated 1 updates -> agg=36 (avg_tau=9.000, test_loss=1.9897, test_acc=0.2644)\n","[Server] Aggregated 1 updates -> agg=37 (avg_tau=9.000, test_loss=1.9797, test_acc=0.2670)\n","[Server] Aggregated 1 updates -> agg=38 (avg_tau=9.000, test_loss=1.9694, test_acc=0.2700)\n","[Server] Aggregated 1 updates -> agg=39 (avg_tau=9.000, test_loss=1.9615, test_acc=0.2737)\n","[Server] Aggregated 1 updates -> agg=40 (avg_tau=9.000, test_loss=1.9543, test_acc=0.2776)\n","[Server] Aggregated 1 updates -> agg=41 (avg_tau=9.000, test_loss=1.9436, test_acc=0.2824)\n","[Server] Aggregated 1 updates -> agg=42 (avg_tau=9.000, test_loss=1.9374, test_acc=0.2839)\n","[Server] Aggregated 1 updates -> agg=43 (avg_tau=9.000, test_loss=1.9290, test_acc=0.2877)\n","[Server] Aggregated 1 updates -> agg=44 (avg_tau=9.000, test_loss=1.9224, test_acc=0.2912)\n","[Server] Aggregated 1 updates -> agg=45 (avg_tau=9.000, test_loss=1.9192, test_acc=0.2916)\n","[Server] Aggregated 1 updates -> agg=46 (avg_tau=9.000, test_loss=1.9130, test_acc=0.2952)\n","[Server] Aggregated 1 updates -> agg=47 (avg_tau=9.000, test_loss=1.9129, test_acc=0.2947)\n","[Server] Aggregated 1 updates -> agg=48 (avg_tau=9.000, test_loss=1.9082, test_acc=0.2945)\n","[Server] Aggregated 1 updates -> agg=49 (avg_tau=8.000, test_loss=1.9027, test_acc=0.2951)\n","[Server] Aggregated 1 updates -> agg=50 (avg_tau=9.000, test_loss=1.8947, test_acc=0.2987)\n","[Server] Aggregated 1 updates -> agg=51 (avg_tau=9.000, test_loss=1.8938, test_acc=0.2990)\n","[Server] Aggregated 1 updates -> agg=52 (avg_tau=9.000, test_loss=1.8909, test_acc=0.2975)\n","[Server] Aggregated 1 updates -> agg=53 (avg_tau=9.000, test_loss=1.8879, test_acc=0.2995)\n","[Server] Aggregated 1 updates -> agg=54 (avg_tau=9.000, test_loss=1.8831, test_acc=0.3016)\n","[Server] Aggregated 1 updates -> agg=55 (avg_tau=9.000, test_loss=1.8882, test_acc=0.2986)\n","[Server] Aggregated 1 updates -> agg=56 (avg_tau=9.000, test_loss=1.8843, test_acc=0.2997)\n","[Server] Aggregated 1 updates -> agg=57 (avg_tau=9.000, test_loss=1.8854, test_acc=0.3003)\n","[Server] Aggregated 1 updates -> agg=58 (avg_tau=9.000, test_loss=1.8837, test_acc=0.2996)\n","[Server] Aggregated 1 updates -> agg=59 (avg_tau=9.000, test_loss=1.8857, test_acc=0.2989)\n","[Server] Aggregated 1 updates -> agg=60 (avg_tau=9.000, test_loss=1.8815, test_acc=0.3033)\n","[Server] Aggregated 1 updates -> agg=61 (avg_tau=9.000, test_loss=1.8811, test_acc=0.3042)\n","[Server] Aggregated 1 updates -> agg=62 (avg_tau=9.000, test_loss=1.8747, test_acc=0.3056)\n","[Server] Aggregated 1 updates -> agg=63 (avg_tau=9.000, test_loss=1.8670, test_acc=0.3095)\n","[Server] Aggregated 1 updates -> agg=64 (avg_tau=9.000, test_loss=1.8644, test_acc=0.3102)\n","[Server] Aggregated 1 updates -> agg=65 (avg_tau=9.000, test_loss=1.8571, test_acc=0.3118)\n","[Server] Aggregated 1 updates -> agg=66 (avg_tau=9.000, test_loss=1.8529, test_acc=0.3133)\n","[Server] Aggregated 1 updates -> agg=67 (avg_tau=9.000, test_loss=1.8528, test_acc=0.3152)\n","[Server] Aggregated 1 updates -> agg=68 (avg_tau=8.000, test_loss=1.8428, test_acc=0.3160)\n","[Server] Aggregated 1 updates -> agg=69 (avg_tau=9.000, test_loss=1.8349, test_acc=0.3189)\n","[Server] Aggregated 1 updates -> agg=70 (avg_tau=9.000, test_loss=1.8280, test_acc=0.3226)\n","[Server] Aggregated 1 updates -> agg=71 (avg_tau=9.000, test_loss=1.8169, test_acc=0.3262)\n","[Server] Aggregated 1 updates -> agg=72 (avg_tau=9.000, test_loss=1.8043, test_acc=0.3308)\n","[Server] Aggregated 1 updates -> agg=73 (avg_tau=9.000, test_loss=1.7992, test_acc=0.3321)\n","[Server] Aggregated 1 updates -> agg=74 (avg_tau=8.000, test_loss=1.7915, test_acc=0.3376)\n","[Server] Aggregated 1 updates -> agg=75 (avg_tau=9.000, test_loss=1.7918, test_acc=0.3349)\n","[Server] Aggregated 1 updates -> agg=76 (avg_tau=9.000, test_loss=1.7831, test_acc=0.3383)\n","[Server] Aggregated 1 updates -> agg=77 (avg_tau=9.000, test_loss=1.7790, test_acc=0.3392)\n","[Server] Aggregated 1 updates -> agg=78 (avg_tau=9.000, test_loss=1.7765, test_acc=0.3395)\n","[Server] Aggregated 1 updates -> agg=79 (avg_tau=9.000, test_loss=1.7710, test_acc=0.3411)\n","[Server] Aggregated 1 updates -> agg=80 (avg_tau=9.000, test_loss=1.7612, test_acc=0.3440)\n","[Server] Aggregated 1 updates -> agg=81 (avg_tau=9.000, test_loss=1.7591, test_acc=0.3456)\n","[Server] Aggregated 1 updates -> agg=82 (avg_tau=9.000, test_loss=1.7528, test_acc=0.3465)\n","[Server] Aggregated 1 updates -> agg=83 (avg_tau=9.000, test_loss=1.7520, test_acc=0.3463)\n","[Server] Aggregated 1 updates -> agg=84 (avg_tau=9.000, test_loss=1.7456, test_acc=0.3483)\n","[Server] Aggregated 1 updates -> agg=85 (avg_tau=9.000, test_loss=1.7410, test_acc=0.3491)\n","[Server] Aggregated 1 updates -> agg=86 (avg_tau=9.000, test_loss=1.7425, test_acc=0.3494)\n","[Server] Aggregated 1 updates -> agg=87 (avg_tau=9.000, test_loss=1.7396, test_acc=0.3501)\n","[Server] Aggregated 1 updates -> agg=88 (avg_tau=9.000, test_loss=1.7349, test_acc=0.3511)\n","[Server] Aggregated 1 updates -> agg=89 (avg_tau=9.000, test_loss=1.7317, test_acc=0.3529)\n","[Server] Aggregated 1 updates -> agg=90 (avg_tau=9.000, test_loss=1.7288, test_acc=0.3534)\n","[Server] Aggregated 1 updates -> agg=91 (avg_tau=9.000, test_loss=1.7228, test_acc=0.3553)\n","[Server] Aggregated 1 updates -> agg=92 (avg_tau=9.000, test_loss=1.7204, test_acc=0.3578)\n","[Server] Aggregated 1 updates -> agg=93 (avg_tau=9.000, test_loss=1.7139, test_acc=0.3598)\n","[Server] Aggregated 1 updates -> agg=94 (avg_tau=9.000, test_loss=1.7099, test_acc=0.3610)\n","[Server] Aggregated 1 updates -> agg=95 (avg_tau=9.000, test_loss=1.7094, test_acc=0.3631)\n","[Server] Aggregated 1 updates -> agg=96 (avg_tau=9.000, test_loss=1.7076, test_acc=0.3622)\n","[Server] Aggregated 1 updates -> agg=97 (avg_tau=9.000, test_loss=1.7066, test_acc=0.3613)\n","[Server] Aggregated 1 updates -> agg=98 (avg_tau=9.000, test_loss=1.7003, test_acc=0.3637)\n","[Server] Aggregated 1 updates -> agg=99 (avg_tau=9.000, test_loss=1.6954, test_acc=0.3655)\n","[Server] Aggregated 1 updates -> agg=100 (avg_tau=9.000, test_loss=1.6883, test_acc=0.3686)\n","[Server] Aggregated 1 updates -> agg=101 (avg_tau=9.000, test_loss=1.6839, test_acc=0.3689)\n","[Server] Aggregated 1 updates -> agg=102 (avg_tau=8.000, test_loss=1.6870, test_acc=0.3671)\n","[Server] Aggregated 1 updates -> agg=103 (avg_tau=9.000, test_loss=1.6827, test_acc=0.3699)\n","[Server] Aggregated 1 updates -> agg=104 (avg_tau=8.000, test_loss=1.6818, test_acc=0.3702)\n","[Server] Aggregated 1 updates -> agg=105 (avg_tau=9.000, test_loss=1.6800, test_acc=0.3711)\n","[Server] Aggregated 1 updates -> agg=106 (avg_tau=9.000, test_loss=1.6744, test_acc=0.3736)\n","[Server] Aggregated 1 updates -> agg=107 (avg_tau=9.000, test_loss=1.6729, test_acc=0.3742)\n","[Server] Aggregated 1 updates -> agg=108 (avg_tau=9.000, test_loss=1.6717, test_acc=0.3747)\n","[Server] Aggregated 1 updates -> agg=109 (avg_tau=9.000, test_loss=1.6676, test_acc=0.3776)\n","[Server] Aggregated 1 updates -> agg=110 (avg_tau=9.000, test_loss=1.6653, test_acc=0.3765)\n","[Server] Aggregated 1 updates -> agg=111 (avg_tau=9.000, test_loss=1.6609, test_acc=0.3778)\n","[Server] Aggregated 1 updates -> agg=112 (avg_tau=9.000, test_loss=1.6603, test_acc=0.3785)\n","[Server] Aggregated 1 updates -> agg=113 (avg_tau=9.000, test_loss=1.6578, test_acc=0.3809)\n","[Server] Aggregated 1 updates -> agg=114 (avg_tau=9.000, test_loss=1.6569, test_acc=0.3810)\n","[Server] Aggregated 1 updates -> agg=115 (avg_tau=9.000, test_loss=1.6570, test_acc=0.3818)\n","[Server] Aggregated 1 updates -> agg=116 (avg_tau=9.000, test_loss=1.6538, test_acc=0.3828)\n","[Server] Aggregated 1 updates -> agg=117 (avg_tau=9.000, test_loss=1.6515, test_acc=0.3838)\n","[Server] Aggregated 1 updates -> agg=118 (avg_tau=9.000, test_loss=1.6470, test_acc=0.3860)\n","[Server] Aggregated 1 updates -> agg=119 (avg_tau=9.000, test_loss=1.6425, test_acc=0.3868)\n","[Server] Aggregated 1 updates -> agg=120 (avg_tau=9.000, test_loss=1.6361, test_acc=0.3886)\n","[Server] Aggregated 1 updates -> agg=121 (avg_tau=9.000, test_loss=1.6329, test_acc=0.3889)\n","[Server] Aggregated 1 updates -> agg=122 (avg_tau=9.000, test_loss=1.6288, test_acc=0.3903)\n","[Server] Aggregated 1 updates -> agg=123 (avg_tau=9.000, test_loss=1.6247, test_acc=0.3901)\n","[Server] Aggregated 1 updates -> agg=124 (avg_tau=9.000, test_loss=1.6274, test_acc=0.3897)\n","[Server] Aggregated 1 updates -> agg=125 (avg_tau=9.000, test_loss=1.6223, test_acc=0.3925)\n","[Server] Aggregated 1 updates -> agg=126 (avg_tau=9.000, test_loss=1.6143, test_acc=0.3947)\n","[Server] Aggregated 1 updates -> agg=127 (avg_tau=9.000, test_loss=1.6142, test_acc=0.3935)\n","[Server] Aggregated 1 updates -> agg=128 (avg_tau=9.000, test_loss=1.6159, test_acc=0.3945)\n","[Server] Aggregated 1 updates -> agg=129 (avg_tau=9.000, test_loss=1.6098, test_acc=0.3945)\n","[Server] Aggregated 1 updates -> agg=130 (avg_tau=9.000, test_loss=1.6100, test_acc=0.3954)\n","[Server] Aggregated 1 updates -> agg=131 (avg_tau=9.000, test_loss=1.6118, test_acc=0.3976)\n","[Server] Aggregated 1 updates -> agg=132 (avg_tau=9.000, test_loss=1.6103, test_acc=0.3984)\n","[Server] Aggregated 1 updates -> agg=133 (avg_tau=8.000, test_loss=1.6111, test_acc=0.4006)\n","[Server] Aggregated 1 updates -> agg=134 (avg_tau=9.000, test_loss=1.6063, test_acc=0.4017)\n","[Server] Aggregated 1 updates -> agg=135 (avg_tau=9.000, test_loss=1.6036, test_acc=0.4027)\n","[Server] Aggregated 1 updates -> agg=136 (avg_tau=8.000, test_loss=1.5953, test_acc=0.4045)\n","[Server] Aggregated 1 updates -> agg=137 (avg_tau=9.000, test_loss=1.5906, test_acc=0.4063)\n","[Server] Aggregated 1 updates -> agg=138 (avg_tau=9.000, test_loss=1.5917, test_acc=0.4072)\n","[Server] Aggregated 1 updates -> agg=139 (avg_tau=9.000, test_loss=1.5978, test_acc=0.4056)\n","[Server] Aggregated 1 updates -> agg=140 (avg_tau=9.000, test_loss=1.5944, test_acc=0.4074)\n","[Server] Aggregated 1 updates -> agg=141 (avg_tau=9.000, test_loss=1.5870, test_acc=0.4081)\n","[Server] Aggregated 1 updates -> agg=142 (avg_tau=8.000, test_loss=1.5816, test_acc=0.4094)\n","[Server] Aggregated 1 updates -> agg=143 (avg_tau=9.000, test_loss=1.5789, test_acc=0.4119)\n","[Server] Aggregated 1 updates -> agg=144 (avg_tau=9.000, test_loss=1.5819, test_acc=0.4118)\n","[Server] Aggregated 1 updates -> agg=145 (avg_tau=9.000, test_loss=1.5820, test_acc=0.4143)\n","[Server] Aggregated 1 updates -> agg=146 (avg_tau=9.000, test_loss=1.5783, test_acc=0.4158)\n","[Server] Aggregated 1 updates -> agg=147 (avg_tau=9.000, test_loss=1.5760, test_acc=0.4166)\n","[Server] Aggregated 1 updates -> agg=148 (avg_tau=9.000, test_loss=1.5770, test_acc=0.4166)\n","[Server] Aggregated 1 updates -> agg=149 (avg_tau=8.000, test_loss=1.5746, test_acc=0.4166)\n","[Server] Aggregated 1 updates -> agg=150 (avg_tau=9.000, test_loss=1.5689, test_acc=0.4179)\n","[Server] Aggregated 1 updates -> agg=151 (avg_tau=9.000, test_loss=1.5719, test_acc=0.4178)\n","[Server] Aggregated 1 updates -> agg=152 (avg_tau=9.000, test_loss=1.5707, test_acc=0.4191)\n","[Server] Aggregated 1 updates -> agg=153 (avg_tau=9.000, test_loss=1.5724, test_acc=0.4196)\n","[Server] Aggregated 1 updates -> agg=154 (avg_tau=8.000, test_loss=1.5705, test_acc=0.4184)\n","[Server] Aggregated 1 updates -> agg=155 (avg_tau=9.000, test_loss=1.5708, test_acc=0.4180)\n","[Server] Aggregated 1 updates -> agg=156 (avg_tau=9.000, test_loss=1.5723, test_acc=0.4183)\n","[Server] Aggregated 1 updates -> agg=157 (avg_tau=9.000, test_loss=1.5689, test_acc=0.4195)\n","[Server] Aggregated 1 updates -> agg=158 (avg_tau=8.000, test_loss=1.5638, test_acc=0.4209)\n","[Server] Aggregated 1 updates -> agg=159 (avg_tau=9.000, test_loss=1.5589, test_acc=0.4213)\n","[Server] Aggregated 1 updates -> agg=160 (avg_tau=9.000, test_loss=1.5554, test_acc=0.4248)\n","[Server] Aggregated 1 updates -> agg=161 (avg_tau=9.000, test_loss=1.5515, test_acc=0.4253)\n","[Server] Aggregated 1 updates -> agg=162 (avg_tau=9.000, test_loss=1.5514, test_acc=0.4223)\n","[Server] Aggregated 1 updates -> agg=163 (avg_tau=9.000, test_loss=1.5496, test_acc=0.4239)\n","[Server] Aggregated 1 updates -> agg=164 (avg_tau=9.000, test_loss=1.5498, test_acc=0.4235)\n","[Server] Aggregated 1 updates -> agg=165 (avg_tau=9.000, test_loss=1.5456, test_acc=0.4238)\n","[Server] Aggregated 1 updates -> agg=166 (avg_tau=8.000, test_loss=1.5423, test_acc=0.4247)\n","[Server] Aggregated 1 updates -> agg=167 (avg_tau=9.000, test_loss=1.5391, test_acc=0.4250)\n","[Server] Aggregated 1 updates -> agg=168 (avg_tau=9.000, test_loss=1.5387, test_acc=0.4260)\n","[Server] Aggregated 1 updates -> agg=169 (avg_tau=9.000, test_loss=1.5386, test_acc=0.4265)\n","[Server] Aggregated 1 updates -> agg=170 (avg_tau=9.000, test_loss=1.5445, test_acc=0.4240)\n","[Server] Aggregated 1 updates -> agg=171 (avg_tau=9.000, test_loss=1.5394, test_acc=0.4277)\n","[Server] Aggregated 1 updates -> agg=172 (avg_tau=9.000, test_loss=1.5383, test_acc=0.4270)\n","[Server] Aggregated 1 updates -> agg=173 (avg_tau=9.000, test_loss=1.5374, test_acc=0.4288)\n","[Server] Aggregated 1 updates -> agg=174 (avg_tau=9.000, test_loss=1.5376, test_acc=0.4292)\n","[Server] Aggregated 1 updates -> agg=175 (avg_tau=9.000, test_loss=1.5434, test_acc=0.4263)\n","[Server] Aggregated 1 updates -> agg=176 (avg_tau=9.000, test_loss=1.5522, test_acc=0.4232)\n","[Server] Aggregated 1 updates -> agg=177 (avg_tau=9.000, test_loss=1.5445, test_acc=0.4266)\n","[Server] Aggregated 1 updates -> agg=178 (avg_tau=9.000, test_loss=1.5416, test_acc=0.4297)\n","[Server] Aggregated 1 updates -> agg=179 (avg_tau=9.000, test_loss=1.5409, test_acc=0.4292)\n","[Server] Aggregated 1 updates -> agg=180 (avg_tau=9.000, test_loss=1.5384, test_acc=0.4326)\n","[Server] Aggregated 1 updates -> agg=181 (avg_tau=9.000, test_loss=1.5380, test_acc=0.4329)\n","[Server] Aggregated 1 updates -> agg=182 (avg_tau=9.000, test_loss=1.5373, test_acc=0.4321)\n","[Server] Aggregated 1 updates -> agg=183 (avg_tau=9.000, test_loss=1.5295, test_acc=0.4331)\n","[Server] Aggregated 1 updates -> agg=184 (avg_tau=9.000, test_loss=1.5251, test_acc=0.4368)\n","[Server] Aggregated 1 updates -> agg=185 (avg_tau=9.000, test_loss=1.5241, test_acc=0.4364)\n","[Server] Aggregated 1 updates -> agg=186 (avg_tau=9.000, test_loss=1.5315, test_acc=0.4335)\n","[Server] Aggregated 1 updates -> agg=187 (avg_tau=9.000, test_loss=1.5226, test_acc=0.4367)\n","[Server] Aggregated 1 updates -> agg=188 (avg_tau=8.000, test_loss=1.5222, test_acc=0.4361)\n","[Server] Aggregated 1 updates -> agg=189 (avg_tau=9.000, test_loss=1.5172, test_acc=0.4409)\n","[Server] Aggregated 1 updates -> agg=190 (avg_tau=8.000, test_loss=1.5123, test_acc=0.4406)\n","[Server] Aggregated 1 updates -> agg=191 (avg_tau=9.000, test_loss=1.5101, test_acc=0.4407)\n","[Server] Aggregated 1 updates -> agg=192 (avg_tau=9.000, test_loss=1.5090, test_acc=0.4417)\n","[Server] Aggregated 1 updates -> agg=193 (avg_tau=9.000, test_loss=1.5044, test_acc=0.4429)\n","[Server] Aggregated 1 updates -> agg=194 (avg_tau=9.000, test_loss=1.5054, test_acc=0.4413)\n","[Server] Aggregated 1 updates -> agg=195 (avg_tau=9.000, test_loss=1.5096, test_acc=0.4393)\n","[Server] Aggregated 1 updates -> agg=196 (avg_tau=9.000, test_loss=1.5069, test_acc=0.4419)\n","[Server] Aggregated 1 updates -> agg=197 (avg_tau=9.000, test_loss=1.5013, test_acc=0.4431)\n","[Server] Aggregated 1 updates -> agg=198 (avg_tau=9.000, test_loss=1.5011, test_acc=0.4424)\n","[Server] Aggregated 1 updates -> agg=199 (avg_tau=9.000, test_loss=1.5000, test_acc=0.4428)\n","[Server] Aggregated 1 updates -> agg=200 (avg_tau=9.000, test_loss=1.4949, test_acc=0.4439)\n","[Server] Aggregated 1 updates -> agg=201 (avg_tau=9.000, test_loss=1.4949, test_acc=0.4447)\n","[Server] Aggregated 1 updates -> agg=202 (avg_tau=9.000, test_loss=1.4927, test_acc=0.4472)\n","[Server] Aggregated 1 updates -> agg=203 (avg_tau=9.000, test_loss=1.4837, test_acc=0.4509)\n","[Server] Aggregated 1 updates -> agg=204 (avg_tau=9.000, test_loss=1.4778, test_acc=0.4561)\n","[Server] Aggregated 1 updates -> agg=205 (avg_tau=9.000, test_loss=1.4729, test_acc=0.4572)\n","[Server] Aggregated 1 updates -> agg=206 (avg_tau=9.000, test_loss=1.4718, test_acc=0.4581)\n","[Server] Aggregated 1 updates -> agg=207 (avg_tau=9.000, test_loss=1.4681, test_acc=0.4608)\n","[Server] Aggregated 1 updates -> agg=208 (avg_tau=9.000, test_loss=1.4651, test_acc=0.4623)\n","[Server] Aggregated 1 updates -> agg=209 (avg_tau=9.000, test_loss=1.4616, test_acc=0.4617)\n","[Server] Aggregated 1 updates -> agg=210 (avg_tau=9.000, test_loss=1.4579, test_acc=0.4636)\n","[Server] Aggregated 1 updates -> agg=211 (avg_tau=9.000, test_loss=1.4567, test_acc=0.4678)\n","[Server] Aggregated 1 updates -> agg=212 (avg_tau=9.000, test_loss=1.4549, test_acc=0.4680)\n","[Server] Aggregated 1 updates -> agg=213 (avg_tau=9.000, test_loss=1.4502, test_acc=0.4686)\n","[Server] Aggregated 1 updates -> agg=214 (avg_tau=9.000, test_loss=1.4457, test_acc=0.4685)\n","[Server] Aggregated 1 updates -> agg=215 (avg_tau=9.000, test_loss=1.4479, test_acc=0.4687)\n","[Server] Aggregated 1 updates -> agg=216 (avg_tau=9.000, test_loss=1.4465, test_acc=0.4705)\n","[Server] Aggregated 1 updates -> agg=217 (avg_tau=8.000, test_loss=1.4477, test_acc=0.4706)\n","[Server] Aggregated 1 updates -> agg=218 (avg_tau=9.000, test_loss=1.4477, test_acc=0.4708)\n","[Server] Aggregated 1 updates -> agg=219 (avg_tau=9.000, test_loss=1.4476, test_acc=0.4696)\n","[Server] Aggregated 1 updates -> agg=220 (avg_tau=9.000, test_loss=1.4486, test_acc=0.4687)\n","[Server] Aggregated 1 updates -> agg=221 (avg_tau=9.000, test_loss=1.4507, test_acc=0.4693)\n","[Server] Aggregated 1 updates -> agg=222 (avg_tau=9.000, test_loss=1.4494, test_acc=0.4708)\n","[Server] Aggregated 1 updates -> agg=223 (avg_tau=9.000, test_loss=1.4495, test_acc=0.4715)\n","[Server] Aggregated 1 updates -> agg=224 (avg_tau=9.000, test_loss=1.4491, test_acc=0.4732)\n","[Server] Aggregated 1 updates -> agg=225 (avg_tau=9.000, test_loss=1.4472, test_acc=0.4741)\n","[Server] Aggregated 1 updates -> agg=226 (avg_tau=9.000, test_loss=1.4513, test_acc=0.4726)\n","[Server] Aggregated 1 updates -> agg=227 (avg_tau=9.000, test_loss=1.4496, test_acc=0.4712)\n","[Server] Aggregated 1 updates -> agg=228 (avg_tau=9.000, test_loss=1.4555, test_acc=0.4685)\n","[Server] Aggregated 1 updates -> agg=229 (avg_tau=9.000, test_loss=1.4513, test_acc=0.4708)\n","[Server] Aggregated 1 updates -> agg=230 (avg_tau=9.000, test_loss=1.4472, test_acc=0.4715)\n","[Server] Aggregated 1 updates -> agg=231 (avg_tau=9.000, test_loss=1.4464, test_acc=0.4705)\n","[Server] Aggregated 1 updates -> agg=232 (avg_tau=9.000, test_loss=1.4425, test_acc=0.4738)\n","[Server] Aggregated 1 updates -> agg=233 (avg_tau=8.000, test_loss=1.4418, test_acc=0.4716)\n","[Server] Aggregated 1 updates -> agg=234 (avg_tau=9.000, test_loss=1.4389, test_acc=0.4749)\n","[Server] Aggregated 1 updates -> agg=235 (avg_tau=9.000, test_loss=1.4336, test_acc=0.4747)\n","[Server] Aggregated 1 updates -> agg=236 (avg_tau=9.000, test_loss=1.4282, test_acc=0.4772)\n","[Server] Aggregated 1 updates -> agg=237 (avg_tau=9.000, test_loss=1.4304, test_acc=0.4763)\n","[Server] Aggregated 1 updates -> agg=238 (avg_tau=8.000, test_loss=1.4222, test_acc=0.4762)\n","[Server] Aggregated 1 updates -> agg=239 (avg_tau=9.000, test_loss=1.4208, test_acc=0.4777)\n","[Server] Aggregated 1 updates -> agg=240 (avg_tau=9.000, test_loss=1.4175, test_acc=0.4770)\n","[Server] Aggregated 1 updates -> agg=241 (avg_tau=9.000, test_loss=1.4190, test_acc=0.4774)\n","[Server] Aggregated 1 updates -> agg=242 (avg_tau=9.000, test_loss=1.4190, test_acc=0.4777)\n","[Server] Aggregated 1 updates -> agg=243 (avg_tau=9.000, test_loss=1.4153, test_acc=0.4781)\n","[Server] Aggregated 1 updates -> agg=244 (avg_tau=8.000, test_loss=1.4091, test_acc=0.4801)\n","[Server] Aggregated 1 updates -> agg=245 (avg_tau=8.000, test_loss=1.4097, test_acc=0.4813)\n","[Server] Aggregated 1 updates -> agg=246 (avg_tau=9.000, test_loss=1.4068, test_acc=0.4814)\n","[Server] Aggregated 1 updates -> agg=247 (avg_tau=9.000, test_loss=1.4073, test_acc=0.4809)\n","[Server] Aggregated 1 updates -> agg=248 (avg_tau=9.000, test_loss=1.4049, test_acc=0.4807)\n","[Server] Aggregated 1 updates -> agg=249 (avg_tau=9.000, test_loss=1.4052, test_acc=0.4813)\n","[Server] Aggregated 1 updates -> agg=250 (avg_tau=8.000, test_loss=1.4017, test_acc=0.4844)\n","[Server] Aggregated 1 updates -> agg=251 (avg_tau=8.000, test_loss=1.4002, test_acc=0.4841)\n","[Server] Aggregated 1 updates -> agg=252 (avg_tau=9.000, test_loss=1.3985, test_acc=0.4834)\n","[Server] Aggregated 1 updates -> agg=253 (avg_tau=9.000, test_loss=1.3955, test_acc=0.4853)\n","[Server] Aggregated 1 updates -> agg=254 (avg_tau=9.000, test_loss=1.3935, test_acc=0.4865)\n","[Server] Aggregated 1 updates -> agg=255 (avg_tau=9.000, test_loss=1.3920, test_acc=0.4883)\n","[Server] Aggregated 1 updates -> agg=256 (avg_tau=9.000, test_loss=1.3916, test_acc=0.4892)\n","[Server] Aggregated 1 updates -> agg=257 (avg_tau=9.000, test_loss=1.3906, test_acc=0.4885)\n","[Server] Aggregated 1 updates -> agg=258 (avg_tau=9.000, test_loss=1.3876, test_acc=0.4887)\n","[Server] Aggregated 1 updates -> agg=259 (avg_tau=9.000, test_loss=1.3838, test_acc=0.4901)\n","[Server] Aggregated 1 updates -> agg=260 (avg_tau=9.000, test_loss=1.3870, test_acc=0.4881)\n","[Server] Aggregated 1 updates -> agg=261 (avg_tau=9.000, test_loss=1.3892, test_acc=0.4888)\n","[Server] Aggregated 1 updates -> agg=262 (avg_tau=9.000, test_loss=1.3949, test_acc=0.4868)\n","[Server] Aggregated 1 updates -> agg=263 (avg_tau=9.000, test_loss=1.3887, test_acc=0.4889)\n","[Server] Aggregated 1 updates -> agg=264 (avg_tau=9.000, test_loss=1.3869, test_acc=0.4898)\n","[Server] Aggregated 1 updates -> agg=265 (avg_tau=9.000, test_loss=1.3855, test_acc=0.4912)\n","[Server] Aggregated 1 updates -> agg=266 (avg_tau=9.000, test_loss=1.3833, test_acc=0.4919)\n","[Server] Aggregated 1 updates -> agg=267 (avg_tau=9.000, test_loss=1.3823, test_acc=0.4908)\n","[Server] Aggregated 1 updates -> agg=268 (avg_tau=9.000, test_loss=1.3849, test_acc=0.4902)\n","[Server] Aggregated 1 updates -> agg=269 (avg_tau=9.000, test_loss=1.3859, test_acc=0.4892)\n","[Server] Aggregated 1 updates -> agg=270 (avg_tau=9.000, test_loss=1.3829, test_acc=0.4901)\n","[Server] Aggregated 1 updates -> agg=271 (avg_tau=9.000, test_loss=1.3868, test_acc=0.4885)\n","[Server] Aggregated 1 updates -> agg=272 (avg_tau=9.000, test_loss=1.3885, test_acc=0.4884)\n","[Server] Aggregated 1 updates -> agg=273 (avg_tau=9.000, test_loss=1.3904, test_acc=0.4874)\n","[Server] Aggregated 1 updates -> agg=274 (avg_tau=8.000, test_loss=1.3941, test_acc=0.4869)\n","[Server] Aggregated 1 updates -> agg=275 (avg_tau=9.000, test_loss=1.4047, test_acc=0.4842)\n","[Server] Aggregated 1 updates -> agg=276 (avg_tau=9.000, test_loss=1.4008, test_acc=0.4854)\n","[Server] Aggregated 1 updates -> agg=277 (avg_tau=9.000, test_loss=1.4028, test_acc=0.4862)\n","[Server] Aggregated 1 updates -> agg=278 (avg_tau=9.000, test_loss=1.4012, test_acc=0.4860)\n","[Server] Aggregated 1 updates -> agg=279 (avg_tau=9.000, test_loss=1.3986, test_acc=0.4877)\n","[Server] Aggregated 1 updates -> agg=280 (avg_tau=9.000, test_loss=1.3960, test_acc=0.4900)\n","[Server] Aggregated 1 updates -> agg=281 (avg_tau=8.000, test_loss=1.3998, test_acc=0.4895)\n","[Server] Aggregated 1 updates -> agg=282 (avg_tau=9.000, test_loss=1.3954, test_acc=0.4892)\n","[Server] Aggregated 1 updates -> agg=283 (avg_tau=9.000, test_loss=1.3942, test_acc=0.4906)\n","[Server] Aggregated 1 updates -> agg=284 (avg_tau=9.000, test_loss=1.3925, test_acc=0.4899)\n","[Server] Aggregated 1 updates -> agg=285 (avg_tau=9.000, test_loss=1.3962, test_acc=0.4913)\n","[Server] Aggregated 1 updates -> agg=286 (avg_tau=8.000, test_loss=1.3946, test_acc=0.4907)\n","[Server] Aggregated 1 updates -> agg=287 (avg_tau=8.000, test_loss=1.3899, test_acc=0.4925)\n","[Server] Aggregated 1 updates -> agg=288 (avg_tau=9.000, test_loss=1.3888, test_acc=0.4914)\n","[Server] Aggregated 1 updates -> agg=289 (avg_tau=9.000, test_loss=1.3855, test_acc=0.4923)\n","[Server] Aggregated 1 updates -> agg=290 (avg_tau=9.000, test_loss=1.3790, test_acc=0.4945)\n","[Server] Aggregated 1 updates -> agg=291 (avg_tau=9.000, test_loss=1.3704, test_acc=0.4953)\n","[Server] Aggregated 1 updates -> agg=292 (avg_tau=9.000, test_loss=1.3687, test_acc=0.4962)\n","[Server] Aggregated 1 updates -> agg=293 (avg_tau=9.000, test_loss=1.3616, test_acc=0.4992)\n","[Server] Aggregated 1 updates -> agg=294 (avg_tau=9.000, test_loss=1.3635, test_acc=0.4995)\n","[Server] Aggregated 1 updates -> agg=295 (avg_tau=9.000, test_loss=1.3629, test_acc=0.4988)\n","[Server] Aggregated 1 updates -> agg=296 (avg_tau=9.000, test_loss=1.3616, test_acc=0.4981)\n","[Server] Aggregated 1 updates -> agg=297 (avg_tau=9.000, test_loss=1.3618, test_acc=0.4974)\n","[Server] Aggregated 1 updates -> agg=298 (avg_tau=8.000, test_loss=1.3608, test_acc=0.4968)\n","[Server] Aggregated 1 updates -> agg=299 (avg_tau=9.000, test_loss=1.3584, test_acc=0.4979)\n","[Server] Aggregated 1 updates -> agg=300 (avg_tau=9.000, test_loss=1.3585, test_acc=0.4985)\n","[Server] Aggregated 1 updates -> agg=301 (avg_tau=9.000, test_loss=1.3575, test_acc=0.4987)\n","[Server] Aggregated 1 updates -> agg=302 (avg_tau=9.000, test_loss=1.3580, test_acc=0.4984)\n","[Server] Aggregated 1 updates -> agg=303 (avg_tau=8.000, test_loss=1.3578, test_acc=0.4963)\n","[Server] Aggregated 1 updates -> agg=304 (avg_tau=9.000, test_loss=1.3560, test_acc=0.4967)\n","[Server] Aggregated 1 updates -> agg=305 (avg_tau=9.000, test_loss=1.3516, test_acc=0.4987)\n","[Server] Aggregated 1 updates -> agg=306 (avg_tau=9.000, test_loss=1.3514, test_acc=0.4987)\n","[Server] Aggregated 1 updates -> agg=307 (avg_tau=9.000, test_loss=1.3464, test_acc=0.5014)\n","[Server] Aggregated 1 updates -> agg=308 (avg_tau=9.000, test_loss=1.3450, test_acc=0.5019)\n","[Server] Aggregated 1 updates -> agg=309 (avg_tau=9.000, test_loss=1.3416, test_acc=0.5045)\n","[Server] Aggregated 1 updates -> agg=310 (avg_tau=9.000, test_loss=1.3436, test_acc=0.5045)\n","[Server] Aggregated 1 updates -> agg=311 (avg_tau=9.000, test_loss=1.3397, test_acc=0.5031)\n","[Server] Aggregated 1 updates -> agg=312 (avg_tau=9.000, test_loss=1.3346, test_acc=0.5073)\n","[Server] Aggregated 1 updates -> agg=313 (avg_tau=9.000, test_loss=1.3361, test_acc=0.5047)\n","[Server] Aggregated 1 updates -> agg=314 (avg_tau=9.000, test_loss=1.3302, test_acc=0.5069)\n","[Server] Aggregated 1 updates -> agg=315 (avg_tau=8.000, test_loss=1.3322, test_acc=0.5075)\n","[Server] Aggregated 1 updates -> agg=316 (avg_tau=9.000, test_loss=1.3308, test_acc=0.5073)\n","[Server] Aggregated 1 updates -> agg=317 (avg_tau=9.000, test_loss=1.3327, test_acc=0.5091)\n","[Server] Aggregated 1 updates -> agg=318 (avg_tau=9.000, test_loss=1.3269, test_acc=0.5109)\n","[Server] Aggregated 1 updates -> agg=319 (avg_tau=9.000, test_loss=1.3270, test_acc=0.5095)\n","[Server] Aggregated 1 updates -> agg=320 (avg_tau=9.000, test_loss=1.3247, test_acc=0.5116)\n","[Server] Aggregated 1 updates -> agg=321 (avg_tau=9.000, test_loss=1.3239, test_acc=0.5102)\n","[Server] Aggregated 1 updates -> agg=322 (avg_tau=9.000, test_loss=1.3178, test_acc=0.5124)\n","[Server] Aggregated 1 updates -> agg=323 (avg_tau=9.000, test_loss=1.3226, test_acc=0.5115)\n","[Server] Aggregated 1 updates -> agg=324 (avg_tau=8.000, test_loss=1.3196, test_acc=0.5127)\n","[Server] Aggregated 1 updates -> agg=325 (avg_tau=8.000, test_loss=1.3126, test_acc=0.5152)\n","[Server] Aggregated 1 updates -> agg=326 (avg_tau=9.000, test_loss=1.3239, test_acc=0.5115)\n","[Server] Aggregated 1 updates -> agg=327 (avg_tau=8.000, test_loss=1.3217, test_acc=0.5126)\n","[Server] Aggregated 1 updates -> agg=328 (avg_tau=9.000, test_loss=1.3213, test_acc=0.5145)\n","[Server] Aggregated 1 updates -> agg=329 (avg_tau=9.000, test_loss=1.3178, test_acc=0.5155)\n","[Server] Aggregated 1 updates -> agg=330 (avg_tau=9.000, test_loss=1.3228, test_acc=0.5140)\n","[Server] Aggregated 1 updates -> agg=331 (avg_tau=9.000, test_loss=1.3209, test_acc=0.5129)\n","[Server] Aggregated 1 updates -> agg=332 (avg_tau=9.000, test_loss=1.3212, test_acc=0.5132)\n","[Server] Aggregated 1 updates -> agg=333 (avg_tau=9.000, test_loss=1.3229, test_acc=0.5145)\n","[Server] Aggregated 1 updates -> agg=334 (avg_tau=8.000, test_loss=1.3198, test_acc=0.5135)\n","[Server] Aggregated 1 updates -> agg=335 (avg_tau=9.000, test_loss=1.3202, test_acc=0.5132)\n","[Server] Aggregated 1 updates -> agg=336 (avg_tau=8.000, test_loss=1.3189, test_acc=0.5145)\n","[Server] Aggregated 1 updates -> agg=337 (avg_tau=9.000, test_loss=1.3098, test_acc=0.5180)\n","[Server] Aggregated 1 updates -> agg=338 (avg_tau=9.000, test_loss=1.3100, test_acc=0.5165)\n","[Server] Aggregated 1 updates -> agg=339 (avg_tau=9.000, test_loss=1.3104, test_acc=0.5182)\n","[Server] Aggregated 1 updates -> agg=340 (avg_tau=8.000, test_loss=1.3145, test_acc=0.5167)\n","[Server] Aggregated 1 updates -> agg=341 (avg_tau=9.000, test_loss=1.3151, test_acc=0.5164)\n","[Server] Aggregated 1 updates -> agg=342 (avg_tau=9.000, test_loss=1.3172, test_acc=0.5163)\n","[Server] Aggregated 1 updates -> agg=343 (avg_tau=9.000, test_loss=1.3169, test_acc=0.5179)\n","[Server] Aggregated 1 updates -> agg=344 (avg_tau=9.000, test_loss=1.3193, test_acc=0.5167)\n","[Server] Aggregated 1 updates -> agg=345 (avg_tau=9.000, test_loss=1.3162, test_acc=0.5182)\n","[Server] Aggregated 1 updates -> agg=346 (avg_tau=9.000, test_loss=1.3161, test_acc=0.5191)\n","[Server] Aggregated 1 updates -> agg=347 (avg_tau=9.000, test_loss=1.3175, test_acc=0.5180)\n","[Server] Aggregated 1 updates -> agg=348 (avg_tau=9.000, test_loss=1.3227, test_acc=0.5158)\n","[Server] Aggregated 1 updates -> agg=349 (avg_tau=9.000, test_loss=1.3274, test_acc=0.5135)\n","[Server] Aggregated 1 updates -> agg=350 (avg_tau=9.000, test_loss=1.3250, test_acc=0.5157)\n","[Server] Aggregated 1 updates -> agg=351 (avg_tau=9.000, test_loss=1.3228, test_acc=0.5158)\n","[Server] Aggregated 1 updates -> agg=352 (avg_tau=9.000, test_loss=1.3177, test_acc=0.5199)\n","[Server] Aggregated 1 updates -> agg=353 (avg_tau=9.000, test_loss=1.3149, test_acc=0.5191)\n","[Server] Aggregated 1 updates -> agg=354 (avg_tau=9.000, test_loss=1.3158, test_acc=0.5179)\n","[Server] Aggregated 1 updates -> agg=355 (avg_tau=9.000, test_loss=1.3166, test_acc=0.5170)\n","[Server] Aggregated 1 updates -> agg=356 (avg_tau=9.000, test_loss=1.3099, test_acc=0.5186)\n","[Server] Aggregated 1 updates -> agg=357 (avg_tau=9.000, test_loss=1.3091, test_acc=0.5194)\n","[Server] Aggregated 1 updates -> agg=358 (avg_tau=9.000, test_loss=1.3093, test_acc=0.5185)\n","[Server] Aggregated 1 updates -> agg=359 (avg_tau=9.000, test_loss=1.3085, test_acc=0.5191)\n","[Server] Aggregated 1 updates -> agg=360 (avg_tau=9.000, test_loss=1.3091, test_acc=0.5194)\n","[Server] Aggregated 1 updates -> agg=361 (avg_tau=9.000, test_loss=1.3026, test_acc=0.5204)\n","[Server] Aggregated 1 updates -> agg=362 (avg_tau=9.000, test_loss=1.2965, test_acc=0.5225)\n","[Server] Aggregated 1 updates -> agg=363 (avg_tau=9.000, test_loss=1.2976, test_acc=0.5239)\n","[Server] Aggregated 1 updates -> agg=364 (avg_tau=9.000, test_loss=1.2967, test_acc=0.5223)\n","[Server] Aggregated 1 updates -> agg=365 (avg_tau=8.000, test_loss=1.2930, test_acc=0.5226)\n","[Server] Aggregated 1 updates -> agg=366 (avg_tau=9.000, test_loss=1.2943, test_acc=0.5252)\n","[Server] Aggregated 1 updates -> agg=367 (avg_tau=9.000, test_loss=1.2882, test_acc=0.5266)\n","[Server] Aggregated 1 updates -> agg=368 (avg_tau=9.000, test_loss=1.2823, test_acc=0.5303)\n","[Server] Aggregated 1 updates -> agg=369 (avg_tau=8.000, test_loss=1.2755, test_acc=0.5322)\n","[Server] Aggregated 1 updates -> agg=370 (avg_tau=9.000, test_loss=1.2749, test_acc=0.5326)\n","[Server] Aggregated 1 updates -> agg=371 (avg_tau=9.000, test_loss=1.2760, test_acc=0.5309)\n","[Server] Aggregated 1 updates -> agg=372 (avg_tau=9.000, test_loss=1.2741, test_acc=0.5312)\n","[Server] Aggregated 1 updates -> agg=373 (avg_tau=9.000, test_loss=1.2745, test_acc=0.5320)\n","[Server] Aggregated 1 updates -> agg=374 (avg_tau=9.000, test_loss=1.2763, test_acc=0.5308)\n","[Server] Aggregated 1 updates -> agg=375 (avg_tau=9.000, test_loss=1.2785, test_acc=0.5330)\n","[Server] Aggregated 1 updates -> agg=376 (avg_tau=9.000, test_loss=1.2799, test_acc=0.5315)\n","[Server] Aggregated 1 updates -> agg=377 (avg_tau=8.000, test_loss=1.2753, test_acc=0.5325)\n","[Server] Aggregated 1 updates -> agg=378 (avg_tau=9.000, test_loss=1.2748, test_acc=0.5326)\n","[Server] Aggregated 1 updates -> agg=379 (avg_tau=9.000, test_loss=1.2758, test_acc=0.5326)\n","[Server] Aggregated 1 updates -> agg=380 (avg_tau=9.000, test_loss=1.2724, test_acc=0.5350)\n","[Server] Aggregated 1 updates -> agg=381 (avg_tau=9.000, test_loss=1.2723, test_acc=0.5341)\n","[Server] Aggregated 1 updates -> agg=382 (avg_tau=9.000, test_loss=1.2754, test_acc=0.5345)\n","[Server] Aggregated 1 updates -> agg=383 (avg_tau=9.000, test_loss=1.2751, test_acc=0.5348)\n","[Server] Aggregated 1 updates -> agg=384 (avg_tau=8.000, test_loss=1.2762, test_acc=0.5346)\n","[Server] Aggregated 1 updates -> agg=385 (avg_tau=9.000, test_loss=1.2740, test_acc=0.5355)\n","[Server] Aggregated 1 updates -> agg=386 (avg_tau=9.000, test_loss=1.2769, test_acc=0.5337)\n","[Server] Aggregated 1 updates -> agg=387 (avg_tau=9.000, test_loss=1.2785, test_acc=0.5320)\n","[Server] Aggregated 1 updates -> agg=388 (avg_tau=9.000, test_loss=1.2795, test_acc=0.5311)\n","[Server] Aggregated 1 updates -> agg=389 (avg_tau=9.000, test_loss=1.2750, test_acc=0.5349)\n","[Server] Aggregated 1 updates -> agg=390 (avg_tau=9.000, test_loss=1.2712, test_acc=0.5339)\n","[Server] Aggregated 1 updates -> agg=391 (avg_tau=9.000, test_loss=1.2750, test_acc=0.5332)\n","[Server] Aggregated 1 updates -> agg=392 (avg_tau=9.000, test_loss=1.2749, test_acc=0.5324)\n","[Server] Aggregated 1 updates -> agg=393 (avg_tau=8.000, test_loss=1.2682, test_acc=0.5332)\n","[Server] Aggregated 1 updates -> agg=394 (avg_tau=9.000, test_loss=1.2690, test_acc=0.5338)\n","[Server] Aggregated 1 updates -> agg=395 (avg_tau=9.000, test_loss=1.2726, test_acc=0.5351)\n","[Server] Aggregated 1 updates -> agg=396 (avg_tau=9.000, test_loss=1.2663, test_acc=0.5382)\n","[Server] Aggregated 1 updates -> agg=397 (avg_tau=9.000, test_loss=1.2671, test_acc=0.5386)\n","[Server] Aggregated 1 updates -> agg=398 (avg_tau=9.000, test_loss=1.2643, test_acc=0.5387)\n","[Server] Aggregated 1 updates -> agg=399 (avg_tau=9.000, test_loss=1.2629, test_acc=0.5365)\n","[Server] Aggregated 1 updates -> agg=400 (avg_tau=9.000, test_loss=1.2602, test_acc=0.5382)\n","[Server] Aggregated 1 updates -> agg=401 (avg_tau=9.000, test_loss=1.2643, test_acc=0.5368)\n","[Server] Aggregated 1 updates -> agg=402 (avg_tau=9.000, test_loss=1.2608, test_acc=0.5386)\n","[Server] Aggregated 1 updates -> agg=403 (avg_tau=9.000, test_loss=1.2616, test_acc=0.5378)\n","[Server] Aggregated 1 updates -> agg=404 (avg_tau=9.000, test_loss=1.2629, test_acc=0.5372)\n","[Server] Aggregated 1 updates -> agg=405 (avg_tau=9.000, test_loss=1.2676, test_acc=0.5364)\n","[Server] Aggregated 1 updates -> agg=406 (avg_tau=9.000, test_loss=1.2735, test_acc=0.5340)\n","[Server] Aggregated 1 updates -> agg=407 (avg_tau=9.000, test_loss=1.2765, test_acc=0.5341)\n","[Server] Aggregated 1 updates -> agg=408 (avg_tau=9.000, test_loss=1.2787, test_acc=0.5344)\n","[Server] Aggregated 1 updates -> agg=409 (avg_tau=9.000, test_loss=1.2756, test_acc=0.5345)\n","[Server] Aggregated 1 updates -> agg=410 (avg_tau=9.000, test_loss=1.2667, test_acc=0.5375)\n","[Server] Aggregated 1 updates -> agg=411 (avg_tau=9.000, test_loss=1.2619, test_acc=0.5403)\n","[Server] Aggregated 1 updates -> agg=412 (avg_tau=9.000, test_loss=1.2617, test_acc=0.5397)\n","[Server] Aggregated 1 updates -> agg=413 (avg_tau=9.000, test_loss=1.2638, test_acc=0.5377)\n","[Server] Aggregated 1 updates -> agg=414 (avg_tau=9.000, test_loss=1.2714, test_acc=0.5355)\n","[Server] Aggregated 1 updates -> agg=415 (avg_tau=8.000, test_loss=1.2715, test_acc=0.5363)\n","[Server] Aggregated 1 updates -> agg=416 (avg_tau=9.000, test_loss=1.2750, test_acc=0.5342)\n","[Server] Aggregated 1 updates -> agg=417 (avg_tau=9.000, test_loss=1.2670, test_acc=0.5373)\n","[Server] Aggregated 1 updates -> agg=418 (avg_tau=9.000, test_loss=1.2708, test_acc=0.5380)\n","[Server] Aggregated 1 updates -> agg=419 (avg_tau=9.000, test_loss=1.2640, test_acc=0.5370)\n","[Server] Aggregated 1 updates -> agg=420 (avg_tau=9.000, test_loss=1.2690, test_acc=0.5371)\n","[Server] Aggregated 1 updates -> agg=421 (avg_tau=9.000, test_loss=1.2651, test_acc=0.5380)\n","[Server] Aggregated 1 updates -> agg=422 (avg_tau=9.000, test_loss=1.2625, test_acc=0.5412)\n","[Server] Aggregated 1 updates -> agg=423 (avg_tau=9.000, test_loss=1.2597, test_acc=0.5400)\n","[Server] Aggregated 1 updates -> agg=424 (avg_tau=9.000, test_loss=1.2583, test_acc=0.5407)\n","[Server] Aggregated 1 updates -> agg=425 (avg_tau=9.000, test_loss=1.2559, test_acc=0.5416)\n","[Server] Aggregated 1 updates -> agg=426 (avg_tau=9.000, test_loss=1.2520, test_acc=0.5427)\n","[Server] Aggregated 1 updates -> agg=427 (avg_tau=9.000, test_loss=1.2497, test_acc=0.5431)\n","[Server] Aggregated 1 updates -> agg=428 (avg_tau=9.000, test_loss=1.2532, test_acc=0.5419)\n","[Server] Aggregated 1 updates -> agg=429 (avg_tau=9.000, test_loss=1.2471, test_acc=0.5435)\n","[Server] Aggregated 1 updates -> agg=430 (avg_tau=9.000, test_loss=1.2466, test_acc=0.5431)\n","[Server] Aggregated 1 updates -> agg=431 (avg_tau=9.000, test_loss=1.2459, test_acc=0.5443)\n","[Server] Aggregated 1 updates -> agg=432 (avg_tau=9.000, test_loss=1.2449, test_acc=0.5450)\n","[Server] Aggregated 1 updates -> agg=433 (avg_tau=9.000, test_loss=1.2459, test_acc=0.5438)\n","[Server] Aggregated 1 updates -> agg=434 (avg_tau=9.000, test_loss=1.2415, test_acc=0.5441)\n","[Server] Aggregated 1 updates -> agg=435 (avg_tau=9.000, test_loss=1.2402, test_acc=0.5464)\n","[Server] Aggregated 1 updates -> agg=436 (avg_tau=8.000, test_loss=1.2413, test_acc=0.5464)\n","[Server] Aggregated 1 updates -> agg=437 (avg_tau=8.000, test_loss=1.2438, test_acc=0.5462)\n","[Server] Aggregated 1 updates -> agg=438 (avg_tau=9.000, test_loss=1.2434, test_acc=0.5453)\n","[Server] Aggregated 1 updates -> agg=439 (avg_tau=8.000, test_loss=1.2426, test_acc=0.5445)\n","[Server] Aggregated 1 updates -> agg=440 (avg_tau=9.000, test_loss=1.2396, test_acc=0.5454)\n","[Server] Aggregated 1 updates -> agg=441 (avg_tau=9.000, test_loss=1.2389, test_acc=0.5465)\n","[Server] Aggregated 1 updates -> agg=442 (avg_tau=9.000, test_loss=1.2391, test_acc=0.5493)\n","[Server] Aggregated 1 updates -> agg=443 (avg_tau=9.000, test_loss=1.2411, test_acc=0.5484)\n","[Server] Aggregated 1 updates -> agg=444 (avg_tau=9.000, test_loss=1.2428, test_acc=0.5482)\n","[Server] Aggregated 1 updates -> agg=445 (avg_tau=9.000, test_loss=1.2449, test_acc=0.5478)\n","[Server] Aggregated 1 updates -> agg=446 (avg_tau=9.000, test_loss=1.2465, test_acc=0.5479)\n","[Server] Aggregated 1 updates -> agg=447 (avg_tau=9.000, test_loss=1.2443, test_acc=0.5483)\n","[Server] Aggregated 1 updates -> agg=448 (avg_tau=9.000, test_loss=1.2388, test_acc=0.5480)\n","[Server] Aggregated 1 updates -> agg=449 (avg_tau=9.000, test_loss=1.2363, test_acc=0.5498)\n","[Server] Aggregated 1 updates -> agg=450 (avg_tau=9.000, test_loss=1.2300, test_acc=0.5518)\n","[Server] Aggregated 1 updates -> agg=451 (avg_tau=8.000, test_loss=1.2321, test_acc=0.5513)\n","[Server] Aggregated 1 updates -> agg=452 (avg_tau=9.000, test_loss=1.2302, test_acc=0.5512)\n","[Server] Aggregated 1 updates -> agg=453 (avg_tau=9.000, test_loss=1.2313, test_acc=0.5512)\n","[Server] Aggregated 1 updates -> agg=454 (avg_tau=9.000, test_loss=1.2319, test_acc=0.5516)\n","[Server] Aggregated 1 updates -> agg=455 (avg_tau=9.000, test_loss=1.2331, test_acc=0.5511)\n","[Server] Aggregated 1 updates -> agg=456 (avg_tau=9.000, test_loss=1.2286, test_acc=0.5520)\n","[Server] Aggregated 1 updates -> agg=457 (avg_tau=9.000, test_loss=1.2228, test_acc=0.5541)\n","[Server] Aggregated 1 updates -> agg=458 (avg_tau=9.000, test_loss=1.2229, test_acc=0.5540)\n","[Server] Aggregated 1 updates -> agg=459 (avg_tau=9.000, test_loss=1.2245, test_acc=0.5547)\n","[Server] Aggregated 1 updates -> agg=460 (avg_tau=9.000, test_loss=1.2206, test_acc=0.5543)\n","[Server] Aggregated 1 updates -> agg=461 (avg_tau=9.000, test_loss=1.2187, test_acc=0.5533)\n","[Server] Aggregated 1 updates -> agg=462 (avg_tau=9.000, test_loss=1.2246, test_acc=0.5532)\n","[Server] Aggregated 1 updates -> agg=463 (avg_tau=9.000, test_loss=1.2225, test_acc=0.5515)\n","[Server] Aggregated 1 updates -> agg=464 (avg_tau=9.000, test_loss=1.2168, test_acc=0.5530)\n","[Server] Aggregated 1 updates -> agg=465 (avg_tau=9.000, test_loss=1.2144, test_acc=0.5557)\n","[Server] Aggregated 1 updates -> agg=466 (avg_tau=9.000, test_loss=1.2122, test_acc=0.5577)\n","[Server] Aggregated 1 updates -> agg=467 (avg_tau=9.000, test_loss=1.2112, test_acc=0.5570)\n","[Server] Aggregated 1 updates -> agg=468 (avg_tau=9.000, test_loss=1.2091, test_acc=0.5598)\n","[Server] Aggregated 1 updates -> agg=469 (avg_tau=9.000, test_loss=1.2108, test_acc=0.5579)\n","[Server] Aggregated 1 updates -> agg=470 (avg_tau=9.000, test_loss=1.2070, test_acc=0.5603)\n","[Server] Aggregated 1 updates -> agg=471 (avg_tau=9.000, test_loss=1.2105, test_acc=0.5584)\n","[Server] Aggregated 1 updates -> agg=472 (avg_tau=9.000, test_loss=1.2082, test_acc=0.5592)\n","[Server] Aggregated 1 updates -> agg=473 (avg_tau=9.000, test_loss=1.2048, test_acc=0.5598)\n","[Server] Aggregated 1 updates -> agg=474 (avg_tau=8.000, test_loss=1.2028, test_acc=0.5611)\n","[Server] Aggregated 1 updates -> agg=475 (avg_tau=9.000, test_loss=1.1999, test_acc=0.5620)\n","[Server] Aggregated 1 updates -> agg=476 (avg_tau=9.000, test_loss=1.2037, test_acc=0.5623)\n","[Server] Aggregated 1 updates -> agg=477 (avg_tau=9.000, test_loss=1.2029, test_acc=0.5620)\n","[Server] Aggregated 1 updates -> agg=478 (avg_tau=9.000, test_loss=1.2035, test_acc=0.5628)\n","[Server] Aggregated 1 updates -> agg=479 (avg_tau=9.000, test_loss=1.2065, test_acc=0.5611)\n","[Server] Aggregated 1 updates -> agg=480 (avg_tau=9.000, test_loss=1.2050, test_acc=0.5611)\n","[Server] Aggregated 1 updates -> agg=481 (avg_tau=9.000, test_loss=1.2030, test_acc=0.5613)\n","[Server] Aggregated 1 updates -> agg=482 (avg_tau=9.000, test_loss=1.2071, test_acc=0.5614)\n","[Server] Aggregated 1 updates -> agg=483 (avg_tau=9.000, test_loss=1.2061, test_acc=0.5614)\n","[Server] Aggregated 1 updates -> agg=484 (avg_tau=9.000, test_loss=1.2064, test_acc=0.5627)\n","[Server] Aggregated 1 updates -> agg=485 (avg_tau=9.000, test_loss=1.2090, test_acc=0.5614)\n","[Server] Aggregated 1 updates -> agg=486 (avg_tau=9.000, test_loss=1.2098, test_acc=0.5622)\n","[Server] Aggregated 1 updates -> agg=487 (avg_tau=9.000, test_loss=1.2067, test_acc=0.5619)\n","[Server] Aggregated 1 updates -> agg=488 (avg_tau=9.000, test_loss=1.2106, test_acc=0.5624)\n","[Server] Aggregated 1 updates -> agg=489 (avg_tau=9.000, test_loss=1.2021, test_acc=0.5633)\n","[Server] Aggregated 1 updates -> agg=490 (avg_tau=9.000, test_loss=1.1995, test_acc=0.5657)\n","[Server] Aggregated 1 updates -> agg=491 (avg_tau=8.000, test_loss=1.2044, test_acc=0.5640)\n","[Server] Aggregated 1 updates -> agg=492 (avg_tau=9.000, test_loss=1.1992, test_acc=0.5643)\n","[Server] Aggregated 1 updates -> agg=493 (avg_tau=9.000, test_loss=1.2014, test_acc=0.5645)\n","[Server] Aggregated 1 updates -> agg=494 (avg_tau=9.000, test_loss=1.2073, test_acc=0.5620)\n","[Server] Aggregated 1 updates -> agg=495 (avg_tau=9.000, test_loss=1.2081, test_acc=0.5631)\n","[Server] Aggregated 1 updates -> agg=496 (avg_tau=9.000, test_loss=1.2024, test_acc=0.5662)\n","[Server] Aggregated 1 updates -> agg=497 (avg_tau=9.000, test_loss=1.2003, test_acc=0.5664)\n","[Server] Aggregated 1 updates -> agg=498 (avg_tau=9.000, test_loss=1.1970, test_acc=0.5665)\n","[Server] Aggregated 1 updates -> agg=499 (avg_tau=9.000, test_loss=1.2001, test_acc=0.5667)\n","[Server] Aggregated 1 updates -> agg=500 (avg_tau=9.000, test_loss=1.2020, test_acc=0.5633)\n","[Server] Stopping: max aggregation rounds reached\n","[Server] Aggregated 1 updates -> agg=501 (avg_tau=9.000, test_loss=1.1990, test_acc=0.5640)\n","[Server] Aggregated 1 updates -> agg=502 (avg_tau=9.000, test_loss=1.2019, test_acc=0.5637)\n","[exp] clients=40 -> logs at /content/drive/MyDrive/TrustWeightColab/logs/TrustWeightClientCountExp/clients_40\n","[Server] Aggregated 1 updates -> agg=503 (avg_tau=9.000, test_loss=1.1966, test_acc=0.5660)\n","[Server] Aggregated 1 updates -> agg=504 (avg_tau=9.000, test_loss=1.1962, test_acc=0.5649)\n","[Server] Aggregated 1 updates -> agg=505 (avg_tau=9.000, test_loss=1.1919, test_acc=0.5667)\n","[Server] Aggregated 1 updates -> agg=506 (avg_tau=9.000, test_loss=1.1962, test_acc=0.5661)\n","[Server] Aggregated 1 updates -> agg=507 (avg_tau=9.000, test_loss=1.1963, test_acc=0.5673)\n","[Server] Aggregated 1 updates -> agg=508 (avg_tau=9.000, test_loss=1.2059, test_acc=0.5627)\n","[Server] Aggregated 1 updates -> agg=509 (avg_tau=9.000, test_loss=1.2050, test_acc=0.5632)\n","[Server] Aggregated 1 updates -> agg=1 (avg_tau=0.000, test_loss=2.4579, test_acc=0.1065)\n","[Server] Aggregated 5 updates -> agg=2 (avg_tau=1.000, test_loss=2.3852, test_acc=0.1050)\n","[Server] Aggregated 5 updates -> agg=3 (avg_tau=1.800, test_loss=2.3381, test_acc=0.1055)\n","[Server] Aggregated 5 updates -> agg=4 (avg_tau=2.400, test_loss=2.3083, test_acc=0.1073)\n","[Server] Aggregated 3 updates -> agg=5 (avg_tau=3.000, test_loss=2.2906, test_acc=0.1123)\n","[Server] Aggregated 5 updates -> agg=6 (avg_tau=3.800, test_loss=2.2811, test_acc=0.1274)\n","[Server] Aggregated 2 updates -> agg=7 (avg_tau=4.000, test_loss=2.2764, test_acc=0.1447)\n","[Server] Aggregated 5 updates -> agg=8 (avg_tau=5.000, test_loss=2.2736, test_acc=0.1510)\n","[Server] Aggregated 1 updates -> agg=9 (avg_tau=5.000, test_loss=2.2715, test_acc=0.1556)\n","[Server] Aggregated 4 updates -> agg=10 (avg_tau=6.000, test_loss=2.2686, test_acc=0.1577)\n","[Server] Aggregated 1 updates -> agg=11 (avg_tau=6.000, test_loss=2.2638, test_acc=0.1596)\n","[Server] Aggregated 3 updates -> agg=12 (avg_tau=7.000, test_loss=2.2590, test_acc=0.1595)\n","[Server] Aggregated 1 updates -> agg=13 (avg_tau=7.000, test_loss=2.2538, test_acc=0.1644)\n","[Server] Aggregated 2 updates -> agg=14 (avg_tau=8.000, test_loss=2.2464, test_acc=0.1660)\n","[Server] Aggregated 2 updates -> agg=15 (avg_tau=8.000, test_loss=2.2372, test_acc=0.1669)\n","[Server] Aggregated 2 updates -> agg=16 (avg_tau=8.500, test_loss=2.2246, test_acc=0.1691)\n","[Server] Aggregated 2 updates -> agg=17 (avg_tau=8.500, test_loss=2.2104, test_acc=0.1712)\n","[Server] Aggregated 2 updates -> agg=18 (avg_tau=8.500, test_loss=2.1933, test_acc=0.1772)\n","[Server] Aggregated 2 updates -> agg=19 (avg_tau=8.500, test_loss=2.1747, test_acc=0.1848)\n","[Server] Aggregated 2 updates -> agg=20 (avg_tau=8.500, test_loss=2.1637, test_acc=0.1889)\n","[Server] Aggregated 2 updates -> agg=21 (avg_tau=8.500, test_loss=2.1754, test_acc=0.1903)\n","[Server] Aggregated 2 updates -> agg=22 (avg_tau=8.500, test_loss=2.3548, test_acc=0.1735)\n","[Server] Aggregated 2 updates -> agg=23 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=24 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=25 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=26 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=27 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=28 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=29 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=30 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=31 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=32 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=33 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=34 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=35 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=36 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=37 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=38 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=39 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Aggregated 2 updates -> agg=40 (avg_tau=8.500, test_loss=nan, test_acc=0.1000)\n","[Server] Dropping client 25 update due to NaN/Inf values\n","[Server] Aggregated 1 updates -> agg=41 (avg_tau=9.000, test_loss=nan, test_acc=0.1000)\n","[Server] Dropping client 4 update due to NaN/Inf values\n","[Server] Dropping client 4 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 29 update due to NaN/Inf values\n","[Server] Dropping client 29 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 2 update due to NaN/Inf values\n","[Server] Dropping client 2 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 8 update due to NaN/Inf values\n","[Server] Dropping client 8 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 1 update due to NaN/Inf values\n","[Server] Dropping client 1 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 9 update due to NaN/Inf values\n","[Server] Dropping client 9 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 0 update due to NaN/Inf values\n","[Server] Dropping client 0 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 21 update due to NaN/Inf values\n","[Server] Dropping client 21 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 6 update due to NaN/Inf values\n","[Server] Dropping client 6 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 29 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 2 update due to NaN/Inf values\n","[Server] Dropping client 21 update due to NaN/Inf values\n","[Server] Dropping client 9 update due to NaN/Inf values\n","[Server] Dropping client 8 update due to NaN/Inf values\n","[Server] Dropping client 4 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 0 update due to NaN/Inf values\n","[Server] Dropping client 1 update due to NaN/Inf values\n","[Server] Dropping client 25 update due to NaN/Inf values\n","[Server] Dropping client 6 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 2 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 36 update due to NaN/Inf values\n","[Server] Dropping client 9 update due to NaN/Inf values\n","[Server] Dropping client 8 update due to NaN/Inf values\n","[Server] Dropping client 4 update due to NaN/Inf values\n","[Server] Dropping client 21 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 1 update due to NaN/Inf values\n","[Server] Dropping client 0 update due to NaN/Inf values\n","[Server] Dropping client 25 update due to NaN/Inf values\n","[Server] Dropping client 6 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 2 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 9 update due to NaN/Inf values\n","[Server] Dropping client 36 update due to NaN/Inf values\n","[Server] Dropping client 8 update due to NaN/Inf values\n","[Server] Dropping client 4 update due to NaN/Inf values\n","[Server] Dropping client 21 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n","[Server] Dropping client 1 update due to NaN/Inf values\n","[Server] Dropping client 0 update due to NaN/Inf values\n","[Server] Dropping client 25 update due to NaN/Inf values\n","[Server] Dropping client 6 update due to NaN/Inf values\n","[Server] Buffer flush skipped: no valid updates after filtering.\n"]}],"source":["# ===== experiment.py =====\n","\n","# Experiment runner for TrustWeight sweeps (partition alpha and straggler percent)\n","import logging\n","from copy import deepcopy\n","from dataclasses import replace\n","from pathlib import Path\n","import math\n","import csv\n","from typing import Iterable, List\n","import threading\n","\n","\n","def _safe_name(val) -> str:\n","    \"\"\"Convert numeric value to a filesystem-friendly token.\"\"\"\n","    s = str(val)\n","    return s.replace(\".\", \"p\")\n","\n","\n","def _override_io(cfg: GlobalConfig, exp_dir: Path) -> GlobalConfig:\n","    \"\"\"Return a copy of cfg with I/O paths pointing inside exp_dir.\"\"\"\n","    exp_dir = exp_dir.resolve()\n","    io = cfg.io\n","    new_io = replace(\n","        io,\n","        logs_dir=str(exp_dir),\n","        checkpoints_dir=str(exp_dir / \"checkpoints\"),\n","        results_dir=str(exp_dir / \"results\"),\n","        global_log_csv=str(exp_dir / \"TrustWeight.csv\"),\n","        client_participation_csv=str(exp_dir / \"TrustWeightClientParticipation.csv\"),\n","        final_model_path=str(exp_dir / \"TrustWeightModel.pt\"),\n","    )\n","    return replace(cfg, io=new_io)\n","\n","\n","def run_with_cfg(cfg: GlobalConfig) -> None:\n","    \"\"\"Run one TrustWeight training session with an already-mutated cfg.\"\"\"\n","    # --------------------- seed ---------------------\n","    logging.basicConfig(level=logging.INFO)\n","\n","    # --------------------- dataset & partition --------------------\n","    distributor = DataDistributor(\n","        dataset_name=cfg.data.dataset,\n","        data_dir=cfg.data.data_dir,\n","    )\n","    distributor.distribute_data(\n","        num_clients=cfg.clients.total,\n","        alpha=cfg.partition_alpha,\n","        seed=cfg.seed,\n","    )\n","    partitions = [distributor.partitions[cid] for cid in range(cfg.clients.total)]\n","\n","    # --------------------- create server -------------------------\n","    server = AsyncServer(cfg=cfg)\n","\n","    # --------------------- create clients ------------------------\n","    clients: List[AsyncClient] = []\n","    for cid in range(cfg.clients.total):\n","        indices = partitions[cid] if cid < len(partitions) else []\n","        clients.append(AsyncClient(cid=cid, indices=indices, cfg=cfg))\n","\n","    # ------------------- concurrency control ---------------------\n","    sem = threading.Semaphore(cfg.clients.concurrent)\n","\n","    def client_loop(cl: AsyncClient) -> None:\n","        while not server.should_stop():\n","            with sem:\n","                cont = cl.run_once(server)\n","            if not cont or server.should_stop():\n","                break\n","\n","    # --------------------- start client threads ------------------\n","    threads: List[threading.Thread] = []\n","    for cl in clients:\n","        t = threading.Thread(target=client_loop, args=(cl,), daemon=True)\n","        t.start()\n","        threads.append(t)\n","\n","    # --------------------- wait for completion -------------------\n","    server.wait()\n","    for t in threads:\n","        t.join(timeout=1.0)\n","\n","\n","def alpha_sweep(\n","    base_cfg: GlobalConfig,\n","    alphas: Iterable[float],\n","    out_root: Path,\n",") -> None:\n","    \"\"\"Run multiple trainings varying the Dirichlet alpha.\"\"\"\n","    for alpha in alphas:\n","        exp_dir = out_root / f\"alpha_{_safe_name(alpha)}\"\n","        cfg = deepcopy(base_cfg)\n","        cfg.partition_alpha = float(alpha)\n","        cfg = _override_io(cfg, exp_dir)\n","        print(f\"[exp] alpha={alpha} -> logs at {exp_dir}\")\n","        run_with_cfg(cfg)\n","\n","\n","def straggler_sweep(\n","    base_cfg: GlobalConfig,\n","    percents: Iterable[int],\n","    out_root: Path,\n",") -> None:\n","    \"\"\"Run multiple trainings varying the percent of slow clients.\"\"\"\n","    for pct in percents:\n","        exp_dir = out_root / f\"straggle_{pct}pct\"\n","        cfg = deepcopy(base_cfg)\n","        cfg.clients.struggle_percent = int(pct)\n","        cfg = _override_io(cfg, exp_dir)\n","        print(f\"[exp] struggle_percent={pct}% -> logs at {exp_dir}\")\n","        run_with_cfg(cfg)\n","\n","\n","def client_sweep(\n","    base_cfg: GlobalConfig,\n","    client_counts: Iterable[int],\n","    out_root: Path,\n",") -> None:\n","    \"\"\"Run multiple trainings varying the total number of clients.\"\"\"\n","    for n_clients in client_counts:\n","        exp_dir = out_root / f\"clients_{_safe_name(n_clients)}\"\n","        cfg = deepcopy(base_cfg)\n","\n","        # Set total number of clients\n","        cfg.clients.total = int(n_clients)\n","\n","        # Optionally, you could also cap concurrent clients here if you want:\n","        # cfg.clients.concurrent = min(cfg.clients.concurrent, cfg.clients.total)\n","\n","        cfg = _override_io(cfg, exp_dir)\n","        print(f\"[exp] clients={n_clients} -> logs at {exp_dir}\")\n","        run_with_cfg(cfg)\n","\n","\n","\n","def sanity_check(exp_root: Path) -> None:\n","    \"\"\"Lightweight sanity check over all experiment folders.\n","\n","    Confirms required CSVs exist and contain finite test_acc values.\n","    Prints a short summary of best/last test_acc for quick inspection.\n","    \"\"\"\n","    exp_root = exp_root.resolve()\n","    if not exp_root.exists():\n","        print(f\"[sanity] No experiment folder found at {exp_root}\")\n","        return\n","\n","    exp_dirs = [p for p in exp_root.iterdir() if p.is_dir()]\n","    if not exp_dirs:\n","        print(f\"[sanity] No sub-experiments found in {exp_root}\")\n","        return\n","\n","    for exp in sorted(exp_dirs):\n","        global_csv = exp / \"TrustWeight.csv\"\n","        client_csv = exp / \"TrustWeightClientParticipation.csv\"\n","\n","        missing = [p.name for p in [global_csv, client_csv] if not p.exists()]\n","        if missing:\n","            print(f\"[sanity] {exp.name}: missing {missing}\")\n","            continue\n","\n","        try:\n","            rows = []\n","            with global_csv.open() as f:\n","                reader = csv.DictReader(f)\n","                for row in reader:\n","                    rows.append(row)\n","        except Exception as e:\n","            print(f\"[sanity] {exp.name}: failed to read CSV ({e})\")\n","            continue\n","\n","        if not rows:\n","            print(f\"[sanity] {exp.name}: global CSV is empty\")\n","            continue\n","\n","        def _safe_float(x):\n","            try:\n","                v = float(x)\n","                return v if math.isfinite(v) else None\n","            except Exception:\n","                return None\n","\n","        test_accs = [_safe_float(r.get(\"test_acc\")) for r in rows]\n","        test_accs = [v for v in test_accs if v is not None]\n","\n","        if not test_accs:\n","            print(f\"[sanity] {exp.name}: no finite test_acc values\")\n","            continue\n","\n","        best = max(test_accs)\n","        last = test_accs[-1]\n","        print(f\"[sanity] {exp.name}: best_test_acc={best:.4f}, last_test_acc={last:.4f}, n_rows={len(rows)}\")\n","\n","\n","if __name__ == \"__main__\":\n","    base = load_config()\n","\n","    client_root = Path(base.io.logs_dir) / \"TrustWeightClientCountExp\"\n","    client_root.mkdir(parents=True, exist_ok=True)\n","\n","    # Experiment 3: client count sweep\n","    client_sweep(base, client_counts=[20, 40, 80], out_root=client_root)\n","\n","\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}