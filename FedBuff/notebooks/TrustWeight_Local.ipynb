{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrustWeight: Complete Self-Contained Notebook (Local Version)\n",
    "\n",
    "This notebook contains all code needed to run TrustWeight experiments locally:\n",
    "- Library installation\n",
    "- Data downloading and loading\n",
    "- All TrustWeight implementation\n",
    "- Logging and checkpointing\n",
    "\n",
    "**Run cells sequentially from top to bottom.**\n",
    "\n",
    "## Local Setup\n",
    "All results (logs, checkpoints, models) will be saved to local directories.\n",
    "Data will be downloaded to `./data` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "✅ Local mode: Results → .\n",
      "✅ Data directory: data\n",
      "✅ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (if not already installed)\n",
    "%pip install torch torchvision pytorch-lightning pyyaml numpy matplotlib pandas -q\n",
    "# Silence libraries\n",
    "import os\n",
    "import logging, warnings\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "os.environ[\"LIGHTNING_DISABLE_RICH\"] = \"1\"\n",
    "for name in [\n",
    "    \"pytorch_lightning\", \"lightning\", \"lightning.pytorch\",\n",
    "    \"lightning_fabric\", \"lightning_utilities\", \"torch\", \"torchvision\",\n",
    "]:\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "    logging.getLogger(name).propagate = False\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Core imports\n",
    "import time\n",
    "import csv\n",
    "import threading\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Setup paths based on environment\n",
    "# Local execution: Use current directory\n",
    "BASE_OUTPUT_DIR = Path(\".\")\n",
    "DATA_DIR = Path(\"./data\")\n",
    "print(f\"✅ Local mode: Results → {BASE_OUTPUT_DIR}\")\n",
    "print(f\"✅ Data directory: {DATA_DIR}\")\n",
    "print(\"✅ Libraries imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# ========== Helper Functions ==========\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed all RNGs for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Return the first available computation device (CUDA/MPS/CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def _device_to_accelerator(device: torch.device) -> str:\n",
    "    \"\"\"Convert torch device to PyTorch Lightning accelerator string.\"\"\"\n",
    "    if device.type == \"cuda\":\n",
    "        return \"gpu\"\n",
    "    if device.type == \"mps\":\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "print(\"✅ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model utilities defined\n"
     ]
    }
   ],
   "source": [
    "# ========== Model Utilities ==========\n",
    "\n",
    "def build_resnet18(num_classes: int = 10, pretrained: bool = False) -> nn.Module:\n",
    "    \"\"\"Create ResNet-18 adapted for CIFAR-10.\"\"\"\n",
    "    if pretrained:\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        m = models.resnet18(weights=None)\n",
    "    # CIFAR-10: 32x32 -> use 3x3 conv, stride 1, no maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    # Replace classifier\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    m.num_classes = num_classes\n",
    "    return m\n",
    "\n",
    "\n",
    "def state_to_list(state: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
    "    \"\"\"Flatten a state_dict to a list of tensors on CPU.\"\"\"\n",
    "    return [t.detach().cpu().clone() for _, t in state.items()]\n",
    "\n",
    "\n",
    "def list_to_state(template: Dict[str, torch.Tensor], arrs: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Rebuild a state_dict from a list of tensors using a template.\"\"\"\n",
    "    out: Dict[str, torch.Tensor] = {}\n",
    "    for (k, v), a in zip(template.items(), arrs):\n",
    "        out[k] = a.to(v.device).type_as(v)\n",
    "    return out\n",
    "\n",
    "print(\"✅ Model utilities defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataDistributor class defined\n"
     ]
    }
   ],
   "source": [
    "# ========== Data Loading and Partitioning ==========\n",
    "\n",
    "class DataDistributor:\n",
    "    \"\"\"Data distributor for federated learning with Dirichlet partitioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name: str, data_dir: str = \"./data\"):\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        self.data_dir = data_dir\n",
    "        self.train_dataset, self.test_dataset, self.num_classes = self._load_dataset()\n",
    "        self.partitions = None\n",
    "\n",
    "    def _load_dataset(self) -> Tuple[Any, Any, int]:\n",
    "        \"\"\"Load CIFAR-10 dataset with augmentation.\"\"\"\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            # Strong data pipeline: augmentation for train, normalization for test\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.4914, 0.4822, 0.4465),\n",
    "                    std=(0.2470, 0.2435, 0.2616),\n",
    "                ),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.4914, 0.4822, 0.4465),\n",
    "                    std=(0.2470, 0.2435, 0.2616),\n",
    "                ),\n",
    "            ])\n",
    "            train = datasets.CIFAR10(self.data_dir, train=True, download=True, transform=transform_train)\n",
    "            test = datasets.CIFAR10(self.data_dir, train=False, download=True, transform=transform_test)\n",
    "            num_classes = 10\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset '{self.dataset_name}' not supported. Use 'cifar10'.\")\n",
    "        return train, test, num_classes\n",
    "\n",
    "    def distribute_data(self, num_clients: int, alpha: float = 0.5, seed: int = 42):\n",
    "        \"\"\"Partition data using Dirichlet distribution.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        targets = np.array(self.train_dataset.targets)\n",
    "        self.partitions = {i: [] for i in range(num_clients)}\n",
    "\n",
    "        for cls in range(self.num_classes):\n",
    "            idxs = np.where(targets == cls)[0]\n",
    "            np.random.shuffle(idxs)\n",
    "            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n",
    "            proportions = np.array([p * len(idxs) for p in proportions]).astype(int)\n",
    "\n",
    "            start = 0\n",
    "            for client_id, size in enumerate(proportions):\n",
    "                self.partitions[client_id].extend(idxs[start:start + size])\n",
    "                start += size\n",
    "\n",
    "        for cid in self.partitions:\n",
    "            np.random.shuffle(self.partitions[cid])\n",
    "\n",
    "    def get_client_data(self, client_id: int) -> Subset:\n",
    "        \"\"\"Get data subset for a specific client.\"\"\"\n",
    "        if self.partitions is None:\n",
    "            raise ValueError(\"Data not distributed yet. Call distribute_data() first.\")\n",
    "        indices = self.partitions[client_id]\n",
    "        return Subset(self.train_dataset, indices)\n",
    "\n",
    "print(\"✅ DataDistributor class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TrustWeight Client Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TrustWeight client implementation defined\n"
     ]
    }
   ],
   "source": [
    "def _build_transform() -> transforms.Compose:\n",
    "    # CIFAR-10 standard augmentation to reduce overfitting\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "\n",
    "def _make_dataloader(data_dir: str, indices: Sequence[int], batch_size: int) -> DataLoader:\n",
    "    dataset = datasets.CIFAR10(root=data_dir, train=True, download=False, transform=_build_transform())\n",
    "    subset = Subset(dataset, indices)\n",
    "    if len(subset) == 0:\n",
    "        return DataLoader(subset, batch_size=1, shuffle=False, num_workers=0)\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Lightning-free async federated client for CIFAR-10\n",
    "import time\n",
    "import random\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import pytorch_lightning as pl  # imported but not required; kept for compatibility\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint  # unused, kept for compatibility\n",
    "\n",
    "# build_resnet18 already defined\n",
    "# get_device already defined\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _build_transform() -> transforms.Compose:\n",
    "    # CIFAR-10 standard augmentation to reduce overfitting\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def _make_dataloader(\n",
    "    data_dir: str,\n",
    "    indices: Sequence[int],\n",
    "    batch_size: int,\n",
    ") -> DataLoader:\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir,\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform=_build_transform(),\n",
    "    )\n",
    "    subset = Subset(dataset, indices)\n",
    "\n",
    "    # Safety: in case partitioning ever returns an empty list for a client\n",
    "    if len(subset) == 0:\n",
    "        return DataLoader(subset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    # num_workers=0 to avoid multiprocessing issues on macOS / Python 3.13\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "class AsyncClient:\n",
    "    \"\"\"Asynchronous client performing local training on its partition.\n",
    "\n",
    "    The client fetches the latest global model from the server, trains for a few\n",
    "    local epochs, and submits the updated parameters plus basic telemetry.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: int,\n",
    "        indices: Sequence[int],\n",
    "        cfg: dict,\n",
    "    ) -> None:\n",
    "        self.cid = cid\n",
    "        self.cfg = cfg\n",
    "        self.device = get_device()\n",
    "        self.loader = _make_dataloader(\n",
    "            cfg[\"data\"][\"data_dir\"],\n",
    "            indices,\n",
    "            cfg[\"clients\"][\"batch_size\"],\n",
    "        )\n",
    "        self.num_classes = cfg[\"data\"][\"num_classes\"]\n",
    "        self.local_epochs = cfg[\"clients\"][\"local_epochs\"]\n",
    "        self.lr = cfg[\"clients\"][\"lr\"]\n",
    "        self.weight_decay = cfg[\"clients\"][\"weight_decay\"]\n",
    "        self.grad_clip = cfg[\"clients\"][\"grad_clip\"]\n",
    "\n",
    "        # --- straggler behaviour ---\n",
    "        num_clients = cfg[\"clients\"][\"total\"]\n",
    "        slow_fraction = cfg[\"clients\"].get(\"struggle_percent\", 0) / 100.0\n",
    "        num_slow = int(round(num_clients * slow_fraction))\n",
    "        self.is_slow = cid < num_slow  # deterministic but good enough\n",
    "\n",
    "        self.delay_slow_range = tuple(cfg[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0]))\n",
    "        self.delay_fast_range = tuple(cfg[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2]))\n",
    "        self.jitter_per_round = float(cfg[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
    "        self.client_delay = float(cfg.get(\"server_runtime\", {}).get(\"client_delay\", 0.0))\n",
    "\n",
    "    # ------------------------------------------------------------------ utils\n",
    "\n",
    "    def _sample_delay(self) -> float:\n",
    "        if self.is_slow:\n",
    "            base = random.uniform(*self.delay_slow_range)\n",
    "        else:\n",
    "            base = random.uniform(*self.delay_fast_range)\n",
    "        jitter = random.uniform(-self.jitter_per_round, self.jitter_per_round)\n",
    "        return max(0.0, base + jitter + self.client_delay)\n",
    "\n",
    "    def _build_model(self) -> nn.Module:\n",
    "        model = build_resnet18(num_classes=self.num_classes)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    # ---------------------------------------------------------------- training\n",
    "\n",
    "    def _evaluate_on_loader(self, model: nn.Module) -> Tuple[float, float]:\n",
    "        \"\"\"Return (loss, accuracy) on the client's local data.\"\"\"\n",
    "        model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                total_correct += (preds == yb).sum().item()\n",
    "                total_examples += xb.size(0)\n",
    "        if total_examples == 0:\n",
    "            return 0.0, 0.0\n",
    "        return total_loss / total_examples, total_correct / total_examples\n",
    "\n",
    "    def _train_local(self, model: nn.Module) -> Tuple[float, float]:\n",
    "        \"\"\"Train for `local_epochs` and return (loss_after, acc_after).\"\"\"\n",
    "        model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optim = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=self.lr,\n",
    "            momentum=0.9,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        for _ in range(self.local_epochs):\n",
    "            for xb, yb in self.loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                if self.grad_clip > 0:\n",
    "                    clip_grad_norm_(model.parameters(), self.grad_clip)\n",
    "                optim.step()\n",
    "\n",
    "        # reuse evaluation code for final metrics\n",
    "        return self._evaluate_on_loader(model)\n",
    "\n",
    "    # ------------------------------------------------------------- main loop\n",
    "\n",
    "    def run_once(self, server) -> bool:\n",
    "        \"\"\"Perform a single async round with the server.\n",
    "\n",
    "        Returns False when the server indicates global stopping, True otherwise.\n",
    "        \"\"\"\n",
    "        # Simulated network / computation delay heterogeneity\n",
    "        delay = self._sample_delay()\n",
    "        if delay > 0:\n",
    "            time.sleep(delay)\n",
    "\n",
    "        if server.should_stop():\n",
    "            return False\n",
    "\n",
    "        # Get the latest global model snapshot\n",
    "        version, global_state = server.get_global_model()\n",
    "        model = self._build_model()\n",
    "        model.load_state_dict(global_state)\n",
    "\n",
    "        # Evaluate before local training to compute loss drop ΔL̃_i\n",
    "        loss_before, _ = self._evaluate_on_loader(model)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss_after, train_acc = self._train_local(model)\n",
    "        train_time_s = time.time() - start_time\n",
    "\n",
    "        # Local \"test\" is just another pass over the client's data\n",
    "        test_loss, test_acc = self._evaluate_on_loader(model)\n",
    "\n",
    "        # Move params to CPU tensors so they are cheap to share with server\n",
    "        # Build an OrderedDict for ``new_params`` using the model's own\n",
    "        # parameter ordering.  Although Python's plain dicts preserve\n",
    "        # insertion order, explicitly constructing an ``OrderedDict`` makes\n",
    "        # the intent clear and avoids any surprises if the language\n",
    "        # specification changes.  The server relies on matching key\n",
    "        # ordering to correctly flatten parameter tensors.\n",
    "        from collections import OrderedDict\n",
    "        new_params = OrderedDict()\n",
    "        for k, v in model.state_dict().items():\n",
    "            new_params[k] = v.detach().cpu().clone()\n",
    "        num_examples = len(self.loader.dataset)\n",
    "\n",
    "        delta_loss = loss_before - loss_after  # ΔL̃_i\n",
    "\n",
    "        # All local metrics are computed here and passed to the server\n",
    "        server.submit_update(\n",
    "            client_id=self.cid,\n",
    "            base_version=version,\n",
    "            new_params=new_params,\n",
    "            num_samples=num_examples,\n",
    "            train_time_s=train_time_s,\n",
    "            delta_loss=delta_loss,\n",
    "            loss_before=loss_before,\n",
    "            loss_after=loss_after,\n",
    "            train_acc=train_acc,\n",
    "            test_loss=test_loss,\n",
    "            test_acc=test_acc,\n",
    "        )\n",
    "        return not server.should_stop()\n",
    "\n",
    "\n",
    "print(\"✅ TrustWeight client implementation defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TrustWeight strategy implementation defined\n"
     ]
    }
   ],
   "source": [
    "# ========== TrustWeight Strategy Implementation ==========\n",
    "# Adapted from TrustWeight/strategy.py for notebook use\n",
    "\n",
    "# Trust-weighted asynchronous aggregation strategy implementing the PDF math\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrustWeightedConfig:\n",
    "    eta: float = 1.0              # server learning rate η\n",
    "    eps: float = 1e-8             # numerical stability ε\n",
    "    freshness_alpha: float = 0.1  # α in s(τ) = exp(-α τ)\n",
    "    beta1: float = 0.0            # Guard term coefficient on staleness\n",
    "    beta2: float = 0.0            # Guard term coefficient on ||u||\n",
    "    momentum_gamma: float = 0.9   # update factor for m_t\n",
    "    theta: Tuple[float, float, float] = (0.0, 0.0, 0.0)  # quality weights\n",
    "\n",
    "\n",
    "class TrustWeightedAsyncStrategy:\n",
    "    \"\"\"Implements the aggregation rule described in the DML solution PDF.\n",
    "\n",
    "    Core formula:\n",
    "\n",
    "        w_{t+1} = w_t + η * Σ_i Weight_i *\n",
    "            [ Proj_m_t(u_i) + Guard_i * (u_i - Proj_m_t(u_i)) ]\n",
    "\n",
    "    with:\n",
    "\n",
    "        Proj_m_t(u_i) = <u_i, m_t> / (||m_t||^2 + eps) * m_t\n",
    "        Guard_i = 1 / (1 + β1 * τ_i + β2 * ||u_i||)\n",
    "        Weight_i ∝ s(τ_i) * exp(θᵀ [ΔL̃_i, ||u_i||, cos(u_i, m_t)]) * (n_i / Σ_j n_j)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, cfg: TrustWeightedConfig | None = None) -> None:\n",
    "        self.dim = int(dim)\n",
    "        self.cfg = cfg or TrustWeightedConfig()\n",
    "        self.m = torch.zeros(self.dim, dtype=torch.float32)  # m_t, server momentum\n",
    "        self.step: int = 0\n",
    "\n",
    "        self.theta = torch.tensor(self.cfg.theta, dtype=torch.float32)\n",
    "\n",
    "    # ------------------------------------------------------------------ helpers\n",
    "\n",
    "    def _proj_m(self, u: torch.Tensor) -> torch.Tensor:\n",
    "        # Proj_m(u) = <u, m> / (||m||^2 + eps) * m\n",
    "        num = torch.dot(u, self.m)\n",
    "        denom = torch.dot(self.m, self.m) + self.cfg.eps\n",
    "        coef = num / denom\n",
    "        return coef * self.m\n",
    "\n",
    "    def _guard(self, tau: torch.Tensor, norm_u: torch.Tensor) -> torch.Tensor:\n",
    "        # Guard_i = 1 / (1 + β1 τ_i + β2 ||u_i||)\n",
    "        return 1.0 / (1.0 + self.cfg.beta1 * tau + self.cfg.beta2 * norm_u)\n",
    "\n",
    "    def _freshness(self, tau: torch.Tensor) -> torch.Tensor:\n",
    "        # s(τ) = exp(-α τ)\n",
    "        return torch.exp(-self.cfg.freshness_alpha * tau)\n",
    "\n",
    "    # ---------------------------------------------------------------- aggregate\n",
    "\n",
    "    def aggregate(\n",
    "        self,\n",
    "        w_t: torch.Tensor,\n",
    "        updates: List[Dict[str, torch.Tensor]],\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        \"\"\"Aggregate a buffer of updates.\n",
    "\n",
    "        Args:\n",
    "            w_t: Flattened current global model.\n",
    "            updates: List of dicts, each containing:\n",
    "                {\n",
    "                    \"u\": update vector (1D tensor),\n",
    "                    \"tau\": scalar tensor τ_i,\n",
    "                    \"num_samples\": scalar tensor n_i,\n",
    "                    \"delta_loss\": scalar tensor ΔL̃_i,\n",
    "                }\n",
    "\n",
    "        Returns:\n",
    "            new_w: updated global model vector.\n",
    "            metrics: small dict with aggregation statistics.\n",
    "        \"\"\"\n",
    "        if not updates:\n",
    "            return w_t, {\"avg_tau\": 0.0, \"buffer_size\": 0.0}\n",
    "\n",
    "        self.step += 1\n",
    "        device = w_t.device\n",
    "        self.m = self.m.to(device)\n",
    "\n",
    "        # Collect basic statistics\n",
    "        taus = torch.stack([u[\"tau\"].to(device) for u in updates])  # [B]\n",
    "        ns = torch.stack([u[\"num_samples\"].to(device) for u in updates])  # [B]\n",
    "        delta_losses = torch.stack([u[\"delta_loss\"].to(device) for u in updates])  # [B]\n",
    "        total_n = ns.sum().clamp_min(1.0)\n",
    "\n",
    "        # Precompute norms, projections, sideways components, cosines\n",
    "        proj_list: List[torch.Tensor] = []\n",
    "        side_list: List[torch.Tensor] = []\n",
    "        norm_u_list: List[torch.Tensor] = []\n",
    "        cos_list: List[torch.Tensor] = []\n",
    "\n",
    "        for u_rec in updates:\n",
    "            u = u_rec[\"u\"].to(device)\n",
    "            norm_u = torch.norm(u).clamp_min(self.cfg.eps)\n",
    "            norm_u_list.append(norm_u)\n",
    "\n",
    "            proj = self._proj_m(u)\n",
    "            side = u - proj\n",
    "            proj_list.append(proj)\n",
    "            side_list.append(side)\n",
    "\n",
    "            # cos(u, m) = <u, m> / (||u|| ||m|| + eps)\n",
    "            norm_m = torch.norm(self.m)\n",
    "            if norm_m.item() > 0.0:\n",
    "                cos_val = torch.dot(u, self.m) / (norm_u * norm_m + self.cfg.eps)\n",
    "            else:\n",
    "                cos_val = torch.tensor(0.0, device=device)\n",
    "            cos_list.append(cos_val)\n",
    "\n",
    "        norm_u_tensor = torch.stack(norm_u_list)  # [B]\n",
    "        cos_tensor = torch.stack(cos_list)  # [B]\n",
    "\n",
    "        # Guard factors per update\n",
    "        guards = self._guard(taus, norm_u_tensor)  # [B]\n",
    "\n",
    "        # Freshness\n",
    "        freshness = self._freshness(taus)  # [B]\n",
    "\n",
    "        # Quality term: exp(θᵀ [ΔL̃_i, ||u_i||, cos(u_i, m_t)])\n",
    "        feats = torch.stack(\n",
    "            [delta_losses, norm_u_tensor, cos_tensor],\n",
    "            dim=1,\n",
    "        )  # [B, 3]\n",
    "        quality_logits = feats @ self.theta.to(device)\n",
    "        quality = torch.exp(quality_logits)\n",
    "\n",
    "        # Data share term: n_i / Σ_j n_j\n",
    "        data_share = ns / total_n  # [B]\n",
    "\n",
    "        # Unnormalized weights, then normalization over buffer\n",
    "        raw_weights = freshness * quality * data_share  # [B]\n",
    "        sum_raw = raw_weights.sum()\n",
    "        if sum_raw.item() <= 0.0:\n",
    "            weights = torch.full_like(raw_weights, 1.0 / len(updates))\n",
    "        else:\n",
    "            weights = raw_weights / sum_raw\n",
    "\n",
    "        # Combine projection and guarded sideways components\n",
    "        agg_update = torch.zeros_like(w_t)\n",
    "        for i in range(len(updates)):\n",
    "            comp = proj_list[i] + guards[i] * side_list[i]\n",
    "            agg_update = agg_update + weights[i] * comp\n",
    "\n",
    "        # Final aggregation step:\n",
    "        # w_{t+1} = w_t + η * Σ_i Weight_i * [Proj_m(u_i) + Guard_i (u_i - Proj_m(u_i))]\n",
    "        new_w = w_t + self.cfg.eta * agg_update\n",
    "\n",
    "        # Update momentum m_t as a running average of aggregated updates\n",
    "        self.m = (1.0 - self.cfg.momentum_gamma) * self.m + self.cfg.momentum_gamma * agg_update\n",
    "\n",
    "        metrics = {\n",
    "            \"avg_tau\": float(taus.mean().item()),\n",
    "            \"avg_norm_u\": float(norm_u_tensor.mean().item()),\n",
    "            \"avg_delta_loss\": float(delta_losses.mean().item()),\n",
    "            \"buffer_size\": float(len(updates)),\n",
    "        }\n",
    "        return new_w, metrics\n",
    "\n",
    "\n",
    "print(\"✅ TrustWeight strategy implementation defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TrustWeight server implementation defined\n"
     ]
    }
   ],
   "source": [
    "# ========== TrustWeight Server Implementation ==========\n",
    "# Adapted from TrustWeight/server.py for notebook use\n",
    "\n",
    "from collections import OrderedDict as ODType\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def _testloader(root: str, batch_size: int = 256) -> DataLoader:\n",
    "    \"\"\"Create test dataloader.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=transform)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "def _flatten_state(state: ODType[str, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"Flatten a state_dict into a 1D tensor.\"\"\"\n",
    "    return torch.cat([p.reshape(-1) for p in state.values()])\n",
    "\n",
    "def _flatten_state_by_template(\n",
    "    state: Dict[str, torch.Tensor], template: ODType[str, torch.Tensor]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Flatten state according to template key ordering.\"\"\"\n",
    "    return torch.cat([state[k].reshape(-1) for k in template.keys()])\n",
    "\n",
    "def _vector_to_state(\n",
    "    vec: torch.Tensor, template: ODType[str, torch.Tensor]\n",
    ") -> ODType[str, torch.Tensor]:\n",
    "    \"\"\"Convert flattened vector back to state_dict.\"\"\"\n",
    "    new_state: ODType[str, torch.Tensor] = type(template)()\n",
    "    offset = 0\n",
    "    for k, t in template.items():\n",
    "        numel = t.numel()\n",
    "        new_state[k] = vec[offset : offset + numel].view_as(t).clone()\n",
    "        offset += numel\n",
    "    assert offset == vec.numel()\n",
    "    return new_state\n",
    "\n",
    "@dataclass\n",
    "class ClientUpdateState:\n",
    "    \"\"\"State for a client update.\"\"\"\n",
    "    client_id: int\n",
    "    base_version: int\n",
    "    new_params: ODType[str, torch.Tensor]\n",
    "    num_samples: int\n",
    "    train_time_s: float\n",
    "    delta_loss: float\n",
    "    loss_before: float\n",
    "    loss_after: float\n",
    "    train_acc: float\n",
    "    test_loss: float\n",
    "    test_acc: float\n",
    "    arrival_ts: float\n",
    "\n",
    "class AsyncServer:\n",
    "    \"\"\"Central server maintaining global model and asynchronous buffer.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        global_model: torch.nn.Module,\n",
    "        total_train_samples: int,\n",
    "        buffer_size: int = 5,\n",
    "        buffer_timeout_s: float = 5.0,\n",
    "        use_sample_weighing: bool = True,\n",
    "        target_accuracy: float = 0.8,\n",
    "        max_rounds: Optional[int] = None,\n",
    "        eval_interval_s: int = 15,\n",
    "        data_dir: str = \"./data\",\n",
    "        checkpoints_dir: str = \"./checkpoints/TrustWeight\",\n",
    "        logs_dir: str = \"./logs/TrustWeight\",\n",
    "        global_log_csv: Optional[str] = None,\n",
    "        client_participation_csv: Optional[str] = None,\n",
    "        final_model_path: Optional[str] = None,\n",
    "        resume: bool = True,\n",
    "        device: Optional[torch.device] = None,\n",
    "        eta: float = 1.0,\n",
    "    ):\n",
    "        self.device = device or get_device()\n",
    "        \n",
    "        # Extract num_classes from global_model\n",
    "        # Try to get it from model attribute, or infer from fc layer\n",
    "        if hasattr(global_model, 'num_classes'):\n",
    "            self.num_classes = global_model.num_classes\n",
    "        elif hasattr(global_model, 'fc'):\n",
    "            self.num_classes = global_model.fc.out_features\n",
    "        else:\n",
    "            # Default for CIFAR-10\n",
    "            self.num_classes = 10\n",
    "        \n",
    "        # data / evaluation\n",
    "        self.testloader = _testloader(data_dir, batch_size=256)\n",
    "        \n",
    "        # global model and version history\n",
    "        self._template_state: ODType[str, torch.Tensor] = ODType(\n",
    "            (k, v.detach().cpu().clone()) for k, v in global_model.state_dict().items()\n",
    "        )\n",
    "        self._global_state: ODType[str, torch.Tensor] = ODType(\n",
    "            (k, v.clone()) for k, v in self._template_state.items()\n",
    "        )\n",
    "        \n",
    "        self._model_versions: List[ODType[str, torch.Tensor]] = [\n",
    "            ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "        ]\n",
    "        self._version: int = 0\n",
    "        \n",
    "        # strategy encapsulating all math - use eta from config\n",
    "        dim = _flatten_state(self._global_state).numel()\n",
    "        strategy_cfg = TrustWeightedConfig(eta=eta)\n",
    "        self.strategy = TrustWeightedAsyncStrategy(dim=dim, cfg=strategy_cfg)\n",
    "        \n",
    "        # async buffer\n",
    "        self.buffer: List[ClientUpdateState] = []\n",
    "        self.buffer_size: int = buffer_size\n",
    "        self.buffer_timeout_s: float = buffer_timeout_s\n",
    "        self._last_flush_ts: float = time.time()\n",
    "        \n",
    "        # logging / control\n",
    "        self.global_log_path = Path(global_log_csv) if global_log_csv else Path(logs_dir) / \"TrustWeight.csv\"\n",
    "        self.global_log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self._init_global_log()\n",
    "        \n",
    "        self.client_log_path = Path(client_participation_csv) if client_participation_csv else Path(logs_dir) / \"TrustWeightClientParticipation.csv\"\n",
    "        self.client_log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self._init_client_log()\n",
    "        \n",
    "        self.eval_interval_s: float = eval_interval_s\n",
    "        self.target_accuracy: float = target_accuracy\n",
    "        self.max_rounds: int = max_rounds if max_rounds is not None else 1000\n",
    "        self.update_clip_norm: float = 10.0  # Default clipping norm\n",
    "        \n",
    "        self._num_aggregations: int = 0\n",
    "        self._stop: bool = False\n",
    "        self._stop_reason: str = \"\"\n",
    "        self._lock = threading.Lock()\n",
    "        self._agg_lock = threading.Lock()\n",
    "        self._start_ts: float = time.time()\n",
    "        \n",
    "        # Evaluation timer (optional, for periodic status)\n",
    "        self._eval_thread = None\n",
    "    \n",
    "    def _init_global_log(self) -> None:\n",
    "        \"\"\"Initialize the global training CSV.\"\"\"\n",
    "        if self.global_log_path.exists():\n",
    "            return\n",
    "        with self.global_log_path.open(\"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"total_agg\", \"avg_train_loss\", \"avg_train_acc\",\n",
    "                \"test_loss\", \"test_acc\", \"time\",\n",
    "            ])\n",
    "    \n",
    "    def _append_global_log(\n",
    "        self,\n",
    "        total_agg: int,\n",
    "        avg_train_loss: float,\n",
    "        avg_train_acc: float,\n",
    "        test_loss: float,\n",
    "        test_acc: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Append a single aggregation row to the global CSV.\"\"\"\n",
    "        ts = time.time() - self._start_ts\n",
    "        with self.global_log_path.open(\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                int(total_agg),\n",
    "                float(avg_train_loss),\n",
    "                float(avg_train_acc),\n",
    "                float(test_loss),\n",
    "                float(test_acc),\n",
    "                ts,\n",
    "            ])\n",
    "    \n",
    "    def _init_client_log(self) -> None:\n",
    "        \"\"\"Initialize the client participation CSV.\"\"\"\n",
    "        if self.client_log_path.exists():\n",
    "            return\n",
    "        with self.client_log_path.open(\"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"client_id\", \"local_train_loss\", \"local_train_acc\",\n",
    "                \"local_test_loss\", \"local_test_acc\", \"total_agg\", \"staleness\",\n",
    "            ])\n",
    "    \n",
    "    def _append_client_participation_log(\n",
    "        self,\n",
    "        total_agg: int,\n",
    "        updates: List[ClientUpdateState],\n",
    "        staleness_list: List[float],\n",
    "    ) -> None:\n",
    "        \"\"\"Append one row per client update.\"\"\"\n",
    "        with self.client_log_path.open(\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for u, tau_i in zip(updates, staleness_list):\n",
    "                writer.writerow([\n",
    "                    int(u.client_id),\n",
    "                    float(u.loss_after),\n",
    "                    float(u.train_acc),\n",
    "                    float(u.test_loss),\n",
    "                    float(u.test_acc),\n",
    "                    int(total_agg),\n",
    "                    float(tau_i),\n",
    "                ])\n",
    "    \n",
    "    def should_stop(self) -> bool:\n",
    "        with self._lock:\n",
    "            return self._stop\n",
    "    \n",
    "    def mark_stop(self, reason: str = \"\") -> None:\n",
    "        with self._lock:\n",
    "            if not self._stop:\n",
    "                self._stop = True\n",
    "                if reason:\n",
    "                    self._stop_reason = reason\n",
    "                print(f\"[Server] Stopping: {self._stop_reason}\")\n",
    "    \n",
    "    def get_global_model(self) -> Tuple[int, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Return (version, state_dict) of the current global model.\"\"\"\n",
    "        with self._lock:\n",
    "            version = self._version\n",
    "            state = ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "        return version, state\n",
    "    \n",
    "    def _make_model_from_state(self, state: Dict[str, torch.Tensor], num_classes: int) -> torch.nn.Module:\n",
    "        model = build_resnet18(num_classes=num_classes)\n",
    "        model.load_state_dict(state)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _evaluate_global(self, num_classes: int) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate the current global model on the test set.\"\"\"\n",
    "        model = self._make_model_from_state(self._global_state, num_classes)\n",
    "        model.eval()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.testloader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                total_correct += (preds == yb).sum().item()\n",
    "                total_examples += xb.size(0)\n",
    "        if total_examples == 0:\n",
    "            return 0.0, 0.0\n",
    "        return total_loss / total_examples, total_correct / total_examples\n",
    "    \n",
    "    def _flush_buffer_if_needed(self) -> None:\n",
    "        now = time.time()\n",
    "        should_flush = False\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            should_flush = True\n",
    "        elif (now - self._last_flush_ts) >= self.buffer_timeout_s and self.buffer:\n",
    "            should_flush = True\n",
    "        \n",
    "        if not should_flush:\n",
    "            return\n",
    "        \n",
    "        # copy buffer locally under lock then release for heavy work\n",
    "        with self._lock:\n",
    "            buffer_copy = list(self.buffer)\n",
    "            self.buffer.clear()\n",
    "            self._last_flush_ts = now\n",
    "        \n",
    "        # Serialize aggregations to avoid version races\n",
    "        with self._agg_lock:\n",
    "            self._aggregate(buffer_copy)\n",
    "    \n",
    "    def _aggregate(self, updates: List[ClientUpdateState]) -> None:\n",
    "        \"\"\"Aggregate a batch of client updates and log to CSVs.\"\"\"\n",
    "        if not updates:\n",
    "            return\n",
    "        \n",
    "        # Snapshot of current global parameters and version history\n",
    "        with self._lock:\n",
    "            global_vec = _flatten_state_by_template(self._global_state, self._template_state)\n",
    "            version_now = self._version\n",
    "            model_versions = list(self._model_versions)\n",
    "        \n",
    "        # Construct per-update vectors and metadata for the strategy\n",
    "        update_vectors: List[Dict[str, torch.Tensor]] = []\n",
    "        staleness_list: List[float] = []\n",
    "        valid_updates: List[ClientUpdateState] = []\n",
    "        \n",
    "        for u in updates:\n",
    "            base_state = model_versions[u.base_version]\n",
    "            base_vec = _flatten_state_by_template(base_state, self._template_state)\n",
    "            new_vec = _flatten_state_by_template(u.new_params, self._template_state)\n",
    "            ui = new_vec - base_vec\n",
    "            \n",
    "            # Skip bad updates\n",
    "            if not torch.isfinite(ui).all():\n",
    "                print(f\"[Server] Dropping client {u.client_id} update due to NaN/Inf values\")\n",
    "                continue\n",
    "            if self.update_clip_norm > 0:\n",
    "                norm = torch.norm(ui)\n",
    "                if torch.isfinite(norm) and norm.item() > self.update_clip_norm:\n",
    "                    ui = ui * (self.update_clip_norm / (norm + 1e-12))\n",
    "            \n",
    "            # τ_i = current-server-version - base_version\n",
    "            tau_i = float(max(0, version_now - u.base_version))\n",
    "            staleness_list.append(tau_i)\n",
    "            \n",
    "            delta_loss = float(u.delta_loss)\n",
    "            update_vectors.append({\n",
    "                \"u\": ui,\n",
    "                \"tau\": torch.tensor(tau_i, dtype=torch.float32),\n",
    "                \"num_samples\": torch.tensor(float(u.num_samples), dtype=torch.float32),\n",
    "                \"delta_loss\": torch.tensor(delta_loss, dtype=torch.float32),\n",
    "            })\n",
    "            valid_updates.append(u)\n",
    "        \n",
    "        if not update_vectors:\n",
    "            print(\"[Server] Buffer flush skipped: no valid updates after filtering.\")\n",
    "            return\n",
    "        \n",
    "        # Run the trust-weighted aggregation strategy\n",
    "        new_global_vec, agg_metrics = self.strategy.aggregate(global_vec, update_vectors)\n",
    "        \n",
    "        # Map back into parameter state_dict form\n",
    "        new_state = _vector_to_state(new_global_vec, self._template_state)\n",
    "        \n",
    "        # Compute average local train metrics\n",
    "        avg_train_loss = sum(u.loss_after for u in valid_updates) / len(valid_updates)\n",
    "        avg_train_acc = sum(u.train_acc for u in valid_updates) / len(valid_updates)\n",
    "        \n",
    "        # Commit the new global model\n",
    "        with self._lock:\n",
    "            self._global_state = ODType((k, v.clone()) for k, v in new_state.items())\n",
    "            self._model_versions.append(\n",
    "                ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "            )\n",
    "            self._version = len(self._model_versions) - 1\n",
    "            self._num_aggregations += 1\n",
    "            total_agg = self._num_aggregations\n",
    "        \n",
    "        # Evaluate updated global model\n",
    "        test_loss, test_acc = self._evaluate_global(self.num_classes)\n",
    "        \n",
    "        # Log metrics\n",
    "        self._append_global_log(\n",
    "            total_agg=total_agg,\n",
    "            avg_train_loss=avg_train_loss,\n",
    "            avg_train_acc=avg_train_acc,\n",
    "            test_loss=test_loss,\n",
    "            test_acc=test_acc,\n",
    "        )\n",
    "        self._append_client_participation_log(\n",
    "            total_agg=total_agg,\n",
    "            updates=valid_updates,\n",
    "            staleness_list=staleness_list,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"[Server] Aggregated {len(valid_updates)} updates -> agg={total_agg} \"\n",
    "            f\"(avg_tau={agg_metrics.get('avg_tau', 0.0):.3f}, \"\n",
    "            f\"test_loss={test_loss:.4f}, test_acc={test_acc:.4f})\"\n",
    "        )\n",
    "        \n",
    "        # Stopping conditions\n",
    "        if test_acc >= self.target_accuracy:\n",
    "            self.mark_stop(f\"target accuracy {test_acc:.4f} reached\")\n",
    "        if total_agg >= self.max_rounds:\n",
    "            self.mark_stop(\"max aggregation rounds reached\")\n",
    "    \n",
    "    def submit_update(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        base_version: int,\n",
    "        new_params: Dict[str, torch.Tensor],\n",
    "        num_samples: int,\n",
    "        train_time_s: float,\n",
    "        delta_loss: float,\n",
    "        loss_before: float,\n",
    "        loss_after: float,\n",
    "        train_acc: float,\n",
    "        test_loss: float,\n",
    "        test_acc: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Entry point called by clients after local training.\"\"\"\n",
    "        cu = ClientUpdateState(\n",
    "            client_id=client_id,\n",
    "            base_version=base_version,\n",
    "            new_params=new_params,\n",
    "            num_samples=num_samples,\n",
    "            train_time_s=float(train_time_s),\n",
    "            delta_loss=float(delta_loss),\n",
    "            loss_before=float(loss_before),\n",
    "            loss_after=float(loss_after),\n",
    "            train_acc=float(train_acc),\n",
    "            test_loss=float(test_loss),\n",
    "            test_acc=float(test_acc),\n",
    "            arrival_ts=time.time(),\n",
    "        )\n",
    "        with self._lock:\n",
    "            self.buffer.append(cu)\n",
    "        self._flush_buffer_if_needed()\n",
    "    \n",
    "    def start_eval_timer(self):\n",
    "        \"\"\"Start periodic evaluation timer.\"\"\"\n",
    "        def _loop():\n",
    "            next_ts = time.time() + self.eval_interval_s\n",
    "            while True:\n",
    "                now = time.time()\n",
    "                sleep_for = max(0.0, next_ts - now)\n",
    "                time.sleep(sleep_for)\n",
    "                with self._lock:\n",
    "                    if self._stop:\n",
    "                        break\n",
    "                next_ts += self.eval_interval_s\n",
    "        self._eval_thread = threading.Thread(target=_loop, daemon=True)\n",
    "        self._eval_thread.start()\n",
    "    \n",
    "    def wait(self) -> None:\n",
    "        \"\"\"Block until training is finished.\"\"\"\n",
    "        try:\n",
    "            while not self.should_stop():\n",
    "                time.sleep(0.2)\n",
    "        finally:\n",
    "            self.mark_stop(self._stop_reason or \"training finished\")\n",
    "\n",
    "print(\"✅ TrustWeight server implementation defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 6 experiment configurations loaded:\n",
      "  Exp1: IID (alpha=1000), no stragglers\n",
      "    - Alpha: 1000.0\n",
      "    - Stragglers: 0%\n",
      "    - Max rounds: 100\n",
      "  Exp2: alpha=0.1, 10% stragglers\n",
      "    - Alpha: 0.1\n",
      "    - Stragglers: 10%\n",
      "    - Max rounds: 100\n",
      "  Exp3: alpha=0.1, 20% stragglers\n",
      "    - Alpha: 0.1\n",
      "    - Stragglers: 20%\n",
      "    - Max rounds: 100\n",
      "  Exp4: alpha=0.1, 30% stragglers\n",
      "    - Alpha: 0.1\n",
      "    - Stragglers: 30%\n",
      "    - Max rounds: 100\n",
      "  Exp5: alpha=0.1, 40% stragglers\n",
      "    - Alpha: 0.1\n",
      "    - Stragglers: 40%\n",
      "    - Max rounds: 100\n",
      "  Exp6: alpha=0.1, 50% stragglers\n",
      "    - Alpha: 0.1\n",
      "    - Stragglers: 50%\n",
      "    - Max rounds: 100\n"
     ]
    }
   ],
   "source": [
    "# ========== All 6 Experiment Configurations ==========\n",
    "\n",
    "# Helper function to get paths (works for both Colab and local)\n",
    "def get_paths(exp_id: str):\n",
    "    \"\"\"Get paths for experiment, using Google Drive if in Colab.\"\"\"\n",
    "    base = BASE_OUTPUT_DIR\n",
    "    \n",
    "    return {\n",
    "        \"data_dir\": str(DATA_DIR),  # Always local for faster access\n",
    "        \"checkpoints_dir\": str(base / \"checkpoints\" / \"TrustWeight\" / exp_id),\n",
    "        \"logs_dir\": str(base / \"logs\" / \"TrustWeight\" / exp_id),\n",
    "        \"results_dir\": str(base / \"results\" / \"TrustWeight\" / exp_id),\n",
    "    }\n",
    "\n",
    "experiments = {\n",
    "    \"Exp1\": {\n",
    "        \"name\": \"IID (alpha=1000), no stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),  # Use DATA_DIR variable\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 0,\n",
    "            \"delay_slow_range\": [0.0, 0.0],\n",
    "            \"delay_fast_range\": [0.0, 0.0],\n",
    "            \"jitter_per_round\": 0.0,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 1000.0,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp1\")\n",
    "    },\n",
    "    \"Exp2\": {\n",
    "        \"name\": \"alpha=0.1, 10% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 10,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp2\")\n",
    "    },\n",
    "    \"Exp3\": {\n",
    "        \"name\": \"alpha=0.1, 20% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 20,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp3\")\n",
    "    },\n",
    "    \"Exp4\": {\n",
    "        \"name\": \"alpha=0.1, 30% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 30,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp4\")\n",
    "    },\n",
    "    \"Exp5\": {\n",
    "        \"name\": \"alpha=0.1, 40% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 40,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp5\")\n",
    "    },\n",
    "    \"Exp6\": {\n",
    "        \"name\": \"alpha=0.1, 50% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 50,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp6\")\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ All 6 experiment configurations loaded:\")\n",
    "for exp_id, exp_config in experiments.items():\n",
    "    print(f\"  {exp_id}: {exp_config['name']}\")\n",
    "    print(f\"    - Alpha: {exp_config['partition_alpha']}\")\n",
    "    print(f\"    - Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
    "    print(f\"    - Max rounds: {exp_config['train']['max_rounds']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Single Experiment (Helper Function)\n",
    "\n",
    "Use this function to run a single experiment by ID (Exp1-Exp6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper function `run_single_experiment()` defined\n",
      "   Usage: run_dir = run_single_experiment('Exp1', experiments)\n"
     ]
    }
   ],
   "source": [
    "def run_single_experiment(exp_id: str, experiments_dict: dict):\n",
    "    \"\"\"\n",
    "    Run a single TrustWeight experiment.\n",
    "    \n",
    "    Args:\n",
    "        exp_id: Experiment ID (e.g., \"Exp1\", \"Exp2\", ..., \"Exp6\")\n",
    "        experiments_dict: Dictionary containing all experiment configs\n",
    "    \"\"\"\n",
    "    if exp_id not in experiments_dict:\n",
    "        print(f\"❌ Error: Experiment {exp_id} not found!\")\n",
    "        return None\n",
    "    \n",
    "    config = experiments_dict[exp_id]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running {exp_id}: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Set random seed\n",
    "    seed = int(config.get(\"seed\", 42))\n",
    "    set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create timestamped run folder\n",
    "    run_dir = Path(config[\"io\"][\"logs_dir\"]) / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Write COMMIT.txt\n",
    "    commit_hash = \"notebook_run\"\n",
    "    csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n",
    "    with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n",
    "        f.write(f\"{commit_hash},{csv_header}\\n\")\n",
    "    \n",
    "    # Save config to run folder\n",
    "    with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    print(f\"✅ Run folder: {run_dir}\")\n",
    "    \n",
    "    # Load and partition data\n",
    "    dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n",
    "    dd.distribute_data(\n",
    "        num_clients=int(config[\"clients\"][\"total\"]),\n",
    "        alpha=float(config[\"partition_alpha\"]),\n",
    "        seed=seed\n",
    "    )\n",
    "    print(f\"✅ Data partitioned: {config['clients']['total']} clients, alpha={config['partition_alpha']}\")\n",
    "    \n",
    "    # Build global model\n",
    "    global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n",
    "    \n",
    "    # Initialize server\n",
    "    server = AsyncServer(\n",
    "        global_model=global_model,\n",
    "        total_train_samples=len(dd.train_dataset),\n",
    "        buffer_size=int(config[\"trustweight\"][\"buffer_size\"]),\n",
    "        buffer_timeout_s=float(config[\"trustweight\"][\"buffer_timeout_s\"]),\n",
    "        use_sample_weighing=bool(config[\"trustweight\"][\"use_sample_weighing\"]),\n",
    "        target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n",
    "        max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n",
    "        eval_interval_s=int(config[\"eval\"][\"interval_seconds\"]),\n",
    "        data_dir=config[\"data\"][\"data_dir\"],\n",
    "        checkpoints_dir=str(run_dir / \"checkpoints\"),\n",
    "        logs_dir=str(run_dir),\n",
    "        global_log_csv=str(run_dir / \"TrustWeight.csv\"),\n",
    "        client_participation_csv=str(run_dir / \"TrustWeightClientParticipation.csv\"),\n",
    "        final_model_path=str(run_dir / \"TrustWeightModel.pt\"),\n",
    "        resume=False,\n",
    "        device=get_device(),\n",
    "        eta=float(config[\"trustweight\"].get(\"eta\", 0.5)),\n",
    "    )\n",
    "    print(f\"✅ Server initialized (device: {server.device})\")\n",
    "    \n",
    "    # Setup clients\n",
    "    n = int(config[\"clients\"][\"total\"])\n",
    "    pct = max(0, min(100, int(config[\"clients\"].get(\"struggle_percent\", 0))))\n",
    "    k_slow = (n * pct) // 100\n",
    "    slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
    "    \n",
    "    a_s, b_s = config[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
    "    a_f, b_f = config[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
    "    fix_delays = bool(config[\"clients\"].get(\"fix_delays_per_client\", True))\n",
    "    jitter = float(config[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
    "    \n",
    "    per_client_base_delay: Dict[int, float] = {}\n",
    "    if fix_delays:\n",
    "        for cid in range(n):\n",
    "            if cid in slow_ids:\n",
    "                per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
    "            else:\n",
    "                per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
    "    \n",
    "    clients = []\n",
    "    for cid in range(n):\n",
    "        indices = dd.partitions[cid] if cid in dd.partitions else []\n",
    "        base_delay = per_client_base_delay.get(cid, 0.0)\n",
    "        is_slow = cid in slow_ids\n",
    "        clients.append(AsyncClient(\n",
    "            cid=cid,\n",
    "            indices=indices,\n",
    "            cfg=config,\n",
    "        ))\n",
    "    \n",
    "    print(f\"✅ Created {len(clients)} clients ({len(slow_ids)} slow, {n - len(slow_ids)} fast)\")\n",
    "    \n",
    "    # Start experiment\n",
    "    server.start_eval_timer()\n",
    "    sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n",
    "    \n",
    "    def client_loop(client: AsyncClient):\n",
    "        while True:\n",
    "            with sem:\n",
    "                cont = client.run_once(server)\n",
    "            if not cont:\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "    \n",
    "    threads = []\n",
    "    for cl in clients:\n",
    "        t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    \n",
    "    print(f\"✅ Started {len(threads)} client threads\")\n",
    "    print(f\"🚀 Experiment running... (max {config['train']['max_rounds']} rounds)\")\n",
    "    \n",
    "    # Wait for completion\n",
    "    server.wait()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    print(f\"\\n✅ {exp_id} completed!\")\n",
    "    print(f\"📁 Results: {run_dir}\")\n",
    "    \n",
    "    return run_dir\n",
    "\n",
    "print(\"✅ Helper function `run_single_experiment()` defined\")\n",
    "print(\"   Usage: run_dir = run_single_experiment('Exp1', experiments)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run All 6 Experiments Sequentially\n",
    "\n",
    "This cell runs all experiments one by one. Each experiment saves to its own folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING ALL 6 EXPERIMENTS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Running Exp1: IID (alpha=1000), no stragglers\n",
      "======================================================================\n",
      "✅ Run folder: logs/TrustWeight/Exp1/run_20251130_013402\n",
      "✅ Data partitioned: 20 clients, alpha=1000.0\n",
      "✅ Server initialized (device: mps)\n",
      "✅ Created 20 clients (0 slow, 20 fast)\n",
      "✅ Started 20 client threads\n",
      "🚀 Experiment running... (max 100 rounds)\n",
      "[Server] Aggregated 1 updates -> agg=1 (avg_tau=0.000, test_loss=2.4009, test_acc=0.1151)\n",
      "[Server] Aggregated 1 updates -> agg=2 (avg_tau=1.000, test_loss=2.3621, test_acc=0.1104)\n",
      "[Server] Aggregated 1 updates -> agg=3 (avg_tau=2.000, test_loss=2.3330, test_acc=0.1182)\n",
      "[Server] Aggregated 1 updates -> agg=4 (avg_tau=3.000, test_loss=2.3115, test_acc=0.1281)\n",
      "[Server] Aggregated 1 updates -> agg=5 (avg_tau=4.000, test_loss=2.2963, test_acc=0.1433)\n",
      "[Server] Aggregated 1 updates -> agg=6 (avg_tau=4.000, test_loss=2.2857, test_acc=0.1519)\n",
      "[Server] Aggregated 1 updates -> agg=7 (avg_tau=4.000, test_loss=2.2772, test_acc=0.1600)\n",
      "[Server] Aggregated 1 updates -> agg=8 (avg_tau=4.000, test_loss=2.2708, test_acc=0.1641)\n",
      "[Server] Aggregated 1 updates -> agg=9 (avg_tau=4.000, test_loss=2.2650, test_acc=0.1650)\n",
      "[Server] Aggregated 1 updates -> agg=10 (avg_tau=4.000, test_loss=2.2606, test_acc=0.1590)\n",
      "[Server] Aggregated 1 updates -> agg=11 (avg_tau=4.000, test_loss=2.2563, test_acc=0.1548)\n",
      "[Server] Aggregated 1 updates -> agg=12 (avg_tau=4.000, test_loss=2.2520, test_acc=0.1522)\n",
      "[Server] Aggregated 1 updates -> agg=13 (avg_tau=4.000, test_loss=2.2467, test_acc=0.1511)\n",
      "[Server] Aggregated 1 updates -> agg=14 (avg_tau=4.000, test_loss=2.2405, test_acc=0.1519)\n",
      "[Server] Aggregated 1 updates -> agg=15 (avg_tau=4.000, test_loss=2.2334, test_acc=0.1533)\n",
      "[Server] Aggregated 1 updates -> agg=16 (avg_tau=4.000, test_loss=2.2248, test_acc=0.1549)\n",
      "[Server] Aggregated 1 updates -> agg=17 (avg_tau=4.000, test_loss=2.2157, test_acc=0.1558)\n",
      "[Server] Aggregated 1 updates -> agg=18 (avg_tau=4.000, test_loss=2.2060, test_acc=0.1582)\n",
      "[Server] Aggregated 1 updates -> agg=19 (avg_tau=4.000, test_loss=2.1938, test_acc=0.1602)\n",
      "[Server] Aggregated 1 updates -> agg=20 (avg_tau=4.000, test_loss=2.1839, test_acc=0.1638)\n",
      "[Server] Aggregated 1 updates -> agg=21 (avg_tau=4.000, test_loss=2.1712, test_acc=0.1675)\n",
      "[Server] Aggregated 1 updates -> agg=22 (avg_tau=4.000, test_loss=2.1600, test_acc=0.1703)\n",
      "[Server] Aggregated 1 updates -> agg=23 (avg_tau=4.000, test_loss=2.1452, test_acc=0.1759)\n",
      "[Server] Aggregated 1 updates -> agg=24 (avg_tau=4.000, test_loss=2.1333, test_acc=0.1787)\n",
      "[Server] Aggregated 1 updates -> agg=25 (avg_tau=4.000, test_loss=2.1198, test_acc=0.1809)\n",
      "[Server] Aggregated 1 updates -> agg=26 (avg_tau=4.000, test_loss=2.1099, test_acc=0.1815)\n",
      "[Server] Aggregated 1 updates -> agg=27 (avg_tau=4.000, test_loss=2.0962, test_acc=0.1852)\n",
      "[Server] Aggregated 1 updates -> agg=28 (avg_tau=4.000, test_loss=2.0844, test_acc=0.1899)\n",
      "[Server] Aggregated 1 updates -> agg=29 (avg_tau=4.000, test_loss=2.0750, test_acc=0.1940)\n",
      "[Server] Aggregated 1 updates -> agg=30 (avg_tau=4.000, test_loss=2.0630, test_acc=0.2005)\n",
      "[Server] Aggregated 1 updates -> agg=31 (avg_tau=4.000, test_loss=2.0525, test_acc=0.2053)\n",
      "[Server] Aggregated 1 updates -> agg=32 (avg_tau=4.000, test_loss=2.0398, test_acc=0.2126)\n",
      "[Server] Aggregated 1 updates -> agg=33 (avg_tau=4.000, test_loss=2.0312, test_acc=0.2201)\n",
      "[Server] Aggregated 1 updates -> agg=34 (avg_tau=4.000, test_loss=2.0233, test_acc=0.2215)\n",
      "[Server] Aggregated 1 updates -> agg=35 (avg_tau=4.000, test_loss=2.0153, test_acc=0.2259)\n",
      "[Server] Aggregated 1 updates -> agg=36 (avg_tau=4.000, test_loss=2.0040, test_acc=0.2308)\n",
      "[Server] Aggregated 1 updates -> agg=37 (avg_tau=4.000, test_loss=1.9986, test_acc=0.2337)\n",
      "[Server] Aggregated 1 updates -> agg=38 (avg_tau=4.000, test_loss=1.9941, test_acc=0.2339)\n",
      "[Server] Aggregated 1 updates -> agg=39 (avg_tau=4.000, test_loss=1.9906, test_acc=0.2347)\n",
      "[Server] Aggregated 1 updates -> agg=40 (avg_tau=4.000, test_loss=1.9841, test_acc=0.2358)\n",
      "[Server] Aggregated 1 updates -> agg=41 (avg_tau=4.000, test_loss=1.9761, test_acc=0.2414)\n",
      "[Server] Aggregated 1 updates -> agg=42 (avg_tau=4.000, test_loss=1.9695, test_acc=0.2441)\n",
      "[Server] Aggregated 1 updates -> agg=43 (avg_tau=4.000, test_loss=1.9614, test_acc=0.2502)\n",
      "[Server] Aggregated 1 updates -> agg=44 (avg_tau=4.000, test_loss=1.9544, test_acc=0.2541)\n",
      "[Server] Aggregated 1 updates -> agg=45 (avg_tau=4.000, test_loss=1.9466, test_acc=0.2584)\n",
      "[Server] Aggregated 1 updates -> agg=46 (avg_tau=4.000, test_loss=1.9392, test_acc=0.2606)\n",
      "[Server] Aggregated 1 updates -> agg=47 (avg_tau=4.000, test_loss=1.9288, test_acc=0.2650)\n",
      "[Server] Aggregated 1 updates -> agg=48 (avg_tau=4.000, test_loss=1.9213, test_acc=0.2688)\n",
      "[Server] Aggregated 1 updates -> agg=49 (avg_tau=4.000, test_loss=1.9126, test_acc=0.2722)\n",
      "[Server] Aggregated 1 updates -> agg=50 (avg_tau=4.000, test_loss=1.9045, test_acc=0.2763)\n",
      "[Server] Aggregated 1 updates -> agg=51 (avg_tau=4.000, test_loss=1.8969, test_acc=0.2822)\n",
      "[Server] Aggregated 1 updates -> agg=52 (avg_tau=4.000, test_loss=1.8894, test_acc=0.2854)\n",
      "[Server] Aggregated 1 updates -> agg=53 (avg_tau=4.000, test_loss=1.8827, test_acc=0.2879)\n",
      "[Server] Aggregated 1 updates -> agg=54 (avg_tau=4.000, test_loss=1.8747, test_acc=0.2930)\n",
      "[Server] Aggregated 1 updates -> agg=55 (avg_tau=4.000, test_loss=1.8657, test_acc=0.2968)\n",
      "[Server] Aggregated 1 updates -> agg=56 (avg_tau=4.000, test_loss=1.8591, test_acc=0.3016)\n",
      "[Server] Aggregated 1 updates -> agg=57 (avg_tau=4.000, test_loss=1.8548, test_acc=0.3034)\n",
      "[Server] Aggregated 1 updates -> agg=58 (avg_tau=4.000, test_loss=1.8509, test_acc=0.3056)\n",
      "[Server] Aggregated 1 updates -> agg=59 (avg_tau=4.000, test_loss=1.8476, test_acc=0.3049)\n",
      "[Server] Aggregated 1 updates -> agg=60 (avg_tau=4.000, test_loss=1.8461, test_acc=0.3055)\n",
      "[Server] Aggregated 1 updates -> agg=61 (avg_tau=4.000, test_loss=1.8378, test_acc=0.3092)\n",
      "[Server] Aggregated 1 updates -> agg=62 (avg_tau=4.000, test_loss=1.8322, test_acc=0.3114)\n",
      "[Server] Aggregated 1 updates -> agg=63 (avg_tau=4.000, test_loss=1.8257, test_acc=0.3121)\n",
      "[Server] Aggregated 1 updates -> agg=64 (avg_tau=4.000, test_loss=1.8182, test_acc=0.3154)\n",
      "[Server] Aggregated 1 updates -> agg=65 (avg_tau=4.000, test_loss=1.8115, test_acc=0.3195)\n",
      "[Server] Aggregated 1 updates -> agg=66 (avg_tau=4.000, test_loss=1.8047, test_acc=0.3213)\n",
      "[Server] Aggregated 1 updates -> agg=67 (avg_tau=4.000, test_loss=1.8001, test_acc=0.3232)\n",
      "[Server] Aggregated 1 updates -> agg=68 (avg_tau=4.000, test_loss=1.7914, test_acc=0.3273)\n",
      "[Server] Aggregated 1 updates -> agg=69 (avg_tau=4.000, test_loss=1.7862, test_acc=0.3288)\n",
      "[Server] Aggregated 1 updates -> agg=70 (avg_tau=4.000, test_loss=1.7809, test_acc=0.3333)\n",
      "[Server] Aggregated 1 updates -> agg=71 (avg_tau=4.000, test_loss=1.7770, test_acc=0.3349)\n",
      "[Server] Aggregated 1 updates -> agg=72 (avg_tau=4.000, test_loss=1.7732, test_acc=0.3356)\n",
      "[Server] Aggregated 1 updates -> agg=73 (avg_tau=4.000, test_loss=1.7673, test_acc=0.3402)\n",
      "[Server] Aggregated 1 updates -> agg=74 (avg_tau=4.000, test_loss=1.7633, test_acc=0.3423)\n",
      "[Server] Aggregated 1 updates -> agg=75 (avg_tau=4.000, test_loss=1.7598, test_acc=0.3431)\n",
      "[Server] Aggregated 1 updates -> agg=76 (avg_tau=4.000, test_loss=1.7563, test_acc=0.3458)\n",
      "[Server] Aggregated 1 updates -> agg=77 (avg_tau=4.000, test_loss=1.7524, test_acc=0.3453)\n",
      "[Server] Aggregated 1 updates -> agg=78 (avg_tau=4.000, test_loss=1.7517, test_acc=0.3465)\n",
      "[Server] Aggregated 1 updates -> agg=79 (avg_tau=4.000, test_loss=1.7473, test_acc=0.3466)\n",
      "[Server] Aggregated 1 updates -> agg=80 (avg_tau=4.000, test_loss=1.7456, test_acc=0.3455)\n",
      "[Server] Aggregated 1 updates -> agg=81 (avg_tau=4.000, test_loss=1.7430, test_acc=0.3450)\n",
      "[Server] Aggregated 1 updates -> agg=82 (avg_tau=4.000, test_loss=1.7414, test_acc=0.3448)\n",
      "[Server] Aggregated 1 updates -> agg=83 (avg_tau=4.000, test_loss=1.7355, test_acc=0.3498)\n",
      "[Server] Aggregated 1 updates -> agg=84 (avg_tau=4.000, test_loss=1.7330, test_acc=0.3478)\n",
      "[Server] Aggregated 1 updates -> agg=85 (avg_tau=4.000, test_loss=1.7322, test_acc=0.3481)\n",
      "[Server] Aggregated 1 updates -> agg=86 (avg_tau=4.000, test_loss=1.7331, test_acc=0.3477)\n",
      "[Server] Aggregated 1 updates -> agg=87 (avg_tau=4.000, test_loss=1.7297, test_acc=0.3490)\n",
      "[Server] Aggregated 1 updates -> agg=88 (avg_tau=4.000, test_loss=1.7258, test_acc=0.3499)\n",
      "[Server] Aggregated 1 updates -> agg=89 (avg_tau=4.000, test_loss=1.7229, test_acc=0.3520)\n",
      "[Server] Aggregated 1 updates -> agg=90 (avg_tau=4.000, test_loss=1.7268, test_acc=0.3504)\n",
      "[Server] Aggregated 1 updates -> agg=91 (avg_tau=4.000, test_loss=1.7287, test_acc=0.3494)\n",
      "[Server] Aggregated 1 updates -> agg=92 (avg_tau=4.000, test_loss=1.7277, test_acc=0.3494)\n",
      "[Server] Aggregated 1 updates -> agg=93 (avg_tau=4.000, test_loss=1.7229, test_acc=0.3513)\n",
      "[Server] Aggregated 1 updates -> agg=94 (avg_tau=4.000, test_loss=1.7172, test_acc=0.3531)\n",
      "[Server] Aggregated 1 updates -> agg=95 (avg_tau=4.000, test_loss=1.7140, test_acc=0.3549)\n",
      "[Server] Aggregated 1 updates -> agg=96 (avg_tau=4.000, test_loss=1.7098, test_acc=0.3573)\n",
      "[Server] Aggregated 1 updates -> agg=97 (avg_tau=4.000, test_loss=1.7080, test_acc=0.3584)\n",
      "[Server] Aggregated 1 updates -> agg=98 (avg_tau=4.000, test_loss=1.7070, test_acc=0.3609)\n",
      "[Server] Aggregated 1 updates -> agg=99 (avg_tau=4.000, test_loss=1.7065, test_acc=0.3607)\n",
      "[Server] Aggregated 1 updates -> agg=100 (avg_tau=4.000, test_loss=1.7023, test_acc=0.3605)\n",
      "[Server] Stopping: max aggregation rounds reached\n",
      "[Server] Aggregated 1 updates -> agg=101 (avg_tau=4.000, test_loss=1.6917, test_acc=0.3646)\n",
      "[Server] Aggregated 1 updates -> agg=102 (avg_tau=4.000, test_loss=1.6893, test_acc=0.3650)\n",
      "[Server] Aggregated 1 updates -> agg=103 (avg_tau=4.000, test_loss=1.6817, test_acc=0.3670)\n",
      "[Server] Aggregated 1 updates -> agg=104 (avg_tau=4.000, test_loss=1.6777, test_acc=0.3689)\n",
      "\n",
      "✅ Exp1 completed!\n",
      "📁 Results: logs/TrustWeight/Exp1/run_20251130_013402\n",
      "✅ Exp1 completed in 17.10 minutes\n",
      "\n",
      "──────────────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Running Exp2: alpha=0.1, 10% stragglers\n",
      "======================================================================\n",
      "✅ Run folder: logs/TrustWeight/Exp2/run_20251130_015108\n",
      "✅ Data partitioned: 20 clients, alpha=0.1\n",
      "✅ Server initialized (device: mps)\n",
      "✅ Created 20 clients (2 slow, 18 fast)\n",
      "✅ Started 20 client threads\n",
      "🚀 Experiment running... (max 100 rounds)\n",
      "[Server] Aggregated 1 updates -> agg=1 (avg_tau=0.000, test_loss=2.3756, test_acc=0.1124)\n",
      "[Server] Aggregated 1 updates -> agg=2 (avg_tau=1.000, test_loss=2.3950, test_acc=0.1144)\n",
      "[Server] Aggregated 1 updates -> agg=3 (avg_tau=2.000, test_loss=2.3738, test_acc=0.1046)\n",
      "[Server] Aggregated 1 updates -> agg=4 (avg_tau=0.000, test_loss=2.3426, test_acc=0.1078)\n",
      "[Server] Aggregated 1 updates -> agg=5 (avg_tau=4.000, test_loss=2.3294, test_acc=0.1073)\n",
      "[Server] Aggregated 1 updates -> agg=6 (avg_tau=4.000, test_loss=2.3198, test_acc=0.1059)\n",
      "[Server] Aggregated 1 updates -> agg=7 (avg_tau=6.000, test_loss=2.3199, test_acc=0.1017)\n",
      "[Server] Aggregated 1 updates -> agg=8 (avg_tau=5.000, test_loss=2.3199, test_acc=0.1056)\n",
      "[Server] Aggregated 1 updates -> agg=9 (avg_tau=2.000, test_loss=2.3517, test_acc=0.1000)\n",
      "[Server] Aggregated 1 updates -> agg=10 (avg_tau=3.000, test_loss=2.3432, test_acc=0.1000)\n",
      "[Server] Aggregated 1 updates -> agg=11 (avg_tau=5.000, test_loss=2.3339, test_acc=0.1000)\n",
      "[Server] Aggregated 1 updates -> agg=12 (avg_tau=2.000, test_loss=2.3319, test_acc=0.1000)\n",
      "[Server] Aggregated 1 updates -> agg=13 (avg_tau=1.000, test_loss=2.3206, test_acc=0.1006)\n",
      "[Server] Aggregated 1 updates -> agg=14 (avg_tau=4.000, test_loss=2.3224, test_acc=0.0984)\n",
      "[Server] Aggregated 1 updates -> agg=15 (avg_tau=7.000, test_loss=2.3151, test_acc=0.0980)\n",
      "[Server] Aggregated 1 updates -> agg=16 (avg_tau=3.000, test_loss=2.3274, test_acc=0.1010)\n",
      "[Server] Aggregated 1 updates -> agg=17 (avg_tau=3.000, test_loss=2.3269, test_acc=0.1010)\n",
      "[Server] Aggregated 1 updates -> agg=18 (avg_tau=2.000, test_loss=2.3355, test_acc=0.0994)\n",
      "[Server] Aggregated 1 updates -> agg=19 (avg_tau=2.000, test_loss=2.3736, test_acc=0.1001)\n",
      "[Server] Aggregated 1 updates -> agg=20 (avg_tau=6.000, test_loss=2.3813, test_acc=0.1000)\n",
      "[Server] Aggregated 1 updates -> agg=21 (avg_tau=2.000, test_loss=2.3609, test_acc=0.1002)\n",
      "[Server] Aggregated 1 updates -> agg=22 (avg_tau=7.000, test_loss=2.3652, test_acc=0.1000)\n",
      "[Server] Aggregated 1 updates -> agg=23 (avg_tau=2.000, test_loss=2.4252, test_acc=0.1000)\n",
      "[Server] Aggregated 1 updates -> agg=24 (avg_tau=5.000, test_loss=2.4153, test_acc=0.1004)\n",
      "[Server] Aggregated 1 updates -> agg=25 (avg_tau=4.000, test_loss=2.4201, test_acc=0.0994)\n",
      "[Server] Aggregated 1 updates -> agg=26 (avg_tau=4.000, test_loss=2.3909, test_acc=0.1100)\n",
      "[Server] Aggregated 1 updates -> agg=27 (avg_tau=2.000, test_loss=2.4551, test_acc=0.1014)\n",
      "[Server] Aggregated 1 updates -> agg=28 (avg_tau=4.000, test_loss=2.4639, test_acc=0.1026)\n",
      "[Server] Aggregated 1 updates -> agg=29 (avg_tau=4.000, test_loss=2.4591, test_acc=0.1020)\n",
      "[Server] Aggregated 1 updates -> agg=30 (avg_tau=3.000, test_loss=2.4140, test_acc=0.1019)\n",
      "[Server] Aggregated 1 updates -> agg=31 (avg_tau=3.000, test_loss=2.3984, test_acc=0.1009)\n",
      "[Server] Aggregated 1 updates -> agg=32 (avg_tau=2.000, test_loss=2.3350, test_acc=0.1069)\n",
      "[Server] Aggregated 1 updates -> agg=33 (avg_tau=5.000, test_loss=2.3046, test_acc=0.1121)\n",
      "[Server] Aggregated 1 updates -> agg=34 (avg_tau=4.000, test_loss=2.3087, test_acc=0.1282)\n",
      "[Server] Aggregated 1 updates -> agg=35 (avg_tau=3.000, test_loss=2.2991, test_acc=0.1048)\n",
      "[Server] Aggregated 1 updates -> agg=36 (avg_tau=2.000, test_loss=2.2982, test_acc=0.1007)\n",
      "[Server] Aggregated 1 updates -> agg=37 (avg_tau=5.000, test_loss=2.3116, test_acc=0.1012)\n",
      "[Server] Aggregated 1 updates -> agg=38 (avg_tau=2.000, test_loss=2.3925, test_acc=0.1036)\n",
      "[Server] Aggregated 1 updates -> agg=39 (avg_tau=4.000, test_loss=2.3467, test_acc=0.1062)\n",
      "[Server] Aggregated 1 updates -> agg=40 (avg_tau=2.000, test_loss=2.3071, test_acc=0.1518)\n",
      "[Server] Aggregated 1 updates -> agg=41 (avg_tau=5.000, test_loss=2.3045, test_acc=0.1368)\n",
      "[Server] Aggregated 1 updates -> agg=42 (avg_tau=2.000, test_loss=2.4428, test_acc=0.1064)\n",
      "[Server] Aggregated 1 updates -> agg=43 (avg_tau=2.000, test_loss=2.3865, test_acc=0.1199)\n",
      "[Server] Aggregated 1 updates -> agg=44 (avg_tau=6.000, test_loss=2.3782, test_acc=0.1285)\n",
      "[Server] Aggregated 1 updates -> agg=45 (avg_tau=2.000, test_loss=2.4414, test_acc=0.1254)\n",
      "[Server] Aggregated 1 updates -> agg=46 (avg_tau=4.000, test_loss=2.4491, test_acc=0.1470)\n",
      "[Server] Aggregated 1 updates -> agg=47 (avg_tau=6.000, test_loss=2.4270, test_acc=0.1440)\n",
      "[Server] Aggregated 1 updates -> agg=48 (avg_tau=3.000, test_loss=2.4371, test_acc=0.1475)\n",
      "[Server] Aggregated 1 updates -> agg=49 (avg_tau=2.000, test_loss=2.4185, test_acc=0.1373)\n",
      "[Server] Aggregated 1 updates -> agg=50 (avg_tau=2.000, test_loss=2.3339, test_acc=0.1467)\n",
      "[Server] Aggregated 1 updates -> agg=51 (avg_tau=5.000, test_loss=2.2621, test_acc=0.1560)\n",
      "[Server] Aggregated 1 updates -> agg=52 (avg_tau=7.000, test_loss=2.2546, test_acc=0.1473)\n",
      "[Server] Aggregated 1 updates -> agg=53 (avg_tau=3.000, test_loss=2.2537, test_acc=0.1854)\n",
      "[Server] Aggregated 1 updates -> agg=54 (avg_tau=3.000, test_loss=2.2193, test_acc=0.1647)\n",
      "[Server] Aggregated 1 updates -> agg=55 (avg_tau=3.000, test_loss=2.2155, test_acc=0.1414)\n",
      "[Server] Aggregated 1 updates -> agg=56 (avg_tau=7.000, test_loss=2.2059, test_acc=0.1178)\n",
      "[Server] Aggregated 1 updates -> agg=57 (avg_tau=3.000, test_loss=2.2931, test_acc=0.1226)\n",
      "[Server] Aggregated 1 updates -> agg=58 (avg_tau=2.000, test_loss=2.2546, test_acc=0.1346)\n",
      "[Server] Aggregated 1 updates -> agg=59 (avg_tau=6.000, test_loss=2.2469, test_acc=0.1365)\n",
      "[Server] Aggregated 1 updates -> agg=60 (avg_tau=3.000, test_loss=2.2253, test_acc=0.1711)\n",
      "[Server] Aggregated 1 updates -> agg=61 (avg_tau=3.000, test_loss=2.4490, test_acc=0.1301)\n",
      "[Server] Aggregated 1 updates -> agg=62 (avg_tau=4.000, test_loss=2.3966, test_acc=0.1292)\n",
      "[Server] Aggregated 1 updates -> agg=63 (avg_tau=4.000, test_loss=2.3164, test_acc=0.1392)\n",
      "[Server] Aggregated 1 updates -> agg=64 (avg_tau=3.000, test_loss=2.4307, test_acc=0.1348)\n",
      "[Server] Aggregated 1 updates -> agg=65 (avg_tau=3.000, test_loss=2.4268, test_acc=0.1489)\n",
      "[Server] Aggregated 1 updates -> agg=66 (avg_tau=6.000, test_loss=2.4347, test_acc=0.1497)\n",
      "[Server] Aggregated 1 updates -> agg=67 (avg_tau=2.000, test_loss=2.3855, test_acc=0.1491)\n",
      "[Server] Aggregated 1 updates -> agg=68 (avg_tau=1.000, test_loss=2.2686, test_acc=0.1606)\n",
      "[Server] Aggregated 1 updates -> agg=69 (avg_tau=5.000, test_loss=2.2314, test_acc=0.1682)\n",
      "[Server] Aggregated 1 updates -> agg=70 (avg_tau=5.000, test_loss=2.2172, test_acc=0.1806)\n",
      "[Server] Aggregated 1 updates -> agg=71 (avg_tau=3.000, test_loss=2.1582, test_acc=0.1946)\n",
      "[Server] Aggregated 1 updates -> agg=72 (avg_tau=3.000, test_loss=2.1719, test_acc=0.1969)\n",
      "[Server] Aggregated 1 updates -> agg=73 (avg_tau=3.000, test_loss=2.1403, test_acc=0.1950)\n",
      "[Server] Aggregated 1 updates -> agg=74 (avg_tau=3.000, test_loss=2.1507, test_acc=0.1764)\n",
      "[Server] Aggregated 1 updates -> agg=75 (avg_tau=6.000, test_loss=2.1447, test_acc=0.1884)\n",
      "[Server] Aggregated 1 updates -> agg=76 (avg_tau=2.000, test_loss=2.2415, test_acc=0.1458)\n",
      "[Server] Aggregated 1 updates -> agg=77 (avg_tau=3.000, test_loss=2.2394, test_acc=0.1538)\n",
      "[Server] Aggregated 1 updates -> agg=78 (avg_tau=6.000, test_loss=2.2020, test_acc=0.1545)\n",
      "[Server] Aggregated 1 updates -> agg=79 (avg_tau=2.000, test_loss=2.1668, test_acc=0.1719)\n",
      "[Server] Aggregated 1 updates -> agg=80 (avg_tau=3.000, test_loss=2.4626, test_acc=0.1329)\n",
      "[Server] Aggregated 1 updates -> agg=81 (avg_tau=6.000, test_loss=2.4155, test_acc=0.1440)\n",
      "[Server] Aggregated 1 updates -> agg=82 (avg_tau=2.000, test_loss=2.3191, test_acc=0.1453)\n",
      "[Server] Aggregated 1 updates -> agg=83 (avg_tau=3.000, test_loss=2.3478, test_acc=0.1693)\n",
      "[Server] Aggregated 1 updates -> agg=84 (avg_tau=5.000, test_loss=2.3324, test_acc=0.1579)\n",
      "[Server] Aggregated 1 updates -> agg=85 (avg_tau=2.000, test_loss=2.3209, test_acc=0.1299)\n",
      "[Server] Stopping: training finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m exp_start = time.time()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     run_dir = \u001b[43mrun_single_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_dir:\n\u001b[32m     15\u001b[39m         experiment_results[exp_id] = {\n\u001b[32m     16\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrun_dir\u001b[39m\u001b[33m\"\u001b[39m: run_dir,\n\u001b[32m     17\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcompleted\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mduration_min\u001b[39m\u001b[33m\"\u001b[39m: (time.time() - exp_start) / \u001b[32m60.0\u001b[39m\n\u001b[32m     19\u001b[39m         }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 127\u001b[39m, in \u001b[36mrun_single_experiment\u001b[39m\u001b[34m(exp_id, experiments_dict)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🚀 Experiment running... (max \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmax_rounds\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rounds)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# Wait for completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m \u001b[43mserver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[32m    129\u001b[39m     t.join()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 419\u001b[39m, in \u001b[36mAsyncServer.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.should_stop():\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    421\u001b[39m     \u001b[38;5;28mself\u001b[39m.mark_stop(\u001b[38;5;28mself\u001b[39m._stop_reason \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtraining finished\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Server] Aggregated 1 updates -> agg=86 (avg_tau=2.000, test_loss=2.2313, test_acc=0.1507)\n",
      "[Server] Aggregated 1 updates -> agg=87 (avg_tau=6.000, test_loss=2.2481, test_acc=0.1492)\n",
      "[Server] Aggregated 1 updates -> agg=88 (avg_tau=6.000, test_loss=2.2707, test_acc=0.1498)\n",
      "[Server] Aggregated 1 updates -> agg=89 (avg_tau=3.000, test_loss=2.1963, test_acc=0.1648)\n",
      "[Server] Aggregated 1 updates -> agg=90 (avg_tau=3.000, test_loss=2.1722, test_acc=0.1702)\n"
     ]
    }
   ],
   "source": [
    "# Run all 6 experiments sequentially\n",
    "experiment_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ALL 6 EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "    exp_start = time.time()\n",
    "    try:\n",
    "        run_dir = run_single_experiment(exp_id, experiments)\n",
    "        if run_dir:\n",
    "            experiment_results[exp_id] = {\n",
    "                \"run_dir\": run_dir,\n",
    "                \"status\": \"completed\",\n",
    "                \"duration_min\": (time.time() - exp_start) / 60.0\n",
    "            }\n",
    "            print(f\"✅ {exp_id} completed in {experiment_results[exp_id]['duration_min']:.2f} minutes\")\n",
    "        else:\n",
    "            experiment_results[exp_id] = {\"status\": \"failed\", \"duration_min\": (time.time() - exp_start) / 60.0}\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {exp_id} failed: {str(e)}\")\n",
    "        experiment_results[exp_id] = {\"status\": \"error\", \"error\": str(e), \"duration_min\": (time.time() - exp_start) / 60.0}\n",
    "    \n",
    "    print(f\"\\n{'─'*70}\\n\")\n",
    "\n",
    "total_duration = (time.time() - total_start) / 60.0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time: {total_duration:.2f} minutes ({total_duration/60:.2f} hours)\")\n",
    "print(\"\\nResults summary:\")\n",
    "for exp_id, result in experiment_results.items():\n",
    "    if result.get(\"status\") == \"completed\":\n",
    "        print(f\"  {exp_id}: ✅ {result['duration_min']:.2f} min -> {result['run_dir']}\")\n",
    "    else:\n",
    "        print(f\"  {exp_id}: ❌ {result.get('status', 'unknown')}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare All Experiments (After Running)\n",
    "\n",
    "This section creates comparison plots across all 6 experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all experiments (load results from experiment_results)\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    # Load all CSV files\n",
    "    all_data = {}\n",
    "    for exp_id, result in experiment_results.items():\n",
    "        if result.get(\"status\") == \"completed\":\n",
    "            csv_path = result[\"run_dir\"] / \"TrustWeight.csv\"\n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['time_min'] = df['time'] / 60.0\n",
    "                all_data[exp_id] = df\n",
    "                print(f\"✅ Loaded {exp_id}: {len(df)} rows\")\n",
    "    \n",
    "    if len(all_data) > 0:\n",
    "        # Create comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "        \n",
    "        # Plot 1: Accuracy vs Rounds\n",
    "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
    "            axes[0, 0].plot(df['total_agg'], df['test_acc'], \n",
    "                          label=f\"{exp_id} ({experiments[exp_id]['name']})\", \n",
    "                          linewidth=2, color=colors[i % len(colors)])\n",
    "        axes[0, 0].set_xlabel('Round', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "        axes[0, 0].set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].legend(fontsize=9, loc='lower right')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_ylim([0, 1.0])\n",
    "        \n",
    "        # Plot 2: Accuracy vs Wall Clock Time\n",
    "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
    "            axes[0, 1].plot(df['time_min'], df['test_acc'], \n",
    "                          label=f\"{exp_id}\", \n",
    "                          linewidth=2, color=colors[i % len(colors)])\n",
    "        axes[0, 1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Test Accuracy', fontsize=12)\n",
    "        axes[0, 1].set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].legend(fontsize=9, loc='lower right')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_ylim([0, 1.0])\n",
    "        \n",
    "        # Plot 3: Best Accuracy by Experiment\n",
    "        best_accs = {exp_id: df['test_acc'].max() for exp_id, df in all_data.items()}\n",
    "        exp_names = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\" \n",
    "                    if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\" \n",
    "                    for exp_id in best_accs.keys()]\n",
    "        axes[1, 0].bar(range(len(best_accs)), list(best_accs.values()), \n",
    "                      color=colors[:len(best_accs)], alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_xticks(range(len(best_accs)))\n",
    "        axes[1, 0].set_xticklabels(exp_names, fontsize=9, rotation=45, ha='right')\n",
    "        axes[1, 0].set_ylabel('Best Test Accuracy', fontsize=12)\n",
    "        axes[1, 0].set_title('Best Test Accuracy by Experiment', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[1, 0].set_ylim([0, 1.0])\n",
    "        # Add value labels\n",
    "        for i, (exp_id, acc) in enumerate(best_accs.items()):\n",
    "            axes[1, 0].text(i, acc, f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Plot 4: Time to reach 50% accuracy\n",
    "        times_to_50 = {}\n",
    "        for exp_id, df in all_data.items():\n",
    "            mask = df['test_acc'] >= 0.5\n",
    "            if mask.any():\n",
    "                first_idx = mask.idxmax()\n",
    "                times_to_50[exp_id] = df.loc[first_idx, 'time_min']\n",
    "            else:\n",
    "                times_to_50[exp_id] = None\n",
    "        \n",
    "        valid_times = {k: v for k, v in times_to_50.items() if v is not None}\n",
    "        if len(valid_times) > 0:\n",
    "            exp_names_50 = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\" \n",
    "                           if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\" \n",
    "                           for exp_id in valid_times.keys()]\n",
    "            axes[1, 1].bar(range(len(valid_times)), list(valid_times.values()), \n",
    "                         color=colors[:len(valid_times)], alpha=0.7, edgecolor='black')\n",
    "            axes[1, 1].set_xticks(range(len(valid_times)))\n",
    "            axes[1, 1].set_xticklabels(exp_names_50, fontsize=9, rotation=45, ha='right')\n",
    "            axes[1, 1].set_ylabel('Time to Reach 50% Accuracy (minutes)', fontsize=12)\n",
    "            axes[1, 1].set_title('Convergence Speed: Time to 50% Accuracy', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "            # Add value labels\n",
    "            for i, (exp_id, t) in enumerate(valid_times.items()):\n",
    "                axes[1, 1].text(i, t, f'{t:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        # Use Google Drive path if in Colab, otherwise local\n",
    "        comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"TrustWeight\" / \"comparisons\"\n",
    "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(comparison_dir / \"all_experiments_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\n✅ Comparison plot saved: {comparison_dir / 'all_experiments_comparison.png'}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPERIMENT COMPARISON SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_data:\n",
    "                df = all_data[exp_id]\n",
    "                cfg = experiments[exp_id]\n",
    "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
    "                      f\"{df['test_acc'].max():<10.4f} {df['test_acc'].iloc[-1]:<11.4f} \"\n",
    "                      f\"{df['time_min'].iloc[-1]:<12.2f}\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"⚠️  No completed experiments found. Run experiments first.\")\n",
    "else:\n",
    "    print(\"⚠️  No experiment results found. Run the experiments first using Section 8.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10 dataset\n",
    "# IMPORTANT: Make sure you've run Section 7 first to set the 'config' variable!\n",
    "if 'config' not in globals():\n",
    "    raise NameError(\n",
    "        \"❌ 'config' is not defined!\\n\"\n",
    "        \"Please run Section 7 first to select an experiment (Exp1-Exp6).\\n\"\n",
    "        \"Or use Section 8 (automated runner) which handles everything automatically.\"\n",
    "    )\n",
    "\n",
    "print(\"Downloading CIFAR-10 dataset...\")\n",
    "dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n",
    "print(f\"✅ Dataset loaded: {len(dd.train_dataset)} training samples, {len(dd.test_dataset)} test samples\")\n",
    "print(f\"✅ Number of classes: {dd.num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which experiment to run manually (Exp1, Exp2, Exp3, Exp4, Exp5, or Exp6)\n",
    "# This sets the 'config' variable for the manual cells below\n",
    "EXP_ID = \"Exp1\"  # Change this to Exp2, Exp3, etc. to run different experiments\n",
    "\n",
    "if EXP_ID in experiments:\n",
    "    config = experiments[EXP_ID]\n",
    "    print(f\"✅ Selected experiment: {EXP_ID} - {config['name']}\")\n",
    "    print(f\"   Alpha: {config['partition_alpha']}\")\n",
    "    print(f\"   Stragglers: {config['clients']['struggle_percent']}%\")\n",
    "    print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
    "else:\n",
    "    raise ValueError(f\"Experiment {EXP_ID} not found! Use Exp1-Exp6.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which experiment to run manually (Exp1, Exp2, Exp3, Exp4, Exp5, or Exp6)\n",
    "# This sets the 'config' variable for the manual cells below\n",
    "EXP_ID = \"Exp1\"  # Change this to Exp2, Exp3, etc. to run different experiments\n",
    "\n",
    "if EXP_ID in experiments:\n",
    "    config = experiments[EXP_ID]\n",
    "    print(f\"✅ Selected experiment: {EXP_ID} - {config['name']}\")\n",
    "    print(f\"   Alpha: {config['partition_alpha']}\")\n",
    "    print(f\"   Stragglers: {config['clients']['struggle_percent']}%\")\n",
    "    print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
    "else:\n",
    "    raise ValueError(f\"Experiment {EXP_ID} not found! Use Exp1-Exp6.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Partition Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data among clients\n",
    "print(f\"Partitioning data with alpha={config['partition_alpha']}...\")\n",
    "dd.distribute_data(\n",
    "    num_clients=int(config[\"clients\"][\"total\"]),\n",
    "    alpha=float(config[\"partition_alpha\"]),\n",
    "    seed=int(config[\"seed\"])\n",
    ")\n",
    "print(f\"✅ Data partitioned among {config['clients']['total']} clients\")\n",
    "for i in range(min(3, config['clients']['total'])):\n",
    "    print(f\"   Client {i}: {len(dd.get_client_data(i))} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Setup Folders and Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamped run folder\n",
    "run_dir = Path(\"logs\") / \"TrustWeight\" / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write COMMIT.txt (simulated git hash for notebook)\n",
    "commit_hash = \"notebook_run\"\n",
    "csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n",
    "with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n",
    "    f.write(f\"{commit_hash},{csv_header}\\n\")\n",
    "\n",
    "# Save config to run folder\n",
    "with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"✅ Run folder created: {run_dir}\")\n",
    "print(f\"✅ COMMIT.txt and CONFIG.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Server and Clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = int(config.get(\"seed\", 42))\n",
    "set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Build global model\n",
    "global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n",
    "print(f\"✅ Global model created: ResNet-18\")\n",
    "\n",
    "# Initialize server\n",
    "server = AsyncServer(\n",
    "    global_model=global_model,\n",
    "    total_train_samples=len(dd.train_dataset),\n",
    "    buffer_size=int(config[\"trustweight\"][\"buffer_size\"]),\n",
    "    buffer_timeout_s=float(config[\"trustweight\"][\"buffer_timeout_s\"]),\n",
    "    use_sample_weighing=bool(config[\"trustweight\"][\"use_sample_weighing\"]),\n",
    "    target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n",
    "    max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n",
    "    eval_interval_s=int(config[\"eval\"][\"interval_seconds\"]),\n",
    "    data_dir=config[\"data\"][\"data_dir\"],\n",
    "    checkpoints_dir=str(run_dir / \"checkpoints\"),\n",
    "    logs_dir=str(run_dir),\n",
    "    global_log_csv=str(run_dir / \"TrustWeight.csv\"),\n",
    "    client_participation_csv=str(run_dir / \"TrustWeightClientParticipation.csv\"),\n",
    "    final_model_path=str(run_dir / \"TrustWeightModel.pt\"),\n",
    "    resume=False,  # Start fresh in notebook\n",
    "    device=get_device(),\n",
    "    eta=float(config[\"trustweight\"].get(\"eta\", 0.5)),\n",
    ")\n",
    "print(f\"✅ Server initialized\")\n",
    "print(f\"   Device: {server.device}\")\n",
    "print(f\"   Buffer size: {server.buffer_size}\")\n",
    "print(f\"   Eta: {server.eta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup client delays (for heterogeneity simulation)\n",
    "n = int(config[\"clients\"][\"total\"])\n",
    "pct = max(0, min(100, int(config[\"clients\"].get(\"struggle_percent\", 0))))\n",
    "k_slow = (n * pct) // 100\n",
    "slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
    "\n",
    "a_s, b_s = config[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
    "a_f, b_f = config[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
    "fix_delays = bool(config[\"clients\"].get(\"fix_delays_per_client\", True))\n",
    "jitter = float(config[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
    "\n",
    "per_client_base_delay: Dict[int, float] = {}\n",
    "if fix_delays:\n",
    "    for cid in range(n):\n",
    "        if cid in slow_ids:\n",
    "            per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
    "        else:\n",
    "            per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
    "\n",
    "# Create clients\n",
    "clients = []\n",
    "for cid in range(n):\n",
    "    indices = dd.partitions[cid] if cid in dd.partitions else []\n",
    "    base_delay = per_client_base_delay.get(cid, 0.0)\n",
    "    is_slow = cid in slow_ids\n",
    "    clients.append(AsyncClient(\n",
    "            cid=cid,\n",
    "            indices=indices,\n",
    "            cfg=config,\n",
    "        )), (float(a_f), float(b_f))),\n",
    "        jitter=jitter,\n",
    "        fix_delay=fix_delays,\n",
    "    ))\n",
    "\n",
    "print(f\"✅ Created {len(clients)} clients\")\n",
    "print(f\"   Slow clients: {len(slow_ids)}\")\n",
    "print(f\"   Fast clients: {n - len(slow_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results for all 6 experiments\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"FEDBUFF RESULTS - ALL 6 EXPERIMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n",
    "            run_dir = experiment_results[exp_id][\"run_dir\"]\n",
    "            csv_path = run_dir / \"TrustWeight.csv\"\n",
    "            \n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                all_results[exp_id] = df\n",
    "                \n",
    "                exp_config = experiments[exp_id]\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"{exp_id}: {exp_config['name']}\")\n",
    "                print(f\"{'='*80}\")\n",
    "                print(f\"  Alpha: {exp_config['partition_alpha']}\")\n",
    "                print(f\"  Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
    "                print(f\"  Max rounds: {exp_config['train']['max_rounds']}\")\n",
    "                print(f\"\\n  Total rounds completed: {df['total_agg'].max()}\")\n",
    "                \n",
    "                final_row = df.iloc[-1]\n",
    "                print(f\"\\n  Final metrics:\")\n",
    "                print(f\"    - Test accuracy: {final_row['test_acc']:.4f}\")\n",
    "                print(f\"    - Train accuracy: {final_row['avg_train_acc']:.4f}\")\n",
    "                print(f\"    - Test loss: {final_row['test_loss']:.4f}\")\n",
    "                print(f\"    - Total time: {final_row['time']:.1f} seconds ({final_row['time']/60:.2f} minutes)\")\n",
    "                \n",
    "                best_acc = df['test_acc'].max()\n",
    "                best_round = df.loc[df['test_acc'].idxmax(), 'total_agg']\n",
    "                print(f\"\\n  Best test accuracy: {best_acc:.4f} (round {best_round})\")\n",
    "                \n",
    "                print(f\"\\n  Run folder: {run_dir}\")\n",
    "            else:\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"{exp_id}: Results file not found\")\n",
    "                print(f\"  Run folder: {run_dir}\")\n",
    "        else:\n",
    "            status = experiment_results.get(exp_id, {}).get(\"status\", \"not run\")\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"{exp_id}: {status.upper()}\")\n",
    "            if status == \"error\":\n",
    "                print(f\"  Error: {experiment_results[exp_id].get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Summary table\n",
    "    if len(all_results) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SUMMARY TABLE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Rounds':<8} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_results:\n",
    "                df = all_results[exp_id]\n",
    "                cfg = experiments[exp_id]\n",
    "                final_row = df.iloc[-1]\n",
    "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
    "                      f\"{df['total_agg'].max():<8} {df['test_acc'].max():<10.4f} {final_row['test_acc']:<11.4f} \"\n",
    "                      f\"{final_row['time']/60:<12.2f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show first and last rows for each experiment\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DETAILED DATA PREVIEW\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_results:\n",
    "                df = all_results[exp_id]\n",
    "                exp_config = experiments[exp_id]\n",
    "                print(f\"\\n{exp_id}: {exp_config['name']}\")\n",
    "                print(f\"  First 3 rows:\")\n",
    "                print(df.head(3).to_string(index=False))\n",
    "                print(f\"\\n  Last 3 rows:\")\n",
    "                print(df.tail(3).to_string(index=False))\n",
    "                print()\n",
    "    else:\n",
    "        print(\"\\n⚠️  No completed experiments found. Run experiments first using Section 8.\")\n",
    "else:\n",
    "    print(\"⚠️  No experiment results found. Run experiments first using Section 8.\")\n",
    "    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run TrustWeight Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization suite for all 6 experiments\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    all_data = {}\n",
    "    all_participation = {}\n",
    "    \n",
    "    # Load all experiment data\n",
    "    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n",
    "            run_dir = experiment_results[exp_id][\"run_dir\"]\n",
    "            csv_path = run_dir / \"TrustWeight.csv\"\n",
    "            participation_path = run_dir / \"TrustWeightClientParticipation.csv\"\n",
    "            \n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['time_min'] = df['time'] / 60.0\n",
    "                all_data[exp_id] = {\n",
    "                    'df': df,\n",
    "                    'run_dir': run_dir,\n",
    "                    'config': experiments[exp_id]\n",
    "                }\n",
    "                \n",
    "                if participation_path.exists():\n",
    "                    part_df = pd.read_csv(participation_path)\n",
    "                    all_participation[exp_id] = part_df\n",
    "    \n",
    "    if len(all_data) > 0:\n",
    "        print(f\"✅ Loaded data for {len(all_data)} experiments\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Generate individual plots for each experiment\n",
    "        for exp_id, exp_data in all_data.items():\n",
    "            df = exp_data['df']\n",
    "            run_dir = exp_data['run_dir']\n",
    "            exp_config = exp_data['config']\n",
    "            participation_path = run_dir / \"TrustWeightClientParticipation.csv\"\n",
    "            \n",
    "            print(f\"\\n📊 Generating plots for {exp_id}: {exp_config['name']}\")\n",
    "            \n",
    "            # Plot 1: Accuracy vs Rounds\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "            train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "            train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n",
    "            if len(train_acc) > 0:\n",
    "                axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "            axes[0].set_xlabel('Round', fontsize=12)\n",
    "            axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "            axes[0].set_title(f'{exp_id}: Accuracy vs Rounds', fontsize=14, fontweight='bold')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].legend(fontsize=10)\n",
    "            axes[0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "            train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "            train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n",
    "            if len(train_loss) > 0:\n",
    "                axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "            axes[1].set_xlabel('Round', fontsize=12)\n",
    "            axes[1].set_ylabel('Loss', fontsize=12)\n",
    "            axes[1].set_title(f'{exp_id}: Loss vs Rounds', fontsize=14, fontweight='bold')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            axes[1].legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 2: Accuracy vs Wall Clock Time\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "            train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "            train_time = df[df['avg_train_acc'] > 0]['time_min']\n",
    "            if len(train_acc_time) > 0:\n",
    "                axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "            axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "            axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "            axes[0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].legend(fontsize=10)\n",
    "            axes[0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "            train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "            train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n",
    "            if len(train_loss_time) > 0:\n",
    "                axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "            axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "            axes[1].set_ylabel('Loss', fontsize=12)\n",
    "            axes[1].set_title(f'{exp_id}: Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            axes[1].legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 3: Convergence Speed Analysis\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "            times_to_threshold = []\n",
    "            rounds_to_threshold = []\n",
    "            reached_thresholds = []\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                mask = df['test_acc'] >= thresh\n",
    "                if mask.any():\n",
    "                    first_idx = mask.idxmax()\n",
    "                    times_to_threshold.append(df.loc[first_idx, 'time_min'])\n",
    "                    rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n",
    "                    reached_thresholds.append(thresh)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if len(reached_thresholds) > 0:\n",
    "                axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "                axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "                axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n",
    "                axes[0].set_title(f'{exp_id}: Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "                axes[0].set_xticks(range(len(reached_thresholds)))\n",
    "                axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "                axes[0].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n",
    "                    axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "                \n",
    "                axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "                axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "                axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n",
    "                axes[1].set_title(f'{exp_id}: Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "                axes[1].set_xticks(range(len(reached_thresholds)))\n",
    "                axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "                axes[1].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n",
    "                    axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 4: Training Efficiency\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "            \n",
    "            if len(df) > 1:\n",
    "                df_sorted = df.sort_values('time_min')\n",
    "                efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n",
    "                efficiency = efficiency.fillna(0)\n",
    "                \n",
    "                ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n",
    "                ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n",
    "                ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "                ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n",
    "                ax.set_title(f'{exp_id}: Training Efficiency', fontsize=14, fontweight='bold')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                max_eff_idx = efficiency.idxmax()\n",
    "                max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n",
    "                max_eff_val = efficiency.loc[max_eff_idx]\n",
    "                ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n",
    "                ax.legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 5: Client Participation Analysis\n",
    "            if participation_path.exists():\n",
    "                part_df = pd.read_csv(participation_path)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "                \n",
    "                client_counts = part_df['client_id'].value_counts().sort_index()\n",
    "                axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n",
    "                axes[0, 0].set_xlabel('Client ID', fontsize=11)\n",
    "                axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n",
    "                axes[0, 0].set_title(f'{exp_id}: Client Participation Frequency', fontsize=12, fontweight='bold')\n",
    "                axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n",
    "                axes[0, 1].plot(participation_by_round.index, participation_by_round.values, \n",
    "                               marker='o', markersize=4, linewidth=2, color='coral')\n",
    "                axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "                axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n",
    "                axes[0, 1].set_title(f'{exp_id}: Client Participation per Round', fontsize=12, fontweight='bold')\n",
    "                axes[0, 1].grid(True, alpha=0.3)\n",
    "                \n",
    "                client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n",
    "                axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values, \n",
    "                                marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n",
    "                axes[1, 0].set_xlabel('Round', fontsize=11)\n",
    "                axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n",
    "                axes[1, 0].set_title(f'{exp_id}: Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "                axes[1, 0].set_ylim([0, 1.0])\n",
    "                \n",
    "                if len(part_df) > 0:\n",
    "                    client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values \n",
    "                                  for cid in sorted(part_df['client_id'].unique())[:10]]\n",
    "                    if len(client_accs) > 0:\n",
    "                        axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n",
    "                        axes[1, 1].set_xlabel('Client ID', fontsize=11)\n",
    "                        axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
    "                        axes[1, 1].set_title(f'{exp_id}: Client Accuracy Distribution', fontsize=12, fontweight='bold')\n",
    "                        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "                        axes[1, 1].set_ylim([0, 1.0])\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(run_dir / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # Plot 6: Training Progress Summary\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            \n",
    "            axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "            train_acc_plot = df[df['avg_train_acc'] > 0]\n",
    "            if len(train_acc_plot) > 0:\n",
    "                axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'], \n",
    "                               'r--', linewidth=2, label='Train')\n",
    "            axes[0, 0].set_xlabel('Round', fontsize=11)\n",
    "            axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "            axes[0, 0].set_title(f'{exp_id}: Accuracy Trajectory', fontsize=12, fontweight='bold')\n",
    "            axes[0, 0].legend(fontsize=10)\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n",
    "            train_loss_plot = df[df['avg_train_loss'] > 0]\n",
    "            if len(train_loss_plot) > 0:\n",
    "                axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'], \n",
    "                               'r--', linewidth=2, label='Train')\n",
    "            axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "            axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
    "            axes[0, 1].set_title(f'{exp_id}: Loss Trajectory', fontsize=12, fontweight='bold')\n",
    "            axes[0, 1].legend(fontsize=10)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "            if len(train_acc_plot) > 0:\n",
    "                train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n",
    "                axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'], \n",
    "                                'r--', linewidth=2, label='Train')\n",
    "            axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n",
    "            axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "            axes[1, 0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n",
    "            axes[1, 0].legend(fontsize=10)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n",
    "            axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n",
    "            axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n",
    "            axes[1, 1].set_title(f'{exp_id}: Round Progression Over Time', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            if len(df) > 1:\n",
    "                total_time = df['time_min'].iloc[-1]\n",
    "                total_rounds = df['total_agg'].iloc[-1]\n",
    "                rate = total_rounds / total_time if total_time > 0 else 0\n",
    "                axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min', \n",
    "                               transform=axes[1, 1].transAxes, fontsize=10,\n",
    "                               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  ✅ All plots saved to {run_dir}\")\n",
    "        \n",
    "        # Create comparison plots across all experiments\n",
    "        print(f\"\\n📊 Creating comparison plots across all experiments...\")\n",
    "        \n",
    "        # Comparison Plot 1: Test Accuracy vs Rounds (all experiments)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n",
    "            df = exp_data['df']\n",
    "            exp_config = exp_data['config']\n",
    "            label = f\"{exp_id} (α={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n",
    "            ax.plot(df['total_agg'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n",
    "        ax.set_xlabel('Round', fontsize=12)\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "        ax.set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([0, 1.0])\n",
    "        \n",
    "        # Save comparison plot\n",
    "        comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"TrustWeight\" / \"comparisons\"\n",
    "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(comparison_dir / \"comparison_accuracy_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ✅ Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_rounds.png'}\")\n",
    "        \n",
    "        # Comparison Plot 2: Test Accuracy vs Time (all experiments)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n",
    "            df = exp_data['df']\n",
    "            exp_config = exp_data['config']\n",
    "            label = f\"{exp_id} (α={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n",
    "            ax.plot(df['time_min'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n",
    "        ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "        ax.set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([0, 1.0])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(comparison_dir / \"comparison_accuracy_vs_time.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  ✅ Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_time.png'}\")\n",
    "        \n",
    "        print(f\"\\n✅ All visualizations completed!\")\n",
    "        print(f\"   Individual plots saved to each experiment's run folder\")\n",
    "        print(f\"   Comparison plots saved to: {comparison_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  No completed experiments found. Run experiments first using Section 8.\")\n",
    "else:\n",
    "    print(\"⚠️  No experiment results found. Run experiments first using Section 8.\")\n",
    "    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start evaluation timer\n",
    "server.start_eval_timer()\n",
    "print(\"✅ Evaluation timer started\")\n",
    "\n",
    "# Concurrency gate\n",
    "sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n",
    "\n",
    "def client_loop(client: AsyncClient):\n",
    "    \"\"\"Client training loop.\"\"\"\n",
    "    while True:\n",
    "        with sem:\n",
    "            cont = client.run_once(server)\n",
    "        if not cont:\n",
    "            break\n",
    "        time.sleep(0.05)\n",
    "\n",
    "# Launch client threads\n",
    "threads = []\n",
    "for cl in clients:\n",
    "    t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "print(f\"✅ Started {len(threads)} client threads\")\n",
    "print(f\"\\n🚀 TrustWeight experiment running...\")\n",
    "print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
    "print(f\"   Target accuracy: {config['eval']['target_accuracy']}\")\n",
    "print(f\"\\nResults will be saved to: {run_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for completion\n",
    "server.wait()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"\\n✅ Experiment completed!\")\n",
    "print(f\"\\n📁 Results saved to: {run_dir}\")\n",
    "print(f\"   - TrustWeight.csv: Global metrics\")\n",
    "print(f\"   - TrustWeightClientParticipation.csv: Client participation\")\n",
    "print(f\"   - TrustWeightModel.pt: Final model\")\n",
    "print(f\"   - CONFIG.yaml: Configuration used\")\n",
    "print(f\"   - COMMIT.txt: Run metadata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. View Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "csv_path = run_dir / \"TrustWeight.csv\"\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"=== TrustWeight Results ===\")\n",
    "    print(f\"\\nTotal rounds: {df['total_agg'].max()}\")\n",
    "    print(f\"\\nFinal metrics:\")\n",
    "    final_row = df.iloc[-1]\n",
    "    print(f\"  - Test accuracy: {final_row['test_acc']:.4f}\")\n",
    "    print(f\"  - Train accuracy: {final_row['avg_train_acc']:.4f}\")\n",
    "    print(f\"  - Test loss: {final_row['test_loss']:.4f}\")\n",
    "    print(f\"  - Total time: {final_row['time']:.1f} seconds\")\n",
    "    \n",
    "    print(f\"\\nBest test accuracy: {df['test_acc'].max():.4f} (round {df.loc[df['test_acc'].idxmax(), 'total_agg']})\")\n",
    "    \n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\nLast 5 rows:\")\n",
    "    print(df.tail())\n",
    "else:\n",
    "    print(\"Results file not found yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Comprehensive Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization suite\n",
    "csv_path = run_dir / \"TrustWeight.csv\"\n",
    "participation_path = run_dir / \"TrustWeightClientParticipation.csv\"\n",
    "\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert time to minutes for better readability\n",
    "    df['time_min'] = df['time'] / 60.0\n",
    "    \n",
    "    # ========== Plot 1: Accuracy vs Rounds ==========\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Test accuracy over rounds\n",
    "    axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "    train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "    train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n",
    "    if len(train_acc) > 0:\n",
    "        axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "    axes[0].set_xlabel('Round', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Accuracy vs Rounds', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # Loss over rounds\n",
    "    axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "    train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "    train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n",
    "    if len(train_loss) > 0:\n",
    "        axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "    axes[1].set_xlabel('Round', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Loss vs Rounds', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"✅ Plot 1 saved: 1_accuracy_loss_vs_rounds.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Plot 2: Accuracy vs Wall Clock Time ==========\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Test accuracy over time\n",
    "    axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "    train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "    train_time = df[df['avg_train_acc'] > 0]['time_min']\n",
    "    if len(train_acc_time) > 0:\n",
    "        axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "    axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # Loss over time\n",
    "    axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "    train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "    train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n",
    "    if len(train_loss_time) > 0:\n",
    "        axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "    axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"✅ Plot 2 saved: 2_accuracy_loss_vs_time.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Plot 3: Convergence Speed Analysis ==========\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Time to reach accuracy thresholds\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    times_to_threshold = []\n",
    "    rounds_to_threshold = []\n",
    "    reached_thresholds = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        mask = df['test_acc'] >= thresh\n",
    "        if mask.any():\n",
    "            first_idx = mask.idxmax()\n",
    "            times_to_threshold.append(df.loc[first_idx, 'time_min'])\n",
    "            rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n",
    "            reached_thresholds.append(thresh)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if len(reached_thresholds) > 0:\n",
    "        axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "        axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "        axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n",
    "        axes[0].set_title('Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xticks(range(len(reached_thresholds)))\n",
    "        axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n",
    "            axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "        axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "        axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n",
    "        axes[1].set_title('Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xticks(range(len(reached_thresholds)))\n",
    "        axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n",
    "            axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"✅ Plot 3 saved: 3_convergence_speed.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Plot 4: Training Efficiency (Accuracy per Time) ==========\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Calculate efficiency: accuracy gain per minute\n",
    "    if len(df) > 1:\n",
    "        df_sorted = df.sort_values('time_min')\n",
    "        efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n",
    "        efficiency = efficiency.fillna(0)\n",
    "        \n",
    "        ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n",
    "        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n",
    "        ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "        ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n",
    "        ax.set_title('Training Efficiency: Accuracy Gain Rate Over Time', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight peak efficiency\n",
    "        max_eff_idx = efficiency.idxmax()\n",
    "        max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n",
    "        max_eff_val = efficiency.loc[max_eff_idx]\n",
    "        ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n",
    "        ax.legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"✅ Plot 4 saved: 4_training_efficiency.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Plot 5: Client Participation Analysis ==========\n",
    "    if participation_path.exists():\n",
    "        part_df = pd.read_csv(participation_path)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Client participation frequency\n",
    "        client_counts = part_df['client_id'].value_counts().sort_index()\n",
    "        axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n",
    "        axes[0, 0].set_xlabel('Client ID', fontsize=11)\n",
    "        axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n",
    "        axes[0, 0].set_title('Client Participation Frequency', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Client participation over rounds\n",
    "        participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n",
    "        axes[0, 1].plot(participation_by_round.index, participation_by_round.values, \n",
    "                       marker='o', markersize=4, linewidth=2, color='coral')\n",
    "        axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "        axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n",
    "        axes[0, 1].set_title('Client Participation per Round', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Average client accuracy over time\n",
    "        client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n",
    "        axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values, \n",
    "                        marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n",
    "        axes[1, 0].set_xlabel('Round', fontsize=11)\n",
    "        axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n",
    "        axes[1, 0].set_title('Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_ylim([0, 1.0])\n",
    "        \n",
    "        # Client accuracy distribution (box plot)\n",
    "        if len(part_df) > 0:\n",
    "            client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values \n",
    "                          for cid in sorted(part_df['client_id'].unique())[:10]]  # Limit to first 10 clients for readability\n",
    "            if len(client_accs) > 0:\n",
    "                axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n",
    "                axes[1, 1].set_xlabel('Client ID', fontsize=11)\n",
    "                axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
    "                axes[1, 1].set_title('Client Accuracy Distribution', fontsize=12, fontweight='bold')\n",
    "                axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "                axes[1, 1].set_ylim([0, 1.0])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(run_dir / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n",
    "        print(f\"✅ Plot 5 saved: 5_client_participation.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    # ========== Plot 6: Training Progress Summary ==========\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Accuracy trajectory\n",
    "    axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "    train_acc_plot = df[df['avg_train_acc'] > 0]\n",
    "    if len(train_acc_plot) > 0:\n",
    "        axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'], \n",
    "                       'r--', linewidth=2, label='Train')\n",
    "    axes[0, 0].set_xlabel('Round', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[0, 0].set_title('Accuracy Trajectory', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # Loss trajectory\n",
    "    axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n",
    "    train_loss_plot = df[df['avg_train_loss'] > 0]\n",
    "    if len(train_loss_plot) > 0:\n",
    "        axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'], \n",
    "                       'r--', linewidth=2, label='Train')\n",
    "    axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
    "    axes[0, 1].set_title('Loss Trajectory', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=10)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy vs time\n",
    "    axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "    if len(train_acc_plot) > 0:\n",
    "        train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n",
    "        axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'], \n",
    "                        'r--', linewidth=2, label='Train')\n",
    "    axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[1, 0].set_title('Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=10)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # Round progression over time\n",
    "    axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n",
    "    axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n",
    "    axes[1, 1].set_title('Round Progression Over Time', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add rate annotation\n",
    "    if len(df) > 1:\n",
    "        total_time = df['time_min'].iloc[-1]\n",
    "        total_rounds = df['total_agg'].iloc[-1]\n",
    "        rate = total_rounds / total_time if total_time > 0 else 0\n",
    "        axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min', \n",
    "                       transform=axes[1, 1].transAxes, fontsize=10,\n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"✅ Plot 6 saved: 6_training_summary.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Print Summary Statistics ==========\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Rounds: {df['total_agg'].max()}\")\n",
    "    print(f\"Total Time: {df['time_min'].iloc[-1]:.2f} minutes ({df['time'].iloc[-1]:.2f} seconds)\")\n",
    "    print(f\"Final Test Accuracy: {df['test_acc'].iloc[-1]:.4f}\")\n",
    "    print(f\"Best Test Accuracy: {df['test_acc'].max():.4f} (Round {df.loc[df['test_acc'].idxmax(), 'total_agg']})\")\n",
    "    if len(df) > 1:\n",
    "        print(f\"Average Round Time: {(df['time'].iloc[-1] / df['total_agg'].iloc[-1]):.2f} seconds\")\n",
    "        print(f\"Rounds per Minute: {(df['total_agg'].iloc[-1] / df['time_min'].iloc[-1]):.2f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"Results file not found yet.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
