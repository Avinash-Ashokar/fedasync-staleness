{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrustWeight: Complete Self-Contained Notebook\n",
    "\n",
    "This notebook contains all code needed to run TrustWeight experiments:\n",
    "- Library installation\n",
    "- Data downloading and loading\n",
    "- All TrustWeight implementation\n",
    "- Logging and checkpointing\n",
    "\n",
    "**Run cells sequentially from top to bottom.**\n",
    "\n",
    "## Google Colab Setup\n",
    "This notebook is configured to save all results (logs, checkpoints, models) to Google Drive.\n",
    "Data will be downloaded locally for faster access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Colab Setup (Mount Drive)\n",
    "\n",
    "**Skip this section if running locally.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Google Colab: Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Define the target directory for saving results\n",
    "    OUTPUT_DIR = \"/content/drive/MyDrive/colab/dml_project\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"âœ… Google Drive mounted\")\n",
    "    print(f\"âœ… Output directory set to: {OUTPUT_DIR}\")\n",
    "    print(f\"ðŸ“ All logs, checkpoints, and results will be saved to Google Drive\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    OUTPUT_DIR = None\n",
    "    print(\"âš ï¸  Not running in Google Colab - using local paths\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "%pip install torch torchvision pytorch-lightning pyyaml numpy matplotlib pandas -q\n",
    "# Silence libraries\n",
    "import os\n",
    "import logging, warnings\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "os.environ[\"LIGHTNING_DISABLE_RICH\"] = \"1\"\n",
    "for name in [\n",
    "    \"pytorch_lightning\", \"lightning\", \"lightning.pytorch\",\n",
    "    \"lightning_fabric\", \"lightning_utilities\", \"torch\", \"torchvision\",\n",
    "]:\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "    logging.getLogger(name).propagate = False\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Core imports\n",
    "import time\n",
    "import csv\n",
    "import threading\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Setup paths based on environment\n",
    "if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "    # Google Colab: Save everything to Drive except data\n",
    "    BASE_OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "    DATA_DIR = Path(\"./data\")  # Local for faster access\n",
    "    print(f\"âœ… Google Colab mode: Results â†’ {BASE_OUTPUT_DIR}\")\n",
    "    print(f\"âœ… Data directory: {DATA_DIR} (local)\")\n",
    "else:\n",
    "    # Local execution: Use current directory\n",
    "    BASE_OUTPUT_DIR = Path(\".\")\n",
    "    DATA_DIR = Path(\"./data\")\n",
    "    print(f\"âœ… Local mode: Results â†’ {BASE_OUTPUT_DIR}\")\n",
    "\n",
    "print(\"âœ… Libraries imported\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Helper Functions ==========\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed all RNGs for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Return the first available computation device (CUDA/MPS/CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model Utilities ==========\n",
    "\n",
    "def build_resnet18(num_classes: int = 10, pretrained: bool = False) -> nn.Module:\n",
    "    \"\"\"Create ResNet-18 adapted for CIFAR-10.\"\"\"\n",
    "    if pretrained:\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        m = models.resnet18(weights=None)\n",
    "    # CIFAR-10: 32x32 -> use 3x3 conv, stride 1, no maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    # Replace classifier\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    m.num_classes = num_classes\n",
    "    return m\n",
    "\n",
    "\n",
    "def state_to_list(state: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
    "    \"\"\"Flatten a state_dict to a list of tensors on CPU.\"\"\"\n",
    "    return [t.detach().cpu().clone() for _, t in state.items()]\n",
    "\n",
    "\n",
    "def list_to_state(template: Dict[str, torch.Tensor], arrs: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Rebuild a state_dict from a list of tensors using a template.\"\"\"\n",
    "    out: Dict[str, torch.Tensor] = {}\n",
    "    for (k, v), a in zip(template.items(), arrs):\n",
    "        out[k] = a.to(v.device).type_as(v)\n",
    "    return out\n",
    "\n",
    "print(\"âœ… Model utilities defined\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Data Loading and Partitioning ==========\n",
    "\n",
    "class DataDistributor:\n",
    "    \"\"\"Data distributor for federated learning with Dirichlet partitioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name: str, data_dir: str = \"./data\"):\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        self.data_dir = data_dir\n",
    "        self.train_dataset, self.test_dataset, self.num_classes = self._load_dataset()\n",
    "        self.partitions = None\n",
    "\n",
    "    def _load_dataset(self) -> Tuple[Any, Any, int]:\n",
    "        \"\"\"Load CIFAR-10 dataset with augmentation.\"\"\"\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            # Strong data pipeline: augmentation for train, normalization for test\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.4914, 0.4822, 0.4465),\n",
    "                    std=(0.2470, 0.2435, 0.2616),\n",
    "                ),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.4914, 0.4822, 0.4465),\n",
    "                    std=(0.2470, 0.2435, 0.2616),\n",
    "                ),\n",
    "            ])\n",
    "            train = datasets.CIFAR10(self.data_dir, train=True, download=True, transform=transform_train)\n",
    "            test = datasets.CIFAR10(self.data_dir, train=False, download=True, transform=transform_test)\n",
    "            num_classes = 10\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset '{self.dataset_name}' not supported. Use 'cifar10'.\")\n",
    "        return train, test, num_classes\n",
    "\n",
    "    def distribute_data(self, num_clients: int, alpha: float = 0.5, seed: int = 42):\n",
    "        \"\"\"Partition data using Dirichlet distribution.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        targets = np.array(self.train_dataset.targets)\n",
    "        self.partitions = {i: [] for i in range(num_clients)}\n",
    "\n",
    "        for cls in range(self.num_classes):\n",
    "            idxs = np.where(targets == cls)[0]\n",
    "            np.random.shuffle(idxs)\n",
    "            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n",
    "            proportions = np.array([p * len(idxs) for p in proportions]).astype(int)\n",
    "            \n",
    "            # Handle leftover samples from integer rounding\n",
    "            total_assigned = proportions.sum()\n",
    "            leftover = len(idxs) - total_assigned\n",
    "            if leftover > 0:\n",
    "                # Distribute leftovers to random clients\n",
    "                recipients = np.random.choice(num_clients, size=leftover, replace=True)\n",
    "                for r in recipients:\n",
    "                    proportions[r] += 1\n",
    "\n",
    "            start = 0\n",
    "            for client_id, size in enumerate(proportions):\n",
    "                self.partitions[client_id].extend(idxs[start:start + size])\n",
    "                start += size\n",
    "\n",
    "        for cid in self.partitions:\n",
    "            np.random.shuffle(self.partitions[cid])\n",
    "\n",
    "    def get_client_data(self, client_id: int) -> Subset:\n",
    "        \"\"\"Get data subset for a specific client.\"\"\"\n",
    "        if self.partitions is None:\n",
    "            raise ValueError(\"Data not distributed yet. Call distribute_data() first.\")\n",
    "        indices = self.partitions[client_id]\n",
    "        return Subset(self.train_dataset, indices)\n",
    "\n",
    "print(\"âœ… DataDistributor class defined\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TrustWeight Client Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning-free async federated client for CIFAR-10\n",
    "import time\n",
    "import random\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "# build_resnet18 already defined\n",
    "# get_device already defined\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _build_transform() -> transforms.Compose:\n",
    "    \"\"\"Build CIFAR-10 training transform (matches DataDistributor._load_dataset).\"\"\"\n",
    "    # CIFAR-10 standard augmentation to reduce overfitting\n",
    "    return transforms.Compose(\n",
    "        [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def _make_dataloader(\n",
    "    data_dir: str,\n",
    "    indices: Sequence[int],\n",
    "    batch_size: int,\n",
    ") -> DataLoader:\n",
    "    dataset = datasets.CIFAR10(\n",
    "        root=data_dir,\n",
    "        train=True,\n",
    "        download=False,\n",
    "        transform=_build_transform(),\n",
    "    )\n",
    "    subset = Subset(dataset, indices)\n",
    "\n",
    "    # Safety: in case partitioning ever returns an empty list for a client\n",
    "    if len(subset) == 0:\n",
    "        return DataLoader(subset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    # num_workers=0 to avoid multiprocessing issues on macOS / Python 3.13\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "class AsyncClient:\n",
    "    \"\"\"Asynchronous client performing local training on its partition.\n",
    "\n",
    "    The client fetches the latest global model from the server, trains for a few\n",
    "    local epochs, and submits the updated parameters plus basic telemetry.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: int,\n",
    "        indices: Sequence[int],\n",
    "        cfg: dict,\n",
    "    ) -> None:\n",
    "        self.cid = cid\n",
    "        self.cfg = cfg\n",
    "        self.device = get_device()\n",
    "        self.loader = _make_dataloader(\n",
    "            cfg[\"data\"][\"data_dir\"],\n",
    "            indices,\n",
    "            cfg[\"clients\"][\"batch_size\"],\n",
    "        )\n",
    "        self.num_classes = cfg[\"data\"][\"num_classes\"]\n",
    "        self.lr = cfg[\"clients\"][\"lr\"]\n",
    "        self.weight_decay = cfg[\"clients\"][\"weight_decay\"]\n",
    "        self.grad_clip = cfg[\"clients\"][\"grad_clip\"]\n",
    "\n",
    "        # --- straggler behaviour ---\n",
    "        num_clients = cfg[\"clients\"][\"total\"]\n",
    "        slow_fraction = cfg[\"clients\"].get(\"struggle_percent\", 0) / 100.0\n",
    "        num_slow = int(round(num_clients * slow_fraction))\n",
    "        self.is_slow = cid < num_slow  # deterministic but good enough\n",
    "        \n",
    "        # Reduce epochs for slow clients (straggler robustness)\n",
    "        base_epochs = cfg[\"clients\"][\"local_epochs\"]\n",
    "        if self.is_slow:\n",
    "            self.local_epochs = max(1, base_epochs // 2)  # half epochs for slow clients\n",
    "        else:\n",
    "            self.local_epochs = base_epochs\n",
    "\n",
    "        self.delay_slow_range = tuple(cfg[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0]))\n",
    "        self.delay_fast_range = tuple(cfg[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2]))\n",
    "        self.jitter_per_round = float(cfg[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
    "        self.client_delay = float(cfg.get(\"server_runtime\", {}).get(\"client_delay\", 0.0))\n",
    "\n",
    "    # ------------------------------------------------------------------ utils\n",
    "\n",
    "    def _sample_delay(self) -> float:\n",
    "        if self.is_slow:\n",
    "            base = random.uniform(*self.delay_slow_range)\n",
    "        else:\n",
    "            base = random.uniform(*self.delay_fast_range)\n",
    "        jitter = random.uniform(-self.jitter_per_round, self.jitter_per_round)\n",
    "        return max(0.0, base + jitter + self.client_delay)\n",
    "\n",
    "    def _build_model(self) -> nn.Module:\n",
    "        model = build_resnet18(num_classes=self.num_classes)\n",
    "        return model.to(self.device)\n",
    "\n",
    "    # ---------------------------------------------------------------- training\n",
    "\n",
    "    def _evaluate_on_loader(self, model: nn.Module) -> Tuple[float, float]:\n",
    "        \"\"\"Return (loss, accuracy) on the client's local data.\"\"\"\n",
    "        model.eval()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                total_correct += (preds == yb).sum().item()\n",
    "                total_examples += xb.size(0)\n",
    "        if total_examples == 0:\n",
    "            return 0.0, 0.0\n",
    "        return total_loss / total_examples, total_correct / total_examples\n",
    "\n",
    "    def _train_local(self, model: nn.Module) -> Tuple[float, float]:\n",
    "        \"\"\"Train for `local_epochs` and return (loss_after, acc_after).\"\"\"\n",
    "        model.train()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # Use momentum from config (defaults to 0.0 if not specified)\n",
    "        client_momentum = self.cfg.get(\"clients\", {}).get(\"momentum\", 0.0)\n",
    "        optim = torch.optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=self.lr,\n",
    "            momentum=client_momentum,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "        for _ in range(self.local_epochs):\n",
    "            for xb, yb in self.loader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                if self.grad_clip > 0:\n",
    "                    clip_grad_norm_(model.parameters(), self.grad_clip)\n",
    "                optim.step()\n",
    "\n",
    "        # reuse evaluation code for final metrics\n",
    "        return self._evaluate_on_loader(model)\n",
    "\n",
    "    # ------------------------------------------------------------- main loop\n",
    "\n",
    "    def run_once(self, server) -> bool:\n",
    "        \"\"\"Perform a single async round with the server.\n",
    "\n",
    "        Returns False when the server indicates global stopping, True otherwise.\n",
    "        \"\"\"\n",
    "        # Simulated network / computation delay heterogeneity\n",
    "        delay = self._sample_delay()\n",
    "        if delay > 0:\n",
    "            time.sleep(delay)\n",
    "\n",
    "        if server.should_stop():\n",
    "            return False\n",
    "\n",
    "        # Get the latest global model snapshot\n",
    "        version, global_state = server.get_global_model()\n",
    "        model = self._build_model()\n",
    "        model.load_state_dict(global_state)\n",
    "\n",
    "        # Evaluate before local training to compute loss drop Î”LÌƒ_i\n",
    "        loss_before, _ = self._evaluate_on_loader(model)\n",
    "\n",
    "        start_time = time.time()\n",
    "        loss_after, train_acc = self._train_local(model)\n",
    "        train_time_s = time.time() - start_time\n",
    "\n",
    "        # Local \"test\" is just another pass over the client's data\n",
    "        test_loss, test_acc = self._evaluate_on_loader(model)\n",
    "\n",
    "        # Move params to CPU tensors so they are cheap to share with server\n",
    "        # Build an OrderedDict for ``new_params`` using the model's own\n",
    "        # parameter ordering.  Although Python's plain dicts preserve\n",
    "        # insertion order, explicitly constructing an ``OrderedDict`` makes\n",
    "        # the intent clear and avoids any surprises if the language\n",
    "        # specification changes.  The server relies on matching key\n",
    "        # ordering to correctly flatten parameter tensors.\n",
    "        from collections import OrderedDict\n",
    "        new_params = OrderedDict()\n",
    "        for k, v in model.state_dict().items():\n",
    "            new_params[k] = v.detach().cpu().clone()\n",
    "        num_examples = len(self.loader.dataset)\n",
    "\n",
    "        delta_loss = loss_before - loss_after  # Î”LÌƒ_i\n",
    "\n",
    "        # All local metrics are computed here and passed to the server\n",
    "        server.submit_update(\n",
    "            client_id=self.cid,\n",
    "            base_version=version,\n",
    "            new_params=new_params,\n",
    "            num_samples=num_examples,\n",
    "            train_time_s=train_time_s,\n",
    "            delta_loss=delta_loss,\n",
    "            loss_before=loss_before,\n",
    "            loss_after=loss_after,\n",
    "            train_acc=train_acc,\n",
    "            test_loss=test_loss,\n",
    "            test_acc=test_acc,\n",
    "        )\n",
    "        return not server.should_stop()\n",
    "\n",
    "\n",
    "print(\"âœ… TrustWeight client implementation defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TrustWeight Strategy Implementation ==========\n",
    "# Adapted from TrustWeight/strategy.py for notebook use\n",
    "\n",
    "# Trust-weighted asynchronous aggregation strategy implementing the PDF math\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrustWeightedConfig:\n",
    "    eta: float = 1.0              # server learning rate Î·\n",
    "    eps: float = 1e-8             # numerical stability Îµ\n",
    "    freshness_alpha: float = 0.1  # Î± in s(Ï„) = exp(-Î± Ï„)\n",
    "    beta1: float = 0.0            # Guard term coefficient on staleness\n",
    "    beta2: float = 0.0            # Guard term coefficient on ||u||\n",
    "    momentum_gamma: float = 0.9   # update factor for m_t\n",
    "    theta: Tuple[float, float, float] = (0.0, 0.0, 0.0)  # quality weights\n",
    "\n",
    "\n",
    "class TrustWeightedAsyncStrategy:\n",
    "    \"\"\"Implements the aggregation rule described in the DML solution PDF.\n",
    "\n",
    "    Core formula:\n",
    "\n",
    "        w_{t+1} = w_t + Î· * Î£_i Weight_i *\n",
    "            [ Proj_m_t(u_i) + Guard_i * (u_i - Proj_m_t(u_i)) ]\n",
    "\n",
    "    with:\n",
    "\n",
    "        Proj_m_t(u_i) = <u_i, m_t> / (||m_t||^2 + eps) * m_t\n",
    "        Guard_i = 1 / (1 + Î²1 * Ï„_i + Î²2 * ||u_i||)\n",
    "        Weight_i âˆ s(Ï„_i) * exp(Î¸áµ€ [Î”LÌƒ_i, ||u_i||, cos(u_i, m_t)]) * (n_i / Î£_j n_j)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, cfg: TrustWeightedConfig | None = None) -> None:\n",
    "        self.dim = int(dim)\n",
    "        self.cfg = cfg or TrustWeightedConfig()\n",
    "        self.m = torch.zeros(self.dim, dtype=torch.float32)  # m_t, server momentum\n",
    "        self.step: int = 0\n",
    "\n",
    "        self.theta = torch.tensor(self.cfg.theta, dtype=torch.float32)\n",
    "\n",
    "    # ------------------------------------------------------------------ helpers\n",
    "\n",
    "    def _proj_m(self, u: torch.Tensor) -> torch.Tensor:\n",
    "        # Proj_m(u) = <u, m> / (||m||^2 + eps) * m\n",
    "        num = torch.dot(u, self.m)\n",
    "        denom = torch.dot(self.m, self.m) + self.cfg.eps\n",
    "        coef = num / denom\n",
    "        return coef * self.m\n",
    "\n",
    "    def _guard(self, tau: torch.Tensor, norm_u: torch.Tensor) -> torch.Tensor:\n",
    "        # Guard_i = 1 / (1 + Î²1 Ï„_i + Î²2 ||u_i||)\n",
    "        return 1.0 / (1.0 + self.cfg.beta1 * tau + self.cfg.beta2 * norm_u)\n",
    "\n",
    "    def _freshness(self, tau: torch.Tensor) -> torch.Tensor:\n",
    "        # s(Ï„) = exp(-Î± Ï„)\n",
    "        return torch.exp(-self.cfg.freshness_alpha * tau)\n",
    "\n",
    "    # ---------------------------------------------------------------- aggregate\n",
    "\n",
    "    def aggregate(\n",
    "        self,\n",
    "        w_t: torch.Tensor,\n",
    "        updates: List[Dict[str, torch.Tensor]],\n",
    "        use_sample_weighing: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "        \"\"\"Aggregate a buffer of updates.\n",
    "\n",
    "        Args:\n",
    "            w_t: Flattened current global model.\n",
    "            updates: List of dicts, each containing:\n",
    "                {\n",
    "                    \"u\": update vector (1D tensor),\n",
    "                    \"tau\": scalar tensor Ï„_i,\n",
    "                    \"num_samples\": scalar tensor n_i,\n",
    "                    \"delta_loss\": scalar tensor Î”LÌƒ_i,\n",
    "                }\n",
    "\n",
    "        Returns:\n",
    "            new_w: updated global model vector.\n",
    "            metrics: small dict with aggregation statistics.\n",
    "        \"\"\"\n",
    "        if not updates:\n",
    "            return w_t, {\"avg_tau\": 0.0, \"buffer_size\": 0.0}\n",
    "\n",
    "        self.step += 1\n",
    "        device = w_t.device\n",
    "        self.m = self.m.to(device)\n",
    "\n",
    "        # Collect basic statistics\n",
    "        taus = torch.stack([u[\"tau\"].to(device) for u in updates])  # [B]\n",
    "        ns = torch.stack([u[\"num_samples\"].to(device) for u in updates])  # [B]\n",
    "        delta_losses = torch.stack([u[\"delta_loss\"].to(device) for u in updates])  # [B]\n",
    "        total_n = ns.sum().clamp_min(1.0)\n",
    "\n",
    "        # Precompute norms, projections, sideways components, cosines\n",
    "        proj_list: List[torch.Tensor] = []\n",
    "        side_list: List[torch.Tensor] = []\n",
    "        norm_u_list: List[torch.Tensor] = []\n",
    "        cos_list: List[torch.Tensor] = []\n",
    "\n",
    "        for u_rec in updates:\n",
    "            u = u_rec[\"u\"].to(device)\n",
    "            norm_u = torch.norm(u).clamp_min(self.cfg.eps)\n",
    "            norm_u_list.append(norm_u)\n",
    "\n",
    "            proj = self._proj_m(u)\n",
    "            side = u - proj\n",
    "            proj_list.append(proj)\n",
    "            side_list.append(side)\n",
    "\n",
    "            # cos(u, m) = <u, m> / (||u|| ||m|| + eps)\n",
    "            norm_m = torch.norm(self.m)\n",
    "            if norm_m.item() > 0.0:\n",
    "                cos_val = torch.dot(u, self.m) / (norm_u * norm_m + self.cfg.eps)\n",
    "            else:\n",
    "                cos_val = torch.tensor(0.0, device=device)\n",
    "            cos_list.append(cos_val)\n",
    "\n",
    "        norm_u_tensor = torch.stack(norm_u_list)  # [B]\n",
    "        cos_tensor = torch.stack(cos_list)  # [B]\n",
    "\n",
    "        # Guard factors per update\n",
    "        guards = self._guard(taus, norm_u_tensor)  # [B]\n",
    "\n",
    "        # Freshness\n",
    "        freshness = self._freshness(taus)  # [B]\n",
    "\n",
    "        # Add scaled staleness as a negative feature (straggler robustness)\n",
    "        lambda_stale = 0.05\n",
    "        effective_delta = delta_losses - lambda_stale * taus\n",
    "        \n",
    "        # Quality term: exp(Î¸áµ€ [Î”LÌƒ_i, ||u_i||, cos(u_i, m_t)])\n",
    "        # Î¸: (Î”L weight, -â€–uâ€– penalty, cosine alignment)\n",
    "        feats = torch.stack(\n",
    "            [effective_delta, norm_u_tensor, cos_tensor],\n",
    "            dim=1,\n",
    "        )  # [B, 3]\n",
    "        quality_logits = feats @ self.theta.to(device)\n",
    "        \n",
    "        # Clamp logits to avoid exploding weights (straggler robustness)\n",
    "        quality_logits = torch.clamp(quality_logits, min=-3.0, max=3.0)\n",
    "        quality = torch.exp(quality_logits)\n",
    "\n",
    "        # Data share term: n_i / Î£_j n_j (if use_sample_weighing) or uniform (1/|B|)\n",
    "        if use_sample_weighing:\n",
    "            data_share = ns / total_n  # [B]\n",
    "        else:\n",
    "            data_share = torch.ones_like(ns) / len(updates)  # [B]\n",
    "\n",
    "        # Unnormalized weights, then normalization over buffer\n",
    "        raw_weights = freshness * quality * data_share  # [B]\n",
    "        sum_raw = raw_weights.sum()\n",
    "        if sum_raw.item() <= 0.0:\n",
    "            weights = torch.full_like(raw_weights, 1.0 / len(updates))\n",
    "        else:\n",
    "            weights = raw_weights / sum_raw\n",
    "\n",
    "        # Combine projection and guarded sideways components\n",
    "        agg_update = torch.zeros_like(w_t)\n",
    "        for i in range(len(updates)):\n",
    "            comp = proj_list[i] + guards[i] * side_list[i]\n",
    "            agg_update = agg_update + weights[i] * comp\n",
    "\n",
    "        # Final aggregation step:\n",
    "        # w_{t+1} = w_t + Î· * Î£_i Weight_i * [Proj_m(u_i) + Guard_i (u_i - Proj_m(u_i))]\n",
    "        new_w = w_t + self.cfg.eta * agg_update\n",
    "\n",
    "        # Update momentum m_t as a running average of aggregated updates\n",
    "        self.m = (1.0 - self.cfg.momentum_gamma) * self.m + self.cfg.momentum_gamma * agg_update\n",
    "\n",
    "        metrics = {\n",
    "            \"avg_tau\": float(taus.mean().item()),\n",
    "            \"avg_norm_u\": float(norm_u_tensor.mean().item()),\n",
    "            \"avg_delta_loss\": float(delta_losses.mean().item()),\n",
    "            \"buffer_size\": float(len(updates)),\n",
    "        }\n",
    "        return new_w, metrics\n",
    "\n",
    "\n",
    "print(\"âœ… TrustWeight strategy implementation defined\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TrustWeight Server Implementation ==========\n",
    "# Adapted from TrustWeight/server.py for notebook use\n",
    "\n",
    "from collections import OrderedDict as ODType\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def _testloader(root: str, batch_size: int = 256) -> DataLoader:\n",
    "    \"\"\"Create test dataloader.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=transform)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "def _flatten_state(state: ODType[str, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"Flatten a state_dict into a 1D tensor.\"\"\"\n",
    "    return torch.cat([p.reshape(-1) for p in state.values()])\n",
    "\n",
    "def _flatten_state_by_template(\n",
    "    state: Dict[str, torch.Tensor], template: ODType[str, torch.Tensor]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Flatten state according to template key ordering.\"\"\"\n",
    "    return torch.cat([state[k].reshape(-1) for k in template.keys()])\n",
    "\n",
    "def _vector_to_state(\n",
    "    vec: torch.Tensor, template: ODType[str, torch.Tensor]\n",
    ") -> ODType[str, torch.Tensor]:\n",
    "    \"\"\"Convert flattened vector back to state_dict.\"\"\"\n",
    "    new_state: ODType[str, torch.Tensor] = type(template)()\n",
    "    offset = 0\n",
    "    for k, t in template.items():\n",
    "        numel = t.numel()\n",
    "        new_state[k] = vec[offset : offset + numel].view_as(t).clone()\n",
    "        offset += numel\n",
    "    assert offset == vec.numel()\n",
    "    return new_state\n",
    "\n",
    "@dataclass\n",
    "class ClientUpdateState:\n",
    "    \"\"\"State for a client update.\"\"\"\n",
    "    client_id: int\n",
    "    base_version: int\n",
    "    new_params: ODType[str, torch.Tensor]\n",
    "    num_samples: int\n",
    "    train_time_s: float\n",
    "    delta_loss: float\n",
    "    loss_before: float\n",
    "    loss_after: float\n",
    "    train_acc: float\n",
    "    test_loss: float\n",
    "    test_acc: float\n",
    "    arrival_ts: float\n",
    "\n",
    "class AsyncServer:\n",
    "    \"\"\"Central server maintaining global model and asynchronous buffer.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        global_model: torch.nn.Module,\n",
    "buffer_size: int = 5,\n",
    "        buffer_timeout_s: float = 5.0,\n",
    "        use_sample_weighing: bool = True,\n",
    "        target_accuracy: float = 0.8,\n",
    "        max_rounds: Optional[int] = None,\n",
    "data_dir: str = \"./data\",\n",
    "logs_dir: str = \"./logs/TrustWeight\",\n",
    "        global_log_csv: Optional[str] = None,\n",
    "        client_participation_csv: Optional[str] = None,\n",
    "resume: bool = True,\n",
    "        device: Optional[torch.device] = None,\n",
    "        freshness_alpha: float = 0.1,\n",
    "        beta1: float = 0.0,\n",
    "        beta2: float = 0.0,\n",
    "        eta: float = 1.0,\n",
    "        theta: Optional[Tuple[float, float, float]] = None,\n",
    "    ):\n",
    "        self.device = device or get_device()\n",
    "        \n",
    "        # Extract num_classes from global_model\n",
    "        # Try to get it from model attribute, or infer from fc layer\n",
    "        if hasattr(global_model, 'num_classes'):\n",
    "            self.num_classes = global_model.num_classes\n",
    "        elif hasattr(global_model, 'fc'):\n",
    "            self.num_classes = global_model.fc.out_features\n",
    "        else:\n",
    "            # Default for CIFAR-10\n",
    "            self.num_classes = 10\n",
    "        \n",
    "        # data / evaluation\n",
    "        self.testloader = _testloader(data_dir, batch_size=256)\n",
    "        \n",
    "        # global model and version history\n",
    "        self._template_state: ODType[str, torch.Tensor] = ODType(\n",
    "            (k, v.detach().cpu().clone()) for k, v in global_model.state_dict().items()\n",
    "        )\n",
    "        self._global_state: ODType[str, torch.Tensor] = ODType(\n",
    "            (k, v.clone()) for k, v in self._template_state.items()\n",
    "        )\n",
    "        \n",
    "        self._model_versions: List[ODType[str, torch.Tensor]] = [\n",
    "            ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "        ]\n",
    "        self._version: int = 0\n",
    "        \n",
    "        # strategy encapsulating all math - use eta, theta, freshness_alpha, beta1, beta2 from config\n",
    "        dim = _flatten_state(self._global_state).numel()\n",
    "        # Î¸: (Î”L weight, -â€–uâ€– penalty, cosine alignment)\n",
    "        theta_cfg = theta if theta is not None else (1.0, -0.1, 0.2)\n",
    "        strategy_cfg = TrustWeightedConfig(\n",
    "            eta=eta,\n",
    "            theta=theta_cfg,\n",
    "            freshness_alpha=freshness_alpha,\n",
    "            beta1=beta1,\n",
    "            beta2=beta2,\n",
    "        )\n",
    "        self.strategy = TrustWeightedAsyncStrategy(dim=dim, cfg=strategy_cfg)\n",
    "        self.eta = eta\n",
    "        self.theta = theta_cfg\n",
    "        self.use_sample_weighing = use_sample_weighing\n",
    "        \n",
    "        # async buffer\n",
    "        self.buffer: List[ClientUpdateState] = []\n",
    "        self.buffer_size: int = buffer_size\n",
    "        self.buffer_timeout_s: float = buffer_timeout_s\n",
    "        self._last_flush_ts: float = time.time()\n",
    "        \n",
    "        # logging / control\n",
    "        self.global_log_path = Path(global_log_csv) if global_log_csv else Path(logs_dir) / \"TrustWeight.csv\"\n",
    "        self.global_log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self._init_global_log()\n",
    "        \n",
    "        self.client_log_path = Path(client_participation_csv) if client_participation_csv else Path(logs_dir) / \"TrustWeightClientParticipation.csv\"\n",
    "        self.client_log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self._init_client_log()\n",
    "        \n",
    "        self.target_accuracy: float = target_accuracy\n",
    "        self.max_rounds: int = max_rounds if max_rounds is not None else 1000\n",
    "        self.update_clip_norm: float = 10.0  # Default clipping norm\n",
    "        \n",
    "        self._num_aggregations: int = 0\n",
    "        self._stop: bool = False\n",
    "        self._stop_reason: str = \"\"\n",
    "        self._lock = threading.Lock()\n",
    "        self._agg_lock = threading.Lock()\n",
    "        self._start_ts: float = time.time()\n",
    "        \n",
    "    \n",
    "    def _init_global_log(self) -> None:\n",
    "        \"\"\"Initialize the global training CSV.\"\"\"\n",
    "        if self.global_log_path.exists():\n",
    "            return\n",
    "        with self.global_log_path.open(\"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"total_agg\", \"avg_train_loss\", \"avg_train_acc\",\n",
    "                \"test_loss\", \"test_acc\", \"time\",\n",
    "            ])\n",
    "    \n",
    "    def _append_global_log(\n",
    "        self,\n",
    "        total_agg: int,\n",
    "        avg_train_loss: float,\n",
    "        avg_train_acc: float,\n",
    "        test_loss: float,\n",
    "        test_acc: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Append a single aggregation row to the global CSV.\"\"\"\n",
    "        ts = time.time() - self._start_ts\n",
    "        with self.global_log_path.open(\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                int(total_agg),\n",
    "                float(avg_train_loss),\n",
    "                float(avg_train_acc),\n",
    "                float(test_loss),\n",
    "                float(test_acc),\n",
    "                ts,\n",
    "            ])\n",
    "    \n",
    "    def _init_client_log(self) -> None:\n",
    "        \"\"\"Initialize the client participation CSV.\"\"\"\n",
    "        if self.client_log_path.exists():\n",
    "            return\n",
    "        with self.client_log_path.open(\"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"client_id\", \"local_train_loss\", \"local_train_acc\",\n",
    "                \"local_test_loss\", \"local_test_acc\", \"total_agg\", \"staleness\",\n",
    "            ])\n",
    "    \n",
    "    def _append_client_participation_log(\n",
    "        self,\n",
    "        total_agg: int,\n",
    "        updates: List[ClientUpdateState],\n",
    "        staleness_list: List[float],\n",
    "    ) -> None:\n",
    "        \"\"\"Append one row per client update.\"\"\"\n",
    "        with self.client_log_path.open(\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for u, tau_i in zip(updates, staleness_list):\n",
    "                writer.writerow([\n",
    "                    int(u.client_id),\n",
    "                    float(u.loss_after),\n",
    "                    float(u.train_acc),\n",
    "                    float(u.test_loss),\n",
    "                    float(u.test_acc),\n",
    "                    int(total_agg),\n",
    "                    float(tau_i),\n",
    "                ])\n",
    "    \n",
    "    def should_stop(self) -> bool:\n",
    "        with self._lock:\n",
    "            return self._stop\n",
    "    \n",
    "    def mark_stop(self, reason: str = \"\") -> None:\n",
    "        with self._lock:\n",
    "            if not self._stop:\n",
    "                self._stop = True\n",
    "                if reason:\n",
    "                    self._stop_reason = reason\n",
    "                print(f\"[Server] Stopping: {self._stop_reason}\")\n",
    "    \n",
    "    def get_global_model(self) -> Tuple[int, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Return (version, state_dict) of the current global model.\"\"\"\n",
    "        with self._lock:\n",
    "            version = self._version\n",
    "            state = ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "        return version, state\n",
    "    \n",
    "    def _make_model_from_state(self, state: Dict[str, torch.Tensor], num_classes: int) -> torch.nn.Module:\n",
    "        model = build_resnet18(num_classes=num_classes)\n",
    "        model.load_state_dict(state)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _evaluate_global(self, num_classes: int) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate the current global model on the test set.\"\"\"\n",
    "        model = self._make_model_from_state(self._global_state, num_classes)\n",
    "        model.eval()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.testloader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                total_correct += (preds == yb).sum().item()\n",
    "                total_examples += xb.size(0)\n",
    "        if total_examples == 0:\n",
    "            return 0.0, 0.0\n",
    "        return total_loss / total_examples, total_correct / total_examples\n",
    "    \n",
    "    def _flush_buffer_if_needed(self) -> None:\n",
    "        now = time.time()\n",
    "        should_flush = False\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            should_flush = True\n",
    "        elif (now - self._last_flush_ts) >= self.buffer_timeout_s and self.buffer:\n",
    "            should_flush = True\n",
    "        \n",
    "        if not should_flush:\n",
    "            return\n",
    "        \n",
    "        # copy buffer locally under lock then release for heavy work\n",
    "        with self._lock:\n",
    "            buffer_copy = list(self.buffer)\n",
    "            self.buffer.clear()\n",
    "            self._last_flush_ts = now\n",
    "        \n",
    "        # Serialize aggregations to avoid version races\n",
    "        with self._agg_lock:\n",
    "            self._aggregate(buffer_copy)\n",
    "    \n",
    "    def _aggregate(self, updates: List[ClientUpdateState]) -> None:\n",
    "        \"\"\"Aggregate a batch of client updates and log to CSVs.\"\"\"\n",
    "        if not updates:\n",
    "            return\n",
    "        \n",
    "        # Snapshot of current global parameters and version history\n",
    "        with self._lock:\n",
    "            global_vec = _flatten_state_by_template(self._global_state, self._template_state)\n",
    "            version_now = self._version\n",
    "            model_versions = list(self._model_versions)\n",
    "        \n",
    "        # Construct per-update vectors and metadata for the strategy\n",
    "        update_vectors: List[Dict[str, torch.Tensor]] = []\n",
    "        staleness_list: List[float] = []\n",
    "        valid_updates: List[ClientUpdateState] = []\n",
    "        \n",
    "        for u in updates:\n",
    "            base_state = model_versions[u.base_version]\n",
    "            base_vec = _flatten_state_by_template(base_state, self._template_state)\n",
    "            new_vec = _flatten_state_by_template(u.new_params, self._template_state)\n",
    "            ui = new_vec - base_vec\n",
    "            \n",
    "            # Skip bad updates\n",
    "            if not torch.isfinite(ui).all():\n",
    "                print(f\"[Server] Dropping client {u.client_id} update due to NaN/Inf values\")\n",
    "                continue\n",
    "            if self.update_clip_norm > 0:\n",
    "                norm = torch.norm(ui)\n",
    "                if torch.isfinite(norm) and norm.item() > self.update_clip_norm:\n",
    "                    ui = ui * (self.update_clip_norm / (norm + 1e-12))\n",
    "            \n",
    "            # Ï„_i = version-based staleness (how many global updates since client started)\n",
    "            tau_i = float(max(0, version_now - u.base_version))\n",
    "            \n",
    "            # Drop extremely stale updates (hard threshold for straggler robustness)\n",
    "            max_tau = 10.0\n",
    "            if tau_i > max_tau:\n",
    "                print(f\"[Server] Dropping client {u.client_id} update (tau={tau_i:.1f} > {max_tau})\")\n",
    "                continue\n",
    "            \n",
    "            staleness_list.append(tau_i)\n",
    "            \n",
    "            delta_loss = float(u.delta_loss)\n",
    "            update_vectors.append({\n",
    "                \"u\": ui,\n",
    "                \"tau\": torch.tensor(tau_i, dtype=torch.float32),\n",
    "                \"num_samples\": torch.tensor(float(u.num_samples), dtype=torch.float32),\n",
    "                \"delta_loss\": torch.tensor(delta_loss, dtype=torch.float32),\n",
    "            })\n",
    "            valid_updates.append(u)\n",
    "        \n",
    "        if not update_vectors:\n",
    "            print(\"[Server] Buffer flush skipped: no valid updates after filtering.\")\n",
    "            return\n",
    "        \n",
    "        # Run the trust-weighted aggregation strategy\n",
    "        new_global_vec, agg_metrics = self.strategy.aggregate(\n",
    "            global_vec, update_vectors, use_sample_weighing=self.use_sample_weighing\n",
    "        )\n",
    "        \n",
    "        # Map back into parameter state_dict form\n",
    "        new_state = _vector_to_state(new_global_vec, self._template_state)\n",
    "        \n",
    "        # Compute average local train metrics\n",
    "        avg_train_loss = sum(u.loss_after for u in valid_updates) / len(valid_updates)\n",
    "        avg_train_acc = sum(u.train_acc for u in valid_updates) / len(valid_updates)\n",
    "        \n",
    "        # Commit the new global model\n",
    "        with self._lock:\n",
    "            self._global_state = ODType((k, v.clone()) for k, v in new_state.items())\n",
    "            self._model_versions.append(\n",
    "                ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "            )\n",
    "            self._version = len(self._model_versions) - 1\n",
    "            self._num_aggregations += 1\n",
    "            total_agg = self._num_aggregations\n",
    "        \n",
    "        # Evaluate updated global model\n",
    "        test_loss, test_acc = self._evaluate_global(self.num_classes)\n",
    "        \n",
    "        # Log metrics\n",
    "        self._append_global_log(\n",
    "            total_agg=total_agg,\n",
    "            avg_train_loss=avg_train_loss,\n",
    "            avg_train_acc=avg_train_acc,\n",
    "            test_loss=test_loss,\n",
    "            test_acc=test_acc,\n",
    "        )\n",
    "        self._append_client_participation_log(\n",
    "            total_agg=total_agg,\n",
    "            updates=valid_updates,\n",
    "            staleness_list=staleness_list,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"[Server] Aggregated {len(valid_updates)} updates -> agg={total_agg} \"\n",
    "            f\"(avg_tau={agg_metrics.get('avg_tau', 0.0):.3f}, \"\n",
    "            f\"test_loss={test_loss:.4f}, test_acc={test_acc:.4f})\"\n",
    "        )\n",
    "        \n",
    "        # Stopping conditions\n",
    "        if test_acc >= self.target_accuracy:\n",
    "            self.mark_stop(f\"target accuracy {test_acc:.4f} reached\")\n",
    "        if total_agg >= self.max_rounds:\n",
    "            self.mark_stop(\"max aggregation rounds reached\")\n",
    "    \n",
    "    def submit_update(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        base_version: int,\n",
    "        new_params: Dict[str, torch.Tensor],\n",
    "        num_samples: int,\n",
    "        train_time_s: float,\n",
    "        delta_loss: float,\n",
    "        loss_before: float,\n",
    "        loss_after: float,\n",
    "        train_acc: float,\n",
    "        test_loss: float,\n",
    "        test_acc: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Entry point called by clients after local training.\"\"\"\n",
    "        cu = ClientUpdateState(\n",
    "            client_id=client_id,\n",
    "            base_version=base_version,\n",
    "            new_params=new_params,\n",
    "            num_samples=num_samples,\n",
    "            train_time_s=float(train_time_s),\n",
    "            delta_loss=float(delta_loss),\n",
    "            loss_before=float(loss_before),\n",
    "            loss_after=float(loss_after),\n",
    "            train_acc=float(train_acc),\n",
    "            test_loss=float(test_loss),\n",
    "            test_acc=float(test_acc),\n",
    "            arrival_ts=time.time(),\n",
    "        )\n",
    "        with self._lock:\n",
    "            self.buffer.append(cu)\n",
    "        self._flush_buffer_if_needed()\n",
    "    \n",
    "    def wait(self) -> None:\n",
    "        \"\"\"Block until training is finished.\"\"\"\n",
    "        try:\n",
    "            while not self.should_stop():\n",
    "                time.sleep(0.2)\n",
    "        finally:\n",
    "            self.mark_stop(self._stop_reason or \"training finished\")\n",
    "\n",
    "print(\"âœ… TrustWeight server implementation defined\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== All 6 Experiment Configurations ==========\n",
    "\n",
    "# Helper function to get paths (works for both Colab and local)\n",
    "def get_paths(exp_id: str):\n",
    "    \"\"\"Get paths for experiment, using Google Drive if in Colab.\"\"\"\n",
    "    if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "        base = BASE_OUTPUT_DIR\n",
    "    else:\n",
    "        base = Path(\".\")\n",
    "    \n",
    "    return {\n",
    "        \"data_dir\": str(DATA_DIR),  # Always local for faster access\n",
    "        \"checkpoints_dir\": str(base / \"checkpoints\" / \"TrustWeight\" / exp_id),\n",
    "        \"logs_dir\": str(base / \"logs\" / \"TrustWeight\" / exp_id),\n",
    "        \"results_dir\": str(base / \"results\" / \"TrustWeight\" / exp_id),\n",
    "    }\n",
    "\n",
    "experiments = {\n",
    "    \"Exp1\": {\n",
    "        \"name\": \"IID (alpha=1000), no stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),  # Use DATA_DIR variable\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 0,\n",
    "            \"delay_slow_range\": [0.0, 0.0],\n",
    "            \"delay_fast_range\": [0.0, 0.0],\n",
    "            \"jitter_per_round\": 0.0,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5,\n",
    "            \"theta\": [1.0, -0.1, 0.2],\n",
    "            \"freshness_alpha\": 0.1,\n",
    "            \"beta1\": 0.0,\n",
    "            \"beta2\": 0.0\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 20\n",
    "        },\n",
    "        \"partition_alpha\": 1000.0,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp1\")\n",
    "    },\n",
    "    \"Exp2\": {\n",
    "        \"name\": \"alpha=0.1, 10% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 10,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5,\n",
    "            \"theta\": [1.0, -0.1, 0.2],\n",
    "            \"freshness_alpha\": 0.1,\n",
    "            \"beta1\": 0.0,\n",
    "            \"beta2\": 0.0\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp2\")\n",
    "    },\n",
    "    \"Exp3\": {\n",
    "        \"name\": \"alpha=0.1, 20% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 20,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5,\n",
    "            \"theta\": [1.0, -0.1, 0.2],\n",
    "            \"freshness_alpha\": 0.1,\n",
    "            \"beta1\": 0.0,\n",
    "            \"beta2\": 0.0\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp3\")\n",
    "    },\n",
    "    \"Exp4\": {\n",
    "        \"name\": \"alpha=0.1, 30% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 30,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5,\n",
    "            \"theta\": [1.0, -0.1, 0.2],\n",
    "            \"freshness_alpha\": 0.1,\n",
    "            \"beta1\": 0.0,\n",
    "            \"beta2\": 0.0\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp4\")\n",
    "    },\n",
    "    \"Exp5\": {\n",
    "        \"name\": \"alpha=0.1, 40% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 40,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.4,\n",
    "            \"theta\": [0.8, -0.05, 0.1],\n",
    "            \"freshness_alpha\": 0.3,\n",
    "            \"beta1\": 0.15,\n",
    "            \"beta2\": 0.02\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp5\")\n",
    "    },\n",
    "    \"Exp6\": {\n",
    "        \"name\": \"alpha=0.1, 50% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 50,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.4,\n",
    "            \"theta\": [0.8, -0.05, 0.1],\n",
    "            \"freshness_alpha\": 0.3,\n",
    "            \"beta1\": 0.15,\n",
    "            \"beta2\": 0.02\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp6\")\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… All 6 experiment configurations loaded:\")\n",
    "for exp_id, exp_config in experiments.items():\n",
    "    print(f\"  {exp_id}: {exp_config['name']}\")\n",
    "    print(f\"    - Alpha: {exp_config['partition_alpha']}\")\n",
    "    print(f\"    - Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
    "    print(f\"    - Max rounds: {exp_config['train']['max_rounds']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Single Experiment (Helper Function)\n",
    "\n",
    "Use this function to run a single experiment by ID (Exp1-Exp6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(exp_id: str, experiments_dict: dict):\n",
    "    \"\"\"\n",
    "    Run a single TrustWeight experiment.\n",
    "    \n",
    "    Args:\n",
    "        exp_id: Experiment ID (e.g., \"Exp1\", \"Exp2\", ..., \"Exp6\")\n",
    "        experiments_dict: Dictionary containing all experiment configs\n",
    "    \n",
    "    Returns:\n",
    "        run_dir: Path to the run directory containing logs and results\n",
    "    \"\"\"\n",
    "    if exp_id not in experiments_dict:\n",
    "        print(f\"âŒ Error: Experiment {exp_id} not found!\")\n",
    "        return None\n",
    "    \n",
    "    config = experiments_dict[exp_id]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running {exp_id}: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Set random seed\n",
    "    seed = int(config.get(\"seed\", 42))\n",
    "    set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create timestamped run folder\n",
    "    run_dir = Path(config[\"io\"][\"logs_dir\"]) / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Write COMMIT.txt\n",
    "    commit_hash = \"notebook_run\"\n",
    "    csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n",
    "    with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n",
    "        f.write(f\"{commit_hash},{csv_header}\\n\")\n",
    "    \n",
    "    # Save config to run folder\n",
    "    with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    print(f\"âœ… Run folder: {run_dir}\")\n",
    "    \n",
    "    # Load and partition data\n",
    "    dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n",
    "    dd.distribute_data(\n",
    "        num_clients=int(config[\"clients\"][\"total\"]),\n",
    "        alpha=float(config[\"partition_alpha\"]),\n",
    "        seed=seed\n",
    "    )\n",
    "    print(f\"âœ… Data partitioned: {config['clients']['total']} clients, alpha={config['partition_alpha']}\")\n",
    "    \n",
    "    # Build global model\n",
    "    global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n",
    "    \n",
    "    # Initialize server\n",
    "    server = AsyncServer(\n",
    "        global_model=global_model,\n",
    "buffer_size=int(config[\"trustweight\"][\"buffer_size\"]),\n",
    "        buffer_timeout_s=float(config[\"trustweight\"][\"buffer_timeout_s\"]),\n",
    "        use_sample_weighing=bool(config[\"trustweight\"][\"use_sample_weighing\"]),\n",
    "        target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n",
    "        max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n",
    "data_dir=config[\"data\"][\"data_dir\"],\n",
    "logs_dir=str(run_dir),\n",
    "        global_log_csv=str(run_dir / \"TrustWeight.csv\"),\n",
    "        client_participation_csv=str(run_dir / \"TrustWeightClientParticipation.csv\"),\n",
    "resume=False,\n",
    "        device=get_device(),\n",
    "        eta=float(config[\"trustweight\"].get(\"eta\", 0.5)),\n",
    "        theta=tuple(config[\"trustweight\"].get(\"theta\", [1.0, -0.1, 0.2])),\n",
    "        freshness_alpha=float(config[\"trustweight\"].get(\"freshness_alpha\", 0.1)),\n",
    "        beta1=float(config[\"trustweight\"].get(\"beta1\", 0.0)),\n",
    "        beta2=float(config[\"trustweight\"].get(\"beta2\", 0.0)),\n",
    "    )\n",
    "    print(f\"âœ… Server initialized (device: {server.device})\")\n",
    "    \n",
    "    # Setup clients\n",
    "    n = int(config[\"clients\"][\"total\"])\n",
    "    clients: List[AsyncClient] = []\n",
    "    for cid in range(n):\n",
    "        indices = dd.partitions[cid] if cid in dd.partitions else []\n",
    "        clients.append(\n",
    "            AsyncClient(\n",
    "                cid=cid,\n",
    "                indices=indices,\n",
    "                cfg=config,\n",
    "            )\n",
    "        )\n",
    "    # Count slow/fast clients based on AsyncClient's internal flag\n",
    "    num_slow = sum(1 for c in clients if getattr(c, \"is_slow\", False))\n",
    "    num_fast = len(clients) - num_slow\n",
    "    print(f\"âœ… Created {len(clients)} clients ({num_slow} slow, {num_fast} fast)\")\n",
    "    \n",
    "    # Start experiment\n",
    "    sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n",
    "    \n",
    "    def client_loop(client: AsyncClient):\n",
    "        while True:\n",
    "            with sem:\n",
    "                cont = client.run_once(server)\n",
    "            if not cont:\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "    \n",
    "    threads = []\n",
    "    for cl in clients:\n",
    "        t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    \n",
    "    print(f\"âœ… Started {len(threads)} client threads\")\n",
    "    print(f\"ðŸš€ Experiment running... (max {config['train']['max_rounds']} rounds)\")\n",
    "    \n",
    "    # Wait for completion\n",
    "    server.wait()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    print(f\"\\nâœ… {exp_id} completed!\")\n",
    "    print(f\"ðŸ“ Results: {run_dir}\")\n",
    "    \n",
    "    return run_dir\n",
    "\n",
    "print(\"âœ… Helper function `run_single_experiment()` defined\")\n",
    "print(\"   Usage: run_dir = run_single_experiment('Exp1', experiments)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run All 6 Experiments Sequentially\n",
    "\n",
    "This cell runs all experiments one by one. Each experiment saves to its own folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all 6 experiments sequentially\n",
    "experiment_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ALL 6 EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "    exp_start = time.time()\n",
    "    try:\n",
    "        run_dir = run_single_experiment(exp_id, experiments)\n",
    "        if run_dir:\n",
    "            experiment_results[exp_id] = {\n",
    "                \"run_dir\": run_dir,\n",
    "                \"status\": \"completed\",\n",
    "                \"duration_min\": (time.time() - exp_start) / 60.0\n",
    "            }\n",
    "            print(f\"âœ… {exp_id} completed in {experiment_results[exp_id]['duration_min']:.2f} minutes\")\n",
    "        else:\n",
    "            experiment_results[exp_id] = {\"status\": \"failed\", \"duration_min\": (time.time() - exp_start) / 60.0}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {exp_id} failed: {str(e)}\")\n",
    "        experiment_results[exp_id] = {\"status\": \"error\", \"error\": str(e), \"duration_min\": (time.time() - exp_start) / 60.0}\n",
    "    \n",
    "    print(f\"\\n{'â”€'*70}\\n\")\n",
    "\n",
    "total_duration = (time.time() - total_start) / 60.0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time: {total_duration:.2f} minutes ({total_duration/60:.2f} hours)\")\n",
    "print(\"\\nResults summary:\")\n",
    "for exp_id, result in experiment_results.items():\n",
    "    if result.get(\"status\") == \"completed\":\n",
    "        print(f\"  {exp_id}: âœ… {result['duration_min']:.2f} min -> {result['run_dir']}\")\n",
    "    else:\n",
    "        print(f\"  {exp_id}: âŒ {result.get('status', 'unknown')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare All Experiments (After Running)\n",
    "\n",
    "This section creates comparison plots across all 6 experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all experiments (load results from experiment_results)\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    # Load all CSV files\n",
    "    all_data = {}\n",
    "    for exp_id, result in experiment_results.items():\n",
    "        if result.get(\"status\") == \"completed\":\n",
    "            csv_path = result[\"run_dir\"] / \"TrustWeight.csv\"\n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['time_min'] = df['time'] / 60.0\n",
    "                all_data[exp_id] = df\n",
    "                print(f\"âœ… Loaded {exp_id}: {len(df)} rows\")\n",
    "\n",
    "    if len(all_data) > 0:\n",
    "        # Create comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "\n",
    "        # Plot 1: Accuracy vs Rounds\n",
    "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
    "            axes[0, 0].plot(df['total_agg'], df['test_acc'],\n",
    "                          label=f\"{exp_id} ({experiments[exp_id]['name']})\",\n",
    "                          linewidth=2, color=colors[i % len(colors)])\n",
    "        axes[0, 0].set_xlabel('Round', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "        axes[0, 0].set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].legend(fontsize=9, loc='lower right')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_ylim([0, 1.0])\n",
    "\n",
    "        # Plot 2: Accuracy vs Wall Clock Time\n",
    "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
    "            axes[0, 1].plot(df['time_min'], df['test_acc'],\n",
    "                          label=f\"{exp_id}\",\n",
    "                          linewidth=2, color=colors[i % len(colors)])\n",
    "        axes[0, 1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Test Accuracy', fontsize=12)\n",
    "        axes[0, 1].set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].legend(fontsize=9, loc='lower right')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_ylim([0, 1.0])\n",
    "\n",
    "        # Plot 3: Best Accuracy by Experiment\n",
    "        best_accs = {exp_id: df['test_acc'].max() for exp_id, df in all_data.items()}\n",
    "        exp_names = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\"\n",
    "                    if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\"\n",
    "                    for exp_id in best_accs.keys()]\n",
    "        axes[1, 0].bar(range(len(best_accs)), list(best_accs.values()),\n",
    "                      color=colors[:len(best_accs)], alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_xticks(range(len(best_accs)))\n",
    "        axes[1, 0].set_xticklabels(exp_names, fontsize=9, rotation=45, ha='right')\n",
    "        axes[1, 0].set_ylabel('Best Test Accuracy', fontsize=12)\n",
    "        axes[1, 0].set_title('Best Test Accuracy by Experiment', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[1, 0].set_ylim([0, 1.0])\n",
    "        # Add value labels\n",
    "        for i, (exp_id, acc) in enumerate(best_accs.items()):\n",
    "            axes[1, 0].text(i, acc, f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        # Plot 4: Time to reach 50% accuracy\n",
    "        times_to_50 = {}\n",
    "        for exp_id, df in all_data.items():\n",
    "            mask = df['test_acc'] >= 0.5\n",
    "            if mask.any():\n",
    "                first_idx = mask.idxmax()\n",
    "                times_to_50[exp_id] = df.loc[first_idx, 'time_min']\n",
    "            else:\n",
    "                times_to_50[exp_id] = None\n",
    "\n",
    "        valid_times = {k: v for k, v in times_to_50.items() if v is not None}\n",
    "        if len(valid_times) > 0:\n",
    "            exp_names_50 = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\"\n",
    "                           if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\"\n",
    "                           for exp_id in valid_times.keys()]\n",
    "            axes[1, 1].bar(range(len(valid_times)), list(valid_times.values()),\n",
    "                         color=colors[:len(valid_times)], alpha=0.7, edgecolor='black')\n",
    "            axes[1, 1].set_xticks(range(len(valid_times)))\n",
    "            axes[1, 1].set_xticklabels(exp_names_50, fontsize=9, rotation=45, ha='right')\n",
    "            axes[1, 1].set_ylabel('Time to Reach 50% Accuracy (minutes)', fontsize=12)\n",
    "            axes[1, 1].set_title('Convergence Speed: Time to 50% Accuracy', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "            # Add value labels\n",
    "            for i, (exp_id, t) in enumerate(valid_times.items()):\n",
    "                axes[1, 1].text(i, t, f'{t:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # Use Google Drive path if in Colab, otherwise local\n",
    "        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"TrustWeight\" / \"comparisons\"\n",
    "        else:\n",
    "            comparison_dir = Path(\"./logs/TrustWeight/comparisons\")\n",
    "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(comparison_dir / \"all_experiments_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\nâœ… Comparison plot saved: {comparison_dir / 'all_experiments_comparison.png'}\")\n",
    "        plt.show()\n",
    "\n",
    "        # Print summary table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPERIMENT COMPARISON SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_data:\n",
    "                df = all_data[exp_id]\n",
    "                cfg = experiments[exp_id]\n",
    "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
    "                      f\"{df['test_acc'].max():<10.4f} {df['test_acc'].iloc[-1]:<11.4f} \"\n",
    "                      f\"{df['time_min'].iloc[-1]:<12.2f}\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"âš ï¸  No completed experiments found. Run experiments first.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No experiment results found. Run the experiments first using Section 8.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. View Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results for all 6 experiments\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"TRUSTWEIGHT RESULTS - ALL 6 EXPERIMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n",
    "            run_dir = experiment_results[exp_id][\"run_dir\"]\n",
    "            csv_path = run_dir / \"TrustWeight.csv\"\n",
    "            \n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                all_results[exp_id] = df\n",
    "                \n",
    "                exp_config = experiments[exp_id]\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"{exp_id}: {exp_config['name']}\")\n",
    "                print(f\"{'='*80}\")\n",
    "                print(f\"  Alpha: {exp_config['partition_alpha']}\")\n",
    "                print(f\"  Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
    "                print(f\"  Max rounds: {exp_config['train']['max_rounds']}\")\n",
    "                print(f\"\\n  Total rounds completed: {df['total_agg'].max()}\")\n",
    "                \n",
    "                final_row = df.iloc[-1]\n",
    "                print(f\"\\n  Final metrics:\")\n",
    "                print(f\"    - Test accuracy: {final_row['test_acc']:.4f}\")\n",
    "                print(f\"    - Train accuracy: {final_row['avg_train_acc']:.4f}\")\n",
    "                print(f\"    - Test loss: {final_row['test_loss']:.4f}\")\n",
    "                print(f\"    - Total time: {final_row['time']:.1f} seconds ({final_row['time']/60:.2f} minutes)\")\n",
    "                \n",
    "                best_acc = df['test_acc'].max()\n",
    "                best_round = df.loc[df['test_acc'].idxmax(), 'total_agg']\n",
    "                print(f\"\\n  Best test accuracy: {best_acc:.4f} (round {best_round})\")\n",
    "                \n",
    "                print(f\"\\n  Run folder: {run_dir}\")\n",
    "            else:\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"{exp_id}: Results file not found\")\n",
    "                print(f\"  Run folder: {run_dir}\")\n",
    "        else:\n",
    "            status = experiment_results.get(exp_id, {}).get(\"status\", \"not run\")\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"{exp_id}: {status.upper()}\")\n",
    "            if status == \"error\":\n",
    "                print(f\"  Error: {experiment_results[exp_id].get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Summary table\n",
    "    if len(all_results) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SUMMARY TABLE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Rounds':<8} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_results:\n",
    "                df = all_results[exp_id]\n",
    "                cfg = experiments[exp_id]\n",
    "                final_row = df.iloc[-1]\n",
    "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
    "                      f\"{df['total_agg'].max():<8} {df['test_acc'].max():<10.4f} {final_row['test_acc']:<11.4f} \"\n",
    "                      f\"{final_row['time']/60:<12.2f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show first and last rows for each experiment\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DETAILED DATA PREVIEW\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_results:\n",
    "                df = all_results[exp_id]\n",
    "                exp_config = experiments[exp_id]\n",
    "                print(f\"\\n{exp_id}: {exp_config['name']}\")\n",
    "                print(f\"  First 3 rows:\")\n",
    "                print(df.head(3).to_string(index=False))\n",
    "                print(f\"\\n  Last 3 rows:\")\n",
    "                print(df.tail(3).to_string(index=False))\n",
    "                print()\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No completed experiments found. Run experiments first using Section 8.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No experiment results found. Run experiments first using Section 8.\")\n",
    "    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Comprehensive Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization suite for all 6 experiments\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    all_data = {}\n",
    "    all_participation = {}\n",
    "    \n",
    "    # Load all experiment data\n",
    "    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n",
    "            run_dir = experiment_results[exp_id][\"run_dir\"]\n",
    "            csv_path = run_dir / \"TrustWeight.csv\"\n",
    "            participation_path = run_dir / \"TrustWeightClientParticipation.csv\"\n",
    "            \n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['time_min'] = df['time'] / 60.0\n",
    "                all_data[exp_id] = {\n",
    "                    'df': df,\n",
    "                    'run_dir': run_dir,\n",
    "                    'config': experiments[exp_id]\n",
    "                }\n",
    "                \n",
    "                if participation_path.exists():\n",
    "                    part_df = pd.read_csv(participation_path)\n",
    "                    all_participation[exp_id] = part_df\n",
    "    \n",
    "    if len(all_data) > 0:\n",
    "        print(f\"âœ… Loaded data for {len(all_data)} experiments\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Generate individual plots for each experiment\n",
    "        for exp_id, exp_data in all_data.items():\n",
    "            df = exp_data['df']\n",
    "            run_dir = exp_data['run_dir']\n",
    "            exp_config = exp_data['config']\n",
    "            participation_path = run_dir / \"TrustWeightClientParticipation.csv\"\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Generating plots for {exp_id}: {exp_config['name']}\")\n",
    "            \n",
    "            # Plot 1: Accuracy vs Rounds\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "            train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "            train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n",
    "            if len(train_acc) > 0:\n",
    "                axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "            axes[0].set_xlabel('Round', fontsize=12)\n",
    "            axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "            axes[0].set_title(f'{exp_id}: Accuracy vs Rounds', fontsize=14, fontweight='bold')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].legend(fontsize=10)\n",
    "            axes[0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "            train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "            train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n",
    "            if len(train_loss) > 0:\n",
    "                axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "            axes[1].set_xlabel('Round', fontsize=12)\n",
    "            axes[1].set_ylabel('Loss', fontsize=12)\n",
    "            axes[1].set_title(f'{exp_id}: Loss vs Rounds', fontsize=14, fontweight='bold')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            axes[1].legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 2: Accuracy vs Wall Clock Time\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "            train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "            train_time = df[df['avg_train_acc'] > 0]['time_min']\n",
    "            if len(train_acc_time) > 0:\n",
    "                axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "            axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "            axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "            axes[0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].legend(fontsize=10)\n",
    "            axes[0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "            train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "            train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n",
    "            if len(train_loss_time) > 0:\n",
    "                axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "            axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "            axes[1].set_ylabel('Loss', fontsize=12)\n",
    "            axes[1].set_title(f'{exp_id}: Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            axes[1].legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 3: Convergence Speed Analysis\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "            times_to_threshold = []\n",
    "            rounds_to_threshold = []\n",
    "            reached_thresholds = []\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                mask = df['test_acc'] >= thresh\n",
    "                if mask.any():\n",
    "                    first_idx = mask.idxmax()\n",
    "                    times_to_threshold.append(df.loc[first_idx, 'time_min'])\n",
    "                    rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n",
    "                    reached_thresholds.append(thresh)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if len(reached_thresholds) > 0:\n",
    "                axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "                axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "                axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n",
    "                axes[0].set_title(f'{exp_id}: Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "                axes[0].set_xticks(range(len(reached_thresholds)))\n",
    "                axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "                axes[0].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n",
    "                    axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "                \n",
    "                axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "                axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "                axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n",
    "                axes[1].set_title(f'{exp_id}: Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "                axes[1].set_xticks(range(len(reached_thresholds)))\n",
    "                axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "                axes[1].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n",
    "                    axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 4: Training Efficiency\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "            \n",
    "            if len(df) > 1:\n",
    "                df_sorted = df.sort_values('time_min')\n",
    "                efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n",
    "                efficiency = efficiency.fillna(0)\n",
    "                \n",
    "                ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n",
    "                ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n",
    "                ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "                ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n",
    "                ax.set_title(f'{exp_id}: Training Efficiency', fontsize=14, fontweight='bold')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                max_eff_idx = efficiency.idxmax()\n",
    "                max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n",
    "                max_eff_val = efficiency.loc[max_eff_idx]\n",
    "                ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n",
    "                ax.legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 5: Client Participation Analysis\n",
    "            if participation_path.exists():\n",
    "                part_df = pd.read_csv(participation_path)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "                \n",
    "                client_counts = part_df['client_id'].value_counts().sort_index()\n",
    "                axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n",
    "                axes[0, 0].set_xlabel('Client ID', fontsize=11)\n",
    "                axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n",
    "                axes[0, 0].set_title(f'{exp_id}: Client Participation Frequency', fontsize=12, fontweight='bold')\n",
    "                axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n",
    "                axes[0, 1].plot(participation_by_round.index, participation_by_round.values, \n",
    "                               marker='o', markersize=4, linewidth=2, color='coral')\n",
    "                axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "                axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n",
    "                axes[0, 1].set_title(f'{exp_id}: Client Participation per Round', fontsize=12, fontweight='bold')\n",
    "                axes[0, 1].grid(True, alpha=0.3)\n",
    "                \n",
    "                client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n",
    "                axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values, \n",
    "                                marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n",
    "                axes[1, 0].set_xlabel('Round', fontsize=11)\n",
    "                axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n",
    "                axes[1, 0].set_title(f'{exp_id}: Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "                axes[1, 0].set_ylim([0, 1.0])\n",
    "                \n",
    "                if len(part_df) > 0:\n",
    "                    client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values \n",
    "                                  for cid in sorted(part_df['client_id'].unique())[:10]]\n",
    "                    if len(client_accs) > 0:\n",
    "                        axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n",
    "                        axes[1, 1].set_xlabel('Client ID', fontsize=11)\n",
    "                        axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
    "                        axes[1, 1].set_title(f'{exp_id}: Client Accuracy Distribution', fontsize=12, fontweight='bold')\n",
    "                        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "                        axes[1, 1].set_ylim([0, 1.0])\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(run_dir / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # Plot 6: Training Progress Summary\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            \n",
    "            axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "            train_acc_plot = df[df['avg_train_acc'] > 0]\n",
    "            if len(train_acc_plot) > 0:\n",
    "                axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'], \n",
    "                               'r--', linewidth=2, label='Train')\n",
    "            axes[0, 0].set_xlabel('Round', fontsize=11)\n",
    "            axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "            axes[0, 0].set_title(f'{exp_id}: Accuracy Trajectory', fontsize=12, fontweight='bold')\n",
    "            axes[0, 0].legend(fontsize=10)\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n",
    "            train_loss_plot = df[df['avg_train_loss'] > 0]\n",
    "            if len(train_loss_plot) > 0:\n",
    "                axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'], \n",
    "                               'r--', linewidth=2, label='Train')\n",
    "            axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "            axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
    "            axes[0, 1].set_title(f'{exp_id}: Loss Trajectory', fontsize=12, fontweight='bold')\n",
    "            axes[0, 1].legend(fontsize=10)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "            if len(train_acc_plot) > 0:\n",
    "                train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n",
    "                axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'], \n",
    "                                'r--', linewidth=2, label='Train')\n",
    "            axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n",
    "            axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "            axes[1, 0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n",
    "            axes[1, 0].legend(fontsize=10)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n",
    "            axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n",
    "            axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n",
    "            axes[1, 1].set_title(f'{exp_id}: Round Progression Over Time', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            if len(df) > 1:\n",
    "                total_time = df['time_min'].iloc[-1]\n",
    "                total_rounds = df['total_agg'].iloc[-1]\n",
    "                rate = total_rounds / total_time if total_time > 0 else 0\n",
    "                axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min', \n",
    "                               transform=axes[1, 1].transAxes, fontsize=10,\n",
    "                               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  âœ… All plots saved to {run_dir}\")\n",
    "        \n",
    "        # Create comparison plots across all experiments\n",
    "        print(f\"\\nðŸ“Š Creating comparison plots across all experiments...\")\n",
    "        \n",
    "        # Comparison Plot 1: Test Accuracy vs Rounds (all experiments)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n",
    "            df = exp_data['df']\n",
    "            exp_config = exp_data['config']\n",
    "            label = f\"{exp_id} (Î±={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n",
    "            ax.plot(df['total_agg'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n",
    "        ax.set_xlabel('Round', fontsize=12)\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "        ax.set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([0, 1.0])\n",
    "        \n",
    "        # Save comparison plot\n",
    "        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"TrustWeight\" / \"comparisons\"\n",
    "        else:\n",
    "            comparison_dir = Path(\"./logs/TrustWeight/comparisons\")\n",
    "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(comparison_dir / \"comparison_accuracy_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  âœ… Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_rounds.png'}\")\n",
    "        \n",
    "        # Comparison Plot 2: Test Accuracy vs Time (all experiments)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n",
    "            df = exp_data['df']\n",
    "            exp_config = exp_data['config']\n",
    "            label = f\"{exp_id} (Î±={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n",
    "            ax.plot(df['time_min'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n",
    "        ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "        ax.set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([0, 1.0])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(comparison_dir / \"comparison_accuracy_vs_time.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  âœ… Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_time.png'}\")\n",
    "\n",
    "        print(f\"\\nâœ… All visualizations completed!\")\n",
    "        print(f\"   Individual plots saved to each experiment's run folder\")\n",
    "        print(f\"   Comparison plots saved to: {comparison_dir}\")\n",
    "\n",
    "    else:\n",
    "        print(\"âš ï¸  No completed experiments found. Run experiments first using Section 8.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No experiment results found. Run experiments first using Section 8.\")\n",
    "    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Assuming comparison_dir is defined from the previous cell's execution\n",
    "# If not, re-define it based on the environment\n",
    "if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "    comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"TrustWeight\" / \"comparisons\"\n",
    "else:\n",
    "    comparison_dir = Path(\"./logs/TrustWeight/comparisons\")\n",
    "\n",
    "print(\"Displaying Comparison Plots:\")\n",
    "\n",
    "# Display comparison plots\n",
    "comp_plot_acc_rounds = comparison_dir / \"comparison_accuracy_vs_rounds.png\"\n",
    "if os.path.exists(comp_plot_acc_rounds):\n",
    "    print(f\"\\n{'-'*50}\\nTest Accuracy vs Rounds (All Experiments)\")\n",
    "    display(Image(filename=comp_plot_acc_rounds))\n",
    "else:\n",
    "    print(f\"Warning: Comparison plot not found: {comp_plot_acc_rounds}\")\n",
    "\n",
    "comp_plot_acc_time = comparison_dir / \"comparison_accuracy_vs_time.png\"\n",
    "if os.path.exists(comp_plot_acc_time):\n",
    "    print(f\"\\n{'-'*50}\\nTest Accuracy vs Wall Clock Time (All Experiments)\")\n",
    "    display(Image(filename=comp_plot_acc_time))\n",
    "else:\n",
    "    print(f\"Warning: Comparison plot not found: {comp_plot_acc_time}\")\n",
    "\n",
    "# Display individual experiment plots as requested\n",
    "print(\"\\nDisplaying Individual Experiment Plots (Accuracy vs Rounds):\")\n",
    "# Ensure 'all_data' is available, it should be from the previous cell\n",
    "if 'all_data' in globals() and len(all_data) > 0:\n",
    "    for exp_id, exp_data in all_data.items():\n",
    "        run_dir = exp_data['run_dir']\n",
    "        plot_path = run_dir / \"1_accuracy_loss_vs_rounds.png\"\n",
    "        if os.path.exists(plot_path):\n",
    "            print(f\"\\n{'-'*50}\\n{exp_id}: Accuracy & Loss vs Rounds\")\n",
    "            display(Image(filename=plot_path))\n",
    "        else:\n",
    "            print(f\"Warning: Plot not found for {exp_id}: {plot_path}\")\n",
    "else:\n",
    "    print(\"Warning: 'all_data' not found. Please ensure Section 11 has been run.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
