{"cells":[{"cell_type":"markdown","metadata":{"id":"mx-GZaczjP_V"},"source":["# FedBuff: Complete Self-Contained Notebook\n","\n","This notebook contains all code needed to run FedBuff experiments:\n","- Library installation\n","- Data downloading and loading\n","- All FedBuff implementation\n","- Logging and checkpointing\n","\n","**Run cells sequentially from top to bottom.**\n","\n","## Google Colab Setup\n","This notebook is configured to save all results (logs, checkpoints, models) to Google Drive.\n","Data will be downloaded locally for faster access.\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cFMzNoKsjd1t","executionInfo":{"status":"ok","timestamp":1764480915692,"user_tz":360,"elapsed":24505,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"c43ee5c3-9a03-42f4-8424-21e091cb57a9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"BoWBNsiGjP_W"},"source":["## 1. Google Colab Setup (Mount Drive)\n","\n","**Skip this section if running locally.**\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VURndg6GjP_W","executionInfo":{"status":"ok","timestamp":1764480924097,"user_tz":360,"elapsed":8406,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"d442d767-900c-40a2-da22-4947797525fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","âœ… Google Drive mounted\n","âœ… Output directory set to: /content/drive/MyDrive/colab/dml_project\n","ðŸ“ All logs, checkpoints, and results will be saved to Google Drive\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["import os\n","\n","# Google Colab: Mount Google Drive\n","try:\n","    from google.colab import drive\n","    IN_COLAB = True\n","    # Mount Google Drive\n","    drive.mount('/content/drive')\n","    # Define the target directory for saving results\n","    OUTPUT_DIR = \"/content/drive/MyDrive/colab/dml_project\"\n","    # Create the directory if it doesn't exist\n","    os.makedirs(OUTPUT_DIR, exist_ok=True)\n","    print(f\"âœ… Google Drive mounted\")\n","    print(f\"âœ… Output directory set to: {OUTPUT_DIR}\")\n","    print(f\"ðŸ“ All logs, checkpoints, and results will be saved to Google Drive\")\n","except ImportError:\n","    IN_COLAB = False\n","    OUTPUT_DIR = None\n","    print(\"âš ï¸  Not running in Google Colab - using local paths\")\n","\n","# Install required packages\n","%pip install torch torchvision pytorch-lightning pyyaml numpy matplotlib pandas -q\n"]},{"cell_type":"markdown","metadata":{"id":"k-Kg_VN8jP_X"},"source":["## 2. Import Libraries and Setup\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NAaqEE0EjP_X","executionInfo":{"status":"ok","timestamp":1764480938702,"user_tz":360,"elapsed":14598,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"d7a69370-a558-443b-fca0-11e653d9b3ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Google Colab mode: Results â†’ /content/drive/MyDrive/colab/dml_project\n","âœ… Data directory: data (local)\n","âœ… Libraries imported\n"]}],"source":["# Silence libraries\n","import os\n","import logging, warnings\n","os.environ[\"TQDM_DISABLE\"] = \"1\"\n","os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n","os.environ[\"LIGHTNING_DISABLE_RICH\"] = \"1\"\n","for name in [\n","    \"pytorch_lightning\", \"lightning\", \"lightning.pytorch\",\n","    \"lightning_fabric\", \"lightning_utilities\", \"torch\", \"torchvision\",\n","]:\n","    logging.getLogger(name).setLevel(logging.ERROR)\n","    logging.getLogger(name).propagate = False\n","logging.getLogger().setLevel(logging.WARNING)\n","warnings.filterwarnings(\"ignore\")\n","\n","# Core imports\n","import time\n","import csv\n","import threading\n","import random\n","import numpy as np\n","from pathlib import Path\n","from typing import List, Tuple, Optional, Dict, Any\n","from datetime import datetime\n","import subprocess\n","import shutil\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import datasets, transforms, models\n","\n","import pytorch_lightning as pl\n","import yaml\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Setup paths based on environment\n","if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n","    # Google Colab: Save everything to Drive except data\n","    BASE_OUTPUT_DIR = Path(OUTPUT_DIR)\n","    DATA_DIR = Path(\"./data\")  # Local for faster access\n","    print(f\"âœ… Google Colab mode: Results â†’ {BASE_OUTPUT_DIR}\")\n","    print(f\"âœ… Data directory: {DATA_DIR} (local)\")\n","else:\n","    # Local execution: Use current directory\n","    BASE_OUTPUT_DIR = Path(\".\")\n","    DATA_DIR = Path(\"./data\")\n","    print(f\"âœ… Local mode: Results â†’ {BASE_OUTPUT_DIR}\")\n","\n","print(\"âœ… Libraries imported\")\n"]},{"cell_type":"markdown","metadata":{"id":"n9ZoHxj2jP_Y"},"source":["## 3. Utility Functions\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6F9cx8bjP_Y","executionInfo":{"status":"ok","timestamp":1764480938721,"user_tz":360,"elapsed":15,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"aca7ddd3-6663-41ff-f688-ab16052481cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Helper functions defined\n"]}],"source":["# ========== Helper Functions ==========\n","\n","def set_seed(seed: int = 42) -> None:\n","    \"\"\"Seed all RNGs for reproducibility.\"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","def get_device() -> torch.device:\n","    \"\"\"Return the first available computation device (CUDA/MPS/CPU).\"\"\"\n","    if torch.cuda.is_available():\n","        return torch.device(\"cuda\")\n","    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n","        return torch.device(\"mps\")\n","    return torch.device(\"cpu\")\n","\n","\n","def _device_to_accelerator(device: torch.device) -> str:\n","    \"\"\"Convert torch device to PyTorch Lightning accelerator string.\"\"\"\n","    if device.type == \"cuda\":\n","        return \"gpu\"\n","    if device.type == \"mps\":\n","        return \"mps\"\n","    return \"cpu\"\n","\n","print(\"âœ… Helper functions defined\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3OlfYQQjP_Y","executionInfo":{"status":"ok","timestamp":1764480938747,"user_tz":360,"elapsed":19,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"9fb484e2-d8f2-41b5-a25d-8b3be65c8b7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Model utilities defined\n"]}],"source":["# ========== Model Utilities ==========\n","\n","def build_resnet18(num_classes: int = 10, pretrained: bool = False) -> nn.Module:\n","    \"\"\"Create ResNet-18 adapted for CIFAR-10.\"\"\"\n","    if pretrained:\n","        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","    else:\n","        m = models.resnet18(weights=None)\n","    # CIFAR-10: 32x32 -> use 3x3 conv, stride 1, no maxpool\n","    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","    m.maxpool = nn.Identity()\n","    # Replace classifier\n","    m.fc = nn.Linear(m.fc.in_features, num_classes)\n","    m.num_classes = num_classes\n","    return m\n","\n","\n","def state_to_list(state: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n","    \"\"\"Flatten a state_dict to a list of tensors on CPU.\"\"\"\n","    return [t.detach().cpu().clone() for _, t in state.items()]\n","\n","\n","def list_to_state(template: Dict[str, torch.Tensor], arrs: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n","    \"\"\"Rebuild a state_dict from a list of tensors using a template.\"\"\"\n","    out: Dict[str, torch.Tensor] = {}\n","    for (k, v), a in zip(template.items(), arrs):\n","        out[k] = a.to(v.device).type_as(v)\n","    return out\n","\n","print(\"âœ… Model utilities defined\")\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"veaBovtPjP_Y","executionInfo":{"status":"ok","timestamp":1764480938785,"user_tz":360,"elapsed":30,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"9cb8ed90-6801-4324-ecce-a5f9a4b14958"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… DataDistributor class defined\n"]}],"source":["# ========== Data Loading and Partitioning ==========\n","\n","class DataDistributor:\n","    \"\"\"Data distributor for federated learning with Dirichlet partitioning.\"\"\"\n","\n","    def __init__(self, dataset_name: str, data_dir: str = \"./data\"):\n","        self.dataset_name = dataset_name.lower()\n","        self.data_dir = data_dir\n","        self.train_dataset, self.test_dataset, self.num_classes = self._load_dataset()\n","        self.partitions = None\n","\n","    def _load_dataset(self) -> Tuple[Any, Any, int]:\n","        \"\"\"Load CIFAR-10 dataset with augmentation.\"\"\"\n","        if self.dataset_name == \"cifar10\":\n","            # Strong data pipeline: augmentation for train, normalization for test\n","            transform_train = transforms.Compose([\n","                transforms.RandomCrop(32, padding=4),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                transforms.Normalize(\n","                    mean=(0.4914, 0.4822, 0.4465),\n","                    std=(0.2470, 0.2435, 0.2616),\n","                ),\n","            ])\n","            transform_test = transforms.Compose([\n","                transforms.ToTensor(),\n","                transforms.Normalize(\n","                    mean=(0.4914, 0.4822, 0.4465),\n","                    std=(0.2470, 0.2435, 0.2616),\n","                ),\n","            ])\n","            train = datasets.CIFAR10(self.data_dir, train=True, download=True, transform=transform_train)\n","            test = datasets.CIFAR10(self.data_dir, train=False, download=True, transform=transform_test)\n","            num_classes = 10\n","        else:\n","            raise ValueError(f\"Dataset '{self.dataset_name}' not supported. Use 'cifar10'.\")\n","        return train, test, num_classes\n","\n","    def distribute_data(self, num_clients: int, alpha: float = 0.5, seed: int = 42):\n","        \"\"\"Partition data using Dirichlet distribution.\"\"\"\n","        np.random.seed(seed)\n","        targets = np.array(self.train_dataset.targets)\n","        self.partitions = {i: [] for i in range(num_clients)}\n","\n","        for cls in range(self.num_classes):\n","            idxs = np.where(targets == cls)[0]\n","            np.random.shuffle(idxs)\n","            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n","            proportions = np.array([p * len(idxs) for p in proportions]).astype(int)\n","\n","            start = 0\n","            for client_id, size in enumerate(proportions):\n","                self.partitions[client_id].extend(idxs[start:start + size])\n","                start += size\n","\n","        for cid in self.partitions:\n","            np.random.shuffle(self.partitions[cid])\n","\n","    def get_client_data(self, client_id: int) -> Subset:\n","        \"\"\"Get data subset for a specific client.\"\"\"\n","        if self.partitions is None:\n","            raise ValueError(\"Data not distributed yet. Call distribute_data() first.\")\n","        indices = self.partitions[client_id]\n","        return Subset(self.train_dataset, indices)\n","\n","print(\"âœ… DataDistributor class defined\")\n"]},{"cell_type":"markdown","metadata":{"id":"tPYlA14njP_Z"},"source":["## 4. FedBuff Client Implementation\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVyym323jP_Z","executionInfo":{"status":"ok","timestamp":1764480938954,"user_tz":360,"elapsed":167,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"26b0bc44-492e-4ef7-d490-7c1d15db374f"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… FedBuff client classes defined\n"]}],"source":["# ========== FedBuff Client ==========\n","\n","def _testloader(root: str, batch_size: int = 256):\n","    \"\"\"Create test dataloader.\"\"\"\n","    tfm = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n","    ])\n","    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n","    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","\n","def _evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Evaluate model on test set.\"\"\"\n","    crit = nn.CrossEntropyLoss()\n","    model = model.to(device)\n","    model.eval()\n","    total, correct, loss_sum = 0, 0, 0.0\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            logits = model(x)\n","            loss = crit(logits, y)\n","            loss_sum += float(loss.item()) * y.size(0)\n","            total += y.size(0)\n","            correct += (logits.argmax(1) == y).sum().item()\n","    return loss_sum / max(1, total), correct / max(1, total)\n","\n","\n","class LitCifar(pl.LightningModule):\n","    \"\"\"PyTorch Lightning module for CIFAR-10 training.\"\"\"\n","\n","    def __init__(self, base_model: nn.Module, lr: float = 1e-3, momentum: float = 0.9, weight_decay: float = 5e-4):\n","        super().__init__()\n","        self.save_hyperparameters(ignore=[\"base_model\"])\n","        self.model = base_model\n","        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n","        # Store optimizer params directly\n","        self._optimizer_lr = lr\n","        self._optimizer_momentum = momentum\n","        self._optimizer_weight_decay = weight_decay\n","        self._train_loss_sum = 0.0\n","        self._train_correct = 0\n","        self._train_total = 0\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, _batch_idx):\n","        x, y = batch\n","        logits = self(x)\n","        loss = self.criterion(logits, y)\n","        pred = logits.argmax(1)\n","        self._train_loss_sum += float(loss.item()) * y.size(0)\n","        self._train_correct += (pred == y).sum().item()\n","        self._train_total += y.size(0)\n","        return loss\n","\n","    def on_train_epoch_start(self):\n","        self._train_loss_sum = 0.0\n","        self._train_correct = 0\n","        self._train_total = 0\n","\n","    def get_epoch_metrics(self) -> Tuple[float, float]:\n","        if self._train_total == 0:\n","            return 0.0, 0.0\n","        return self._train_loss_sum / self._train_total, self._train_correct / self._train_total\n","\n","    def configure_optimizers(self):\n","        return torch.optim.SGD(\n","            self.parameters(),\n","            lr=self._optimizer_lr,\n","            momentum=self._optimizer_momentum,\n","            weight_decay=self._optimizer_weight_decay\n","        )\n","\n","\n","class LocalBuffClient:\n","    \"\"\"FedBuff client that trains locally and submits updates to server.\"\"\"\n","\n","    def __init__(\n","        self,\n","        cid: int,\n","        cfg: dict,\n","        subset: Subset,\n","        work_dir: str = \"./checkpoints/clients\",\n","        base_delay: float = 0.0,\n","        slow: bool = False,\n","        delay_ranges: Optional[tuple] = None,\n","        jitter: float = 0.0,\n","        fix_delay: bool = True,\n","    ):\n","        self.cid = cid\n","        self.cfg = cfg\n","        self.device = get_device()\n","\n","        base = build_resnet18(num_classes=cfg[\"data\"][\"num_classes\"], pretrained=False)\n","        lr = float(cfg[\"clients\"][\"lr\"])\n","        momentum = float(cfg[\"clients\"].get(\"momentum\", 0.9))\n","        weight_decay = float(cfg[\"clients\"].get(\"weight_decay\", 5e-4))\n","        self.lit = LitCifar(base, lr=lr, momentum=momentum, weight_decay=weight_decay)\n","\n","        self.loader = DataLoader(subset, batch_size=int(cfg[\"clients\"][\"batch_size\"]),\n","                                 shuffle=True, num_workers=0)\n","\n","        self.base_delay = float(base_delay)\n","        self.slow = bool(slow)\n","        self.delay_ranges = delay_ranges\n","        self.jitter = float(jitter)\n","        self.fix_delay = bool(fix_delay)\n","\n","        if self.fix_delay and self.delay_ranges is not None:\n","            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n","            if self.slow:\n","                self.base_delay = random.uniform(float(a_s), float(b_s))\n","            else:\n","                self.base_delay = random.uniform(float(a_f), float(b_f))\n","\n","        self.accelerator = _device_to_accelerator(self.device)\n","        self.testloader = _testloader(cfg[\"data\"][\"data_dir\"])\n","\n","    def _to_list(self) -> List[torch.Tensor]:\n","        return state_to_list(self.lit.model.state_dict())\n","\n","    def _from_list(self, arrs: List[torch.Tensor]) -> None:\n","        sd = self.lit.model.state_dict()\n","        new_sd = list_to_state(sd, arrs)\n","        self.lit.model.load_state_dict(new_sd, strict=True)\n","        self.lit.to(self.device)\n","\n","    def _sleep_delay(self):\n","        global_d = float(self.cfg.get(\"server_runtime\", {}).get(\"client_delay\", 0.0))\n","        base = self.base_delay\n","\n","        if not self.fix_delay and self.delay_ranges is not None:\n","            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n","            if self.slow:\n","                base = random.uniform(float(a_s), float(b_s))\n","            else:\n","                base = random.uniform(float(a_f), float(b_f))\n","\n","        jit = random.uniform(-self.jitter, self.jitter) if self.jitter > 0.0 else 0.0\n","        delay = max(0.0, global_d + base + jit)\n","        if delay > 0.0:\n","            time.sleep(delay)\n","\n","    def fit_once(self, server) -> bool:\n","        params, version = server.get_global()\n","        self._from_list(params)\n","\n","        self._sleep_delay()\n","\n","        epochs = int(self.cfg[\"clients\"][\"local_epochs\"])\n","        grad_clip = float(self.cfg[\"clients\"].get(\"grad_clip\", 1.0))\n","        trainer = pl.Trainer(\n","            max_epochs=epochs,\n","            accelerator=self.accelerator,\n","            devices=1,\n","            enable_checkpointing=False,\n","            logger=False,\n","            enable_model_summary=False,\n","            num_sanity_val_steps=0,\n","            enable_progress_bar=False,\n","            callbacks=[],\n","            gradient_clip_val=grad_clip,\n","            gradient_clip_algorithm=\"norm\",\n","        )\n","        start = time.time()\n","        trainer.fit(self.lit, train_dataloaders=self.loader)\n","        duration = time.time() - start\n","\n","        train_loss, train_acc = self.lit.get_epoch_metrics()\n","        test_loss, test_acc = _evaluate(self.lit.model, self.testloader, self.device)\n","\n","        new_params = self._to_list()\n","        num_examples = len(self.loader.dataset)\n","\n","        server.submit_update(\n","            client_id=self.cid,\n","            base_version=version,\n","            new_params=new_params,\n","            num_samples=num_examples,\n","            train_time_s=duration,\n","            train_loss=train_loss,\n","            train_acc=train_acc,\n","            test_loss=test_loss,\n","            test_acc=test_acc,\n","        )\n","        return not server.should_stop()\n","\n","print(\"âœ… FedBuff client classes defined\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iE34xI-YjP_a","executionInfo":{"status":"ok","timestamp":1764480939035,"user_tz":360,"elapsed":79,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"34bfb4c1-726a-41de-9ecc-eca75324a6d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… FedBuff server class defined\n"]}],"source":["# ========== FedBuff Server ==========\n","\n","def _testloader_server(root: str, batch_size: int = 256):\n","    \"\"\"Create test dataloader for server evaluation.\"\"\"\n","    tfm = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n","    ])\n","    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n","    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n","\n","\n","def _evaluate_server(model: torch.nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n","    \"\"\"Evaluate model on test set.\"\"\"\n","    criterion = torch.nn.CrossEntropyLoss()\n","    model.eval()\n","    total, correct, loss_sum = 0, 0, 0.0\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(device), y.to(device)\n","            logits = model(x)\n","            loss = criterion(logits, y)\n","            loss_sum += float(loss.item()) * y.size(0)\n","            total += y.size(0)\n","            correct += (logits.argmax(1) == y).sum().item()\n","    return loss_sum / max(1, total), correct / max(1, total)\n","\n","\n","class BufferedFedServer:\n","    \"\"\"FedBuff server that buffers client updates and aggregates them.\"\"\"\n","\n","    def __init__(\n","        self,\n","        global_model: torch.nn.Module,\n","        total_train_samples: int,\n","        buffer_size: int = 5,\n","        buffer_timeout_s: float = 10.0,\n","        use_sample_weighing: bool = True,\n","        target_accuracy: float = 0.70,\n","        max_rounds: Optional[int] = None,\n","        eval_interval_s: int = 15,\n","        data_dir: str = \"./data\",\n","        checkpoints_dir: str = \"./checkpoints/FedBuff\",\n","        logs_dir: str = \"./logs/FedBuff\",\n","        global_log_csv: Optional[str] = None,\n","        client_participation_csv: Optional[str] = None,\n","        final_model_path: Optional[str] = None,\n","        resume: bool = True,\n","        device: Optional[torch.device] = None,\n","        eta: float = 0.5,\n","    ):\n","        self.model = global_model\n","        self.template = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n","        self.device = device or get_device()\n","        self.model.to(self.device)\n","\n","        self.total_train_samples = int(total_train_samples)\n","        self.buffer_size = int(buffer_size)\n","        self.buffer_timeout_s = float(buffer_timeout_s)\n","        self.use_sample_weighing = bool(use_sample_weighing)\n","        self.eta = float(eta)\n","\n","        self.eval_interval_s = int(eval_interval_s)\n","        self.target_accuracy = float(target_accuracy)\n","        self.max_rounds = int(max_rounds) if max_rounds is not None else None\n","\n","        self.data_dir = data_dir\n","        self.ckpt_dir = Path(checkpoints_dir)\n","        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n","        self.log_dir = Path(logs_dir)\n","        self.log_dir.mkdir(parents=True, exist_ok=True)\n","\n","        self.csv_path = Path(global_log_csv) if global_log_csv else (self.log_dir / \"FedBuff.csv\")\n","        self.participation_csv = Path(client_participation_csv) if client_participation_csv else (self.log_dir / \"FedBuffClientParticipation.csv\")\n","        self.final_model_path = Path(final_model_path) if final_model_path else Path(\"./results/FedBuffModel.pt\")\n","        self.final_model_path.parent.mkdir(parents=True, exist_ok=True)\n","\n","        if not self.csv_path.exists():\n","            self.csv_path.parent.mkdir(parents=True, exist_ok=True)\n","            with self.csv_path.open(\"w\", newline=\"\") as f:\n","                csv.writer(f).writerow([\"total_agg\", \"avg_train_loss\", \"avg_train_acc\",\n","                                        \"test_loss\", \"test_acc\", \"time\"])\n","\n","        if not self.participation_csv.exists():\n","            self.participation_csv.parent.mkdir(parents=True, exist_ok=True)\n","            with self.participation_csv.open(\"w\", newline=\"\") as f:\n","                csv.writer(f).writerow([\n","                    \"client_id\", \"local_train_loss\", \"local_train_acc\",\n","                    \"local_test_loss\", \"local_test_acc\", \"total_agg\"\n","                ])\n","\n","        self._lock = threading.Lock()\n","        self._stop = False\n","        self.t_round = 0\n","        self._log_count = 0\n","        self.testloader = _testloader_server(self.data_dir)\n","        self._train_loss_acc_accum: List[Tuple[float, float, int]] = []\n","        self._start_ts = time.time()\n","\n","        self._buffer: List[Dict] = []\n","        self._buffer_last_flush = time.time()\n","\n","        if resume:\n","            self._maybe_resume()\n","\n","    def _ckpt_file(self) -> Path:\n","        return self.ckpt_dir / \"server_last.ckpt\"\n","\n","    def _maybe_resume(self) -> None:\n","        ck = self._ckpt_file()\n","        if ck.exists():\n","            try:\n","                blob = torch.load(ck, map_location=\"cpu\")\n","                state = list_to_state(self.template, blob[\"global_params\"])\n","                self.model.load_state_dict(state, strict=True)\n","                self.t_round = int(blob[\"t_round\"])\n","                print(f\"[resume] Loaded server checkpoint at total_agg={self.t_round}\")\n","            except (RuntimeError, KeyError) as e:\n","                print(f\"[resume] Checkpoint incompatible, starting fresh: {type(e).__name__}\")\n","                ck.unlink()\n","                self.t_round = 0\n","\n","    def _save_ckpt(self) -> None:\n","        sd = state_to_list(self.model.state_dict())\n","        torch.save({\"t_round\": self.t_round, \"global_params\": sd}, self._ckpt_file())\n","\n","    def _save_final_model(self) -> None:\n","        torch.save(self.model.state_dict(), self.final_model_path)\n","\n","    def get_global(self):\n","        with self._lock:\n","            return state_to_list(self.model.state_dict()), self.t_round\n","\n","    def _flush_buffer(self) -> None:\n","        if not self._buffer:\n","            return\n","\n","        g = state_to_list(self.model.state_dict())\n","        total_samples = sum(u[\"num_samples\"] for u in self._buffer)\n","\n","        # Aggregate client models (weighted average)\n","        aggregated = []\n","        for gi in g:\n","            if gi.dtype.is_floating_point:\n","                aggregated.append(torch.zeros_like(gi, device=gi.device, dtype=torch.float32))\n","            else:\n","                aggregated.append(torch.zeros(gi.shape, device=gi.device, dtype=torch.float32))\n","\n","        for u in self._buffer:\n","            weight = float(u[\"num_samples\"]) / float(total_samples) if self.use_sample_weighing else 1.0 / len(self._buffer)\n","            for i, ci in enumerate(u[\"new_params\"]):\n","                ci_tensor = ci.to(aggregated[i].device)\n","                if ci_tensor.dtype.is_floating_point:\n","                    ci_tensor = ci_tensor.float()\n","                else:\n","                    ci_tensor = ci_tensor.float()\n","                aggregated[i] += weight * ci_tensor\n","\n","        # Convert back to original dtypes\n","        for i, gi in enumerate(g):\n","            if not gi.dtype.is_floating_point:\n","                aggregated[i] = aggregated[i].round().to(gi.dtype)\n","            else:\n","                aggregated[i] = aggregated[i].to(gi.dtype)\n","\n","        # Update global model: FedAvg-style (eta=1.0) or relaxed update (eta<1.0)\n","        if self.eta >= 1.0:\n","            merged = aggregated\n","        else:\n","            merged = [(1.0 - self.eta) * gi + self.eta * aggregated[i] for i, gi in enumerate(g)]\n","\n","        new_state = list_to_state(self.template, merged)\n","        self.model.load_state_dict(new_state, strict=True)\n","\n","        for u in self._buffer:\n","            self._train_loss_acc_accum.append((u[\"train_loss\"], u[\"train_acc\"], u[\"num_samples\"]))\n","\n","        self._buffer.clear()\n","        self._buffer_last_flush = time.time()\n","        self.t_round += 1\n","        self._save_ckpt()\n","\n","    def submit_update(\n","        self,\n","        client_id: int,\n","        base_version: int,\n","        new_params: List[torch.Tensor],\n","        num_samples: int,\n","        train_time_s: float,\n","        train_loss: float,\n","        train_acc: float,\n","        test_loss: float,\n","        test_acc: float,\n","    ) -> None:\n","        with self._lock:\n","            if self._stop:\n","                return\n","            if self.max_rounds is not None and self.t_round >= self.max_rounds:\n","                self._stop = True\n","                return\n","\n","            with self.participation_csv.open(\"a\", newline=\"\") as f:\n","                csv.writer(f).writerow([\n","                    client_id, f\"{train_loss:.6f}\", f\"{train_acc:.6f}\",\n","                    f\"{test_loss:.6f}\", f\"{test_acc:.6f}\", self.t_round\n","                ])\n","\n","            self._buffer.append({\n","                \"client_id\": client_id,\n","                \"base_version\": base_version,\n","                \"new_params\": new_params,\n","                \"num_samples\": num_samples,\n","                \"train_loss\": float(train_loss),\n","                \"train_acc\": float(train_acc),\n","                \"test_loss\": float(test_loss),\n","                \"test_acc\": float(test_acc),\n","            })\n","\n","            should_flush = False\n","            if len(self._buffer) >= self.buffer_size:\n","                should_flush = True\n","            elif time.time() - self._buffer_last_flush >= self.buffer_timeout_s:\n","                should_flush = True\n","\n","            if should_flush:\n","                self._flush_buffer()\n","\n","    def should_stop(self) -> bool:\n","        with self._lock:\n","            return self._stop\n","\n","    def mark_stop(self) -> None:\n","        with self._lock:\n","            if self._buffer:\n","                self._flush_buffer()\n","            self._stop = True\n","            self._save_final_model()\n","            print(f\"[LOG] saved final model -> {self.final_model_path}\")\n","\n","    def _compute_avg_train(self) -> Tuple[float, float]:\n","        if not self._train_loss_acc_accum:\n","            return 0.0, 0.0\n","        loss_sum, acc_sum, n_sum = 0.0, 0.0, 0\n","        for l, a, n in self._train_loss_acc_accum:\n","            loss_sum += l * n\n","            acc_sum += a * n\n","            n_sum += n\n","        return loss_sum / max(1, n_sum), acc_sum / max(1, n_sum)\n","\n","    def _periodic_eval_and_log(self):\n","        test_loss, test_acc = _evaluate_server(self.model, self.testloader, self.device)\n","        avg_train_loss, avg_train_acc = self._compute_avg_train()\n","        now = time.time() - self._start_ts\n","        self._train_loss_acc_accum.clear()\n","\n","        with self.csv_path.open(\"a\", newline=\"\") as f:\n","            csv.writer(f).writerow([\n","                self.t_round, f\"{avg_train_loss:.6f}\", f\"{avg_train_acc:.6f}\",\n","                f\"{test_loss:.6f}\", f\"{test_acc:.6f}\", f\"{now:.3f}\"\n","            ])\n","        print(f\"[LOG] total_agg={self.t_round} \"\n","              f\"avg_train_loss={avg_train_loss:.4f} avg_train_acc={avg_train_acc:.4f} \"\n","              f\"test_loss={test_loss:.4f} test_acc={test_acc:.4f} time={now:.1f}s\")\n","\n","        self._log_count += 1\n","        if self._log_count % 100 == 0:\n","            path = self.ckpt_dir / f\"global_log{self._log_count}_t{self.t_round}.pt\"\n","            torch.save(self.model.state_dict(), path)\n","\n","        if test_acc >= self.target_accuracy:\n","            self._stop = True\n","\n","    def start_eval_timer(self):\n","        def _loop():\n","            next_ts = time.time() + self.eval_interval_s\n","            while True:\n","                now = time.time()\n","                sleep_for = max(0.0, next_ts - now)\n","                time.sleep(sleep_for)\n","                with self._lock:\n","                    if self._stop:\n","                        break\n","                    self._periodic_eval_and_log()\n","                next_ts += self.eval_interval_s\n","        threading.Thread(target=_loop, daemon=True).start()\n","\n","    def wait(self):\n","        try:\n","            while not self.should_stop():\n","                time.sleep(0.2)\n","        finally:\n","            self.mark_stop()\n","\n","print(\"âœ… FedBuff server class defined\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kTxeqal4jP_a","executionInfo":{"status":"ok","timestamp":1764480939093,"user_tz":360,"elapsed":56,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"0a5be7f8-9b11-4df9-b3a4-c47e7189d350"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… All 6 experiment configurations loaded:\n","  Exp1: IID (alpha=1000), no stragglers\n","    - Alpha: 1000.0\n","    - Stragglers: 0%\n","    - Max rounds: 500\n","  Exp2: alpha=0.1, 10% stragglers\n","    - Alpha: 0.1\n","    - Stragglers: 10%\n","    - Max rounds: 500\n","  Exp3: alpha=0.1, 20% stragglers\n","    - Alpha: 0.1\n","    - Stragglers: 20%\n","    - Max rounds: 500\n","  Exp4: alpha=0.1, 30% stragglers\n","    - Alpha: 0.1\n","    - Stragglers: 30%\n","    - Max rounds: 500\n","  Exp5: alpha=0.1, 40% stragglers\n","    - Alpha: 0.1\n","    - Stragglers: 40%\n","    - Max rounds: 500\n","  Exp6: alpha=0.1, 50% stragglers\n","    - Alpha: 0.1\n","    - Stragglers: 50%\n","    - Max rounds: 500\n"]}],"source":["# ========== All 6 Experiment Configurations ==========\n","\n","# Helper function to get paths (works for both Colab and local)\n","def get_paths(exp_id: str):\n","    \"\"\"Get paths for experiment, using Google Drive if in Colab.\"\"\"\n","    if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n","        base = BASE_OUTPUT_DIR\n","    else:\n","        base = Path(\".\")\n","\n","    return {\n","        \"data_dir\": str(DATA_DIR),  # Always local for faster access\n","        \"checkpoints_dir\": str(base / \"checkpoints\" / \"FedBuff\" / exp_id),\n","        \"logs_dir\": str(base / \"logs\" / \"FedBuff\" / exp_id),\n","        \"results_dir\": str(base / \"results\" / \"FedBuff\" / exp_id),\n","    }\n","\n","experiments = {\n","    \"Exp1\": {\n","        \"name\": \"IID (alpha=1000), no stragglers\",\n","        \"data\": {\n","            \"dataset\": \"cifar10\",\n","            \"data_dir\": str(DATA_DIR),  # Use DATA_DIR variable\n","            \"num_classes\": 10\n","        },\n","        \"clients\": {\n","            \"total\": 20,\n","            \"concurrent\": 5,\n","            \"local_epochs\": 1,\n","            \"batch_size\": 128,\n","            \"lr\": 0.005,\n","            \"momentum\": 0.0,\n","            \"weight_decay\": 0.001,\n","            \"grad_clip\": 5.0,\n","            \"struggle_percent\": 0,\n","            \"delay_slow_range\": [0.0, 0.0],\n","            \"delay_fast_range\": [0.0, 0.0],\n","            \"jitter_per_round\": 0.0,\n","            \"fix_delays_per_client\": True\n","        },\n","        \"buff\": {\n","            \"buffer_size\": 5,\n","            \"buffer_timeout_s\": 0.0,\n","            \"use_sample_weighing\": True,\n","            \"eta\": 0.5\n","        },\n","        \"eval\": {\n","            \"interval_seconds\": 1.0,\n","            \"target_accuracy\": 0.8\n","        },\n","        \"train\": {\n","            \"max_rounds\": 500\n","        },\n","        \"partition_alpha\": 1000.0,\n","        \"seed\": 1,\n","        \"server_runtime\": {\n","            \"client_delay\": 0.0\n","        },\n","        \"io\": get_paths(\"Exp1\")\n","    },\n","    \"Exp2\": {\n","        \"name\": \"alpha=0.1, 10% stragglers\",\n","        \"data\": {\n","            \"dataset\": \"cifar10\",\n","            \"data_dir\": str(DATA_DIR),\n","            \"num_classes\": 10\n","        },\n","        \"clients\": {\n","            \"total\": 20,\n","            \"concurrent\": 5,\n","            \"local_epochs\": 1,\n","            \"batch_size\": 128,\n","            \"lr\": 0.005,\n","            \"momentum\": 0.0,\n","            \"weight_decay\": 0.001,\n","            \"grad_clip\": 5.0,\n","            \"struggle_percent\": 10,\n","            \"delay_slow_range\": [0.8, 2.0],\n","            \"delay_fast_range\": [0.0, 0.2],\n","            \"jitter_per_round\": 0.05,\n","            \"fix_delays_per_client\": True\n","        },\n","        \"buff\": {\n","            \"buffer_size\": 5,\n","            \"buffer_timeout_s\": 0.0,\n","            \"use_sample_weighing\": True,\n","            \"eta\": 0.5\n","        },\n","        \"eval\": {\n","            \"interval_seconds\": 1.0,\n","            \"target_accuracy\": 0.8\n","        },\n","        \"train\": {\n","            \"max_rounds\": 500\n","        },\n","        \"partition_alpha\": 0.1,\n","        \"seed\": 1,\n","        \"server_runtime\": {\n","            \"client_delay\": 0.0\n","        },\n","        \"io\": get_paths(\"Exp2\")\n","    },\n","    \"Exp3\": {\n","        \"name\": \"alpha=0.1, 20% stragglers\",\n","        \"data\": {\n","            \"dataset\": \"cifar10\",\n","            \"data_dir\": str(DATA_DIR),\n","            \"num_classes\": 10\n","        },\n","        \"clients\": {\n","            \"total\": 20,\n","            \"concurrent\": 5,\n","            \"local_epochs\": 1,\n","            \"batch_size\": 128,\n","            \"lr\": 0.005,\n","            \"momentum\": 0.0,\n","            \"weight_decay\": 0.001,\n","            \"grad_clip\": 5.0,\n","            \"struggle_percent\": 20,\n","            \"delay_slow_range\": [0.8, 2.0],\n","            \"delay_fast_range\": [0.0, 0.2],\n","            \"jitter_per_round\": 0.05,\n","            \"fix_delays_per_client\": True\n","        },\n","        \"buff\": {\n","            \"buffer_size\": 5,\n","            \"buffer_timeout_s\": 0.0,\n","            \"use_sample_weighing\": True,\n","            \"eta\": 0.5\n","        },\n","        \"eval\": {\n","            \"interval_seconds\": 1.0,\n","            \"target_accuracy\": 0.8\n","        },\n","        \"train\": {\n","            \"max_rounds\": 500\n","        },\n","        \"partition_alpha\": 0.1,\n","        \"seed\": 1,\n","        \"server_runtime\": {\n","            \"client_delay\": 0.0\n","        },\n","        \"io\": get_paths(\"Exp3\")\n","    },\n","    \"Exp4\": {\n","        \"name\": \"alpha=0.1, 30% stragglers\",\n","        \"data\": {\n","            \"dataset\": \"cifar10\",\n","            \"data_dir\": str(DATA_DIR),\n","            \"num_classes\": 10\n","        },\n","        \"clients\": {\n","            \"total\": 20,\n","            \"concurrent\": 5,\n","            \"local_epochs\": 1,\n","            \"batch_size\": 128,\n","            \"lr\": 0.005,\n","            \"momentum\": 0.0,\n","            \"weight_decay\": 0.001,\n","            \"grad_clip\": 5.0,\n","            \"struggle_percent\": 30,\n","            \"delay_slow_range\": [0.8, 2.0],\n","            \"delay_fast_range\": [0.0, 0.2],\n","            \"jitter_per_round\": 0.05,\n","            \"fix_delays_per_client\": True\n","        },\n","        \"buff\": {\n","            \"buffer_size\": 5,\n","            \"buffer_timeout_s\": 0.0,\n","            \"use_sample_weighing\": True,\n","            \"eta\": 0.5\n","        },\n","        \"eval\": {\n","            \"interval_seconds\": 1.0,\n","            \"target_accuracy\": 0.8\n","        },\n","        \"train\": {\n","            \"max_rounds\": 500\n","        },\n","        \"partition_alpha\": 0.1,\n","        \"seed\": 1,\n","        \"server_runtime\": {\n","            \"client_delay\": 0.0\n","        },\n","        \"io\": get_paths(\"Exp4\")\n","    },\n","    \"Exp5\": {\n","        \"name\": \"alpha=0.1, 40% stragglers\",\n","        \"data\": {\n","            \"dataset\": \"cifar10\",\n","            \"data_dir\": str(DATA_DIR),\n","            \"num_classes\": 10\n","        },\n","        \"clients\": {\n","            \"total\": 20,\n","            \"concurrent\": 5,\n","            \"local_epochs\": 1,\n","            \"batch_size\": 128,\n","            \"lr\": 0.005,\n","            \"momentum\": 0.0,\n","            \"weight_decay\": 0.001,\n","            \"grad_clip\": 5.0,\n","            \"struggle_percent\": 40,\n","            \"delay_slow_range\": [0.8, 2.0],\n","            \"delay_fast_range\": [0.0, 0.2],\n","            \"jitter_per_round\": 0.05,\n","            \"fix_delays_per_client\": True\n","        },\n","        \"buff\": {\n","            \"buffer_size\": 5,\n","            \"buffer_timeout_s\": 0.0,\n","            \"use_sample_weighing\": True,\n","            \"eta\": 0.5\n","        },\n","        \"eval\": {\n","            \"interval_seconds\": 1.0,\n","            \"target_accuracy\": 0.8\n","        },\n","        \"train\": {\n","            \"max_rounds\": 500\n","        },\n","        \"partition_alpha\": 0.1,\n","        \"seed\": 1,\n","        \"server_runtime\": {\n","            \"client_delay\": 0.0\n","        },\n","        \"io\": get_paths(\"Exp5\")\n","    },\n","    \"Exp6\": {\n","        \"name\": \"alpha=0.1, 50% stragglers\",\n","        \"data\": {\n","            \"dataset\": \"cifar10\",\n","            \"data_dir\": str(DATA_DIR),\n","            \"num_classes\": 10\n","        },\n","        \"clients\": {\n","            \"total\": 20,\n","            \"concurrent\": 5,\n","            \"local_epochs\": 1,\n","            \"batch_size\": 128,\n","            \"lr\": 0.005,\n","            \"momentum\": 0.0,\n","            \"weight_decay\": 0.001,\n","            \"grad_clip\": 5.0,\n","            \"struggle_percent\": 50,\n","            \"delay_slow_range\": [0.8, 2.0],\n","            \"delay_fast_range\": [0.0, 0.2],\n","            \"jitter_per_round\": 0.05,\n","            \"fix_delays_per_client\": True\n","        },\n","        \"buff\": {\n","            \"buffer_size\": 5,\n","            \"buffer_timeout_s\": 0.0,\n","            \"use_sample_weighing\": True,\n","            \"eta\": 0.5\n","        },\n","        \"eval\": {\n","            \"interval_seconds\": 1.0,\n","            \"target_accuracy\": 0.8\n","        },\n","        \"train\": {\n","            \"max_rounds\": 500\n","        },\n","        \"partition_alpha\": 0.1,\n","        \"seed\": 1,\n","        \"server_runtime\": {\n","            \"client_delay\": 0.0\n","        },\n","        \"io\": get_paths(\"Exp6\")\n","    }\n","}\n","\n","print(\"âœ… All 6 experiment configurations loaded:\")\n","for exp_id, exp_config in experiments.items():\n","    print(f\"  {exp_id}: {exp_config['name']}\")\n","    print(f\"    - Alpha: {exp_config['partition_alpha']}\")\n","    print(f\"    - Stragglers: {exp_config['clients']['struggle_percent']}%\")\n","    print(f\"    - Max rounds: {exp_config['train']['max_rounds']}\")\n"]},{"cell_type":"markdown","metadata":{"id":"0qLA27mEjP_b"},"source":["## 7. Run Single Experiment (Helper Function)\n","\n","Use this function to run a single experiment by ID (Exp1-Exp6).\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jiFFTEVOjP_b","executionInfo":{"status":"ok","timestamp":1764480939134,"user_tz":360,"elapsed":38,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"85537070-3029-4144-9afa-8197ffe44dcf"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Helper function `run_single_experiment()` defined\n","   Usage: run_dir = run_single_experiment('Exp1', experiments)\n"]}],"source":["def run_single_experiment(exp_id: str, experiments_dict: dict):\n","    \"\"\"\n","    Run a single FedBuff experiment.\n","\n","    Args:\n","        exp_id: Experiment ID (e.g., \"Exp1\", \"Exp2\", ..., \"Exp6\")\n","        experiments_dict: Dictionary containing all experiment configs\n","    \"\"\"\n","    if exp_id not in experiments_dict:\n","        print(f\"âŒ Error: Experiment {exp_id} not found!\")\n","        return None\n","\n","    config = experiments_dict[exp_id]\n","    print(f\"\\n{'='*70}\")\n","    print(f\"Running {exp_id}: {config['name']}\")\n","    print(f\"{'='*70}\")\n","\n","    # Set random seed\n","    seed = int(config.get(\"seed\", 42))\n","    set_seed(seed)\n","    random.seed(seed)\n","\n","    # Create timestamped run folder\n","    run_dir = Path(config[\"io\"][\"logs_dir\"]) / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n","    run_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # Write COMMIT.txt\n","    commit_hash = \"notebook_run\"\n","    csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n","    with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n","        f.write(f\"{commit_hash},{csv_header}\\n\")\n","\n","    # Save config to run folder\n","    with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n","        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n","\n","    print(f\"âœ… Run folder: {run_dir}\")\n","\n","    # Load and partition data\n","    dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n","    dd.distribute_data(\n","        num_clients=int(config[\"clients\"][\"total\"]),\n","        alpha=float(config[\"partition_alpha\"]),\n","        seed=seed\n","    )\n","    print(f\"âœ… Data partitioned: {config['clients']['total']} clients, alpha={config['partition_alpha']}\")\n","\n","    # Build global model\n","    global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n","\n","    # Initialize server\n","    server = BufferedFedServer(\n","        global_model=global_model,\n","        total_train_samples=len(dd.train_dataset),\n","        buffer_size=int(config[\"buff\"][\"buffer_size\"]),\n","        buffer_timeout_s=float(config[\"buff\"][\"buffer_timeout_s\"]),\n","        use_sample_weighing=bool(config[\"buff\"][\"use_sample_weighing\"]),\n","        target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n","        max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n","        eval_interval_s=int(config[\"eval\"][\"interval_seconds\"]),\n","        data_dir=config[\"data\"][\"data_dir\"],\n","        checkpoints_dir=str(run_dir / \"checkpoints\"),\n","        logs_dir=str(run_dir),\n","        global_log_csv=str(run_dir / \"FedBuff.csv\"),\n","        client_participation_csv=str(run_dir / \"FedBuffClientParticipation.csv\"),\n","        final_model_path=str(run_dir / \"FedBuffModel.pt\"),\n","        resume=False,\n","        device=get_device(),\n","        eta=float(config[\"buff\"].get(\"eta\", 0.5)),\n","    )\n","    print(f\"âœ… Server initialized (device: {server.device})\")\n","\n","    # Setup clients\n","    n = int(config[\"clients\"][\"total\"])\n","    pct = max(0, min(100, int(config[\"clients\"].get(\"struggle_percent\", 0))))\n","    k_slow = (n * pct) // 100\n","    slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n","\n","    a_s, b_s = config[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n","    a_f, b_f = config[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n","    fix_delays = bool(config[\"clients\"].get(\"fix_delays_per_client\", True))\n","    jitter = float(config[\"clients\"].get(\"jitter_per_round\", 0.0))\n","\n","    per_client_base_delay: Dict[int, float] = {}\n","    if fix_delays:\n","        for cid in range(n):\n","            if cid in slow_ids:\n","                per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n","            else:\n","                per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n","\n","    clients = []\n","    for cid in range(n):\n","        subset = dd.get_client_data(cid)\n","        base_delay = per_client_base_delay.get(cid, 0.0)\n","        is_slow = cid in slow_ids\n","        clients.append(LocalBuffClient(\n","            cid=cid,\n","            cfg=config,\n","            subset=subset,\n","            work_dir=str(run_dir / \"checkpoints\" / \"clients\"),\n","            base_delay=base_delay,\n","            slow=is_slow,\n","            delay_ranges=((float(a_s), float(b_s)), (float(a_f), float(b_f))),\n","            jitter=jitter,\n","            fix_delay=fix_delays,\n","        ))\n","\n","    print(f\"âœ… Created {len(clients)} clients ({len(slow_ids)} slow, {n - len(slow_ids)} fast)\")\n","\n","    # Start experiment\n","    server.start_eval_timer()\n","    sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n","\n","    def client_loop(client: LocalBuffClient):\n","        while True:\n","            with sem:\n","                cont = client.fit_once(server)\n","            if not cont:\n","                break\n","            time.sleep(0.05)\n","\n","    threads = []\n","    for cl in clients:\n","        t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n","        t.start()\n","        threads.append(t)\n","\n","    print(f\"âœ… Started {len(threads)} client threads\")\n","    print(f\"ðŸš€ Experiment running... (max {config['train']['max_rounds']} rounds)\")\n","\n","    # Wait for completion\n","    server.wait()\n","    for t in threads:\n","        t.join()\n","\n","    print(f\"\\nâœ… {exp_id} completed!\")\n","    print(f\"ðŸ“ Results: {run_dir}\")\n","\n","    return run_dir\n","\n","print(\"âœ… Helper function `run_single_experiment()` defined\")\n","print(\"   Usage: run_dir = run_single_experiment('Exp1', experiments)\")\n"]},{"cell_type":"markdown","metadata":{"id":"Ngrt8g8JjP_b"},"source":["## 8. Run All 6 Experiments Sequentially\n","\n","This cell runs all experiments one by one. Each experiment saves to its own folder.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"mU9bgIBmjP_b","executionInfo":{"status":"error","timestamp":1764482206532,"user_tz":360,"elapsed":1267397,"user":{"displayName":"Avinash","userId":"15681946276512401675"}},"outputId":"697c9227-edd5-4c7d-9e66-4e5ebf8fa04b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","STARTING ALL 6 EXPERIMENTS\n","======================================================================\n","\n","======================================================================\n","Running Exp1: IID (alpha=1000), no stragglers\n","======================================================================\n","âœ… Run folder: /content/drive/MyDrive/colab/dml_project/logs/FedBuff/Exp1/run_20251130_053538\n","âœ… Data partitioned: 20 clients, alpha=1000.0\n","âœ… Server initialized (device: cuda)\n","âœ… Created 20 clients (0 slow, 20 fast)\n","âœ… Started 20 client threads\n","ðŸš€ Experiment running... (max 500 rounds)\n","[LOG] total_agg=0 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=2.4518 test_acc=0.1214 time=45.4s\n","[LOG] total_agg=0 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=2.4518 test_acc=0.1214 time=63.8s\n","[LOG] total_agg=4 avg_train_loss=2.3119 avg_train_acc=0.1181 test_loss=2.2937 test_acc=0.1244 time=67.1s\n","[LOG] total_agg=5 avg_train_loss=2.3144 avg_train_acc=0.1116 test_loss=2.2909 test_acc=0.1247 time=78.8s\n","[LOG] total_agg=5 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=2.2909 test_acc=0.1247 time=95.7s\n","[LOG] total_agg=7 avg_train_loss=2.2629 avg_train_acc=0.1567 test_loss=2.2598 test_acc=0.1568 time=105.6s\n","[LOG] total_agg=9 avg_train_loss=2.2713 avg_train_acc=0.1449 test_loss=2.2492 test_acc=0.1769 time=125.0s\n","[LOG] total_agg=10 avg_train_loss=2.2709 avg_train_acc=0.1454 test_loss=2.2471 test_acc=0.1718 time=139.0s\n","[LOG] total_agg=12 avg_train_loss=2.2421 avg_train_acc=0.1758 test_loss=2.2210 test_acc=0.1818 time=146.5s\n","[LOG] total_agg=14 avg_train_loss=2.2335 avg_train_acc=0.1729 test_loss=2.2081 test_acc=0.1956 time=167.1s\n","[LOG] total_agg=15 avg_train_loss=2.2330 avg_train_acc=0.1786 test_loss=2.2069 test_acc=0.1934 time=180.4s\n","[LOG] total_agg=17 avg_train_loss=2.2065 avg_train_acc=0.1927 test_loss=2.1771 test_acc=0.2059 time=184.2s\n","[LOG] total_agg=19 avg_train_loss=2.2154 avg_train_acc=0.1824 test_loss=2.1698 test_acc=0.2031 time=203.1s\n","[LOG] total_agg=19 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=2.1698 test_acc=0.2031 time=221.7s\n","[LOG] total_agg=22 avg_train_loss=2.1775 avg_train_acc=0.2085 test_loss=2.1059 test_acc=0.2166 time=227.7s\n","[LOG] total_agg=23 avg_train_loss=2.1785 avg_train_acc=0.2123 test_loss=2.0971 test_acc=0.2268 time=246.2s\n","[LOG] total_agg=24 avg_train_loss=2.1674 avg_train_acc=0.2026 test_loss=2.0961 test_acc=0.2170 time=258.7s\n","[LOG] total_agg=27 avg_train_loss=2.1309 avg_train_acc=0.2256 test_loss=2.0380 test_acc=0.2558 time=267.0s\n","[LOG] total_agg=28 avg_train_loss=2.1263 avg_train_acc=0.2152 test_loss=2.0460 test_acc=0.2400 time=279.5s\n","[LOG] total_agg=29 avg_train_loss=2.1234 avg_train_acc=0.2127 test_loss=2.0304 test_acc=0.2426 time=290.4s\n","[LOG] total_agg=31 avg_train_loss=2.0992 avg_train_acc=0.2256 test_loss=1.9990 test_acc=0.2550 time=303.4s\n","[LOG] total_agg=32 avg_train_loss=2.0973 avg_train_acc=0.2344 test_loss=1.9957 test_acc=0.2454 time=320.1s\n","[LOG] total_agg=34 avg_train_loss=2.0749 avg_train_acc=0.2442 test_loss=1.9804 test_acc=0.2611 time=330.3s\n","[LOG] total_agg=35 avg_train_loss=2.0690 avg_train_acc=0.2456 test_loss=1.9829 test_acc=0.2606 time=338.1s\n","[LOG] total_agg=37 avg_train_loss=2.0614 avg_train_acc=0.2496 test_loss=1.9466 test_acc=0.2759 time=350.5s\n","[LOG] total_agg=38 avg_train_loss=2.0495 avg_train_acc=0.2491 test_loss=1.9412 test_acc=0.2808 time=367.3s\n","[LOG] total_agg=39 avg_train_loss=2.0561 avg_train_acc=0.2496 test_loss=1.9395 test_acc=0.2741 time=373.1s\n","[LOG] total_agg=41 avg_train_loss=2.0543 avg_train_acc=0.2559 test_loss=1.9603 test_acc=0.2621 time=385.1s\n","[LOG] total_agg=42 avg_train_loss=2.0389 avg_train_acc=0.2622 test_loss=1.9429 test_acc=0.2688 time=394.6s\n","[LOG] total_agg=43 avg_train_loss=2.0251 avg_train_acc=0.2540 test_loss=1.9151 test_acc=0.2893 time=409.1s\n","[LOG] total_agg=44 avg_train_loss=2.0340 avg_train_acc=0.2803 test_loss=1.9100 test_acc=0.2881 time=419.4s\n","[LOG] total_agg=44 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.9100 test_acc=0.2881 time=423.2s\n","[LOG] total_agg=47 avg_train_loss=2.0228 avg_train_acc=0.2697 test_loss=1.9024 test_acc=0.2910 time=429.6s\n","[LOG] total_agg=48 avg_train_loss=2.0087 avg_train_acc=0.2784 test_loss=1.9046 test_acc=0.2813 time=437.0s\n","[LOG] total_agg=49 avg_train_loss=2.0200 avg_train_acc=0.2737 test_loss=1.8913 test_acc=0.2942 time=458.4s\n","[LOG] total_agg=50 avg_train_loss=2.0308 avg_train_acc=0.2551 test_loss=1.8865 test_acc=0.3040 time=466.2s\n","[LOG] total_agg=53 avg_train_loss=1.9996 avg_train_acc=0.2833 test_loss=1.8791 test_acc=0.3063 time=479.4s\n","[LOG] total_agg=53 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.8791 test_acc=0.3063 time=499.4s\n","[LOG] total_agg=55 avg_train_loss=1.9936 avg_train_acc=0.2824 test_loss=1.8662 test_acc=0.3021 time=506.9s\n","[LOG] total_agg=58 avg_train_loss=1.9887 avg_train_acc=0.2897 test_loss=1.9101 test_acc=0.2746 time=518.8s\n","[LOG] total_agg=58 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.9101 test_acc=0.2746 time=533.9s\n","[LOG] total_agg=60 avg_train_loss=1.9843 avg_train_acc=0.2839 test_loss=1.8448 test_acc=0.3116 time=551.8s\n","[LOG] total_agg=62 avg_train_loss=1.9842 avg_train_acc=0.2908 test_loss=1.8251 test_acc=0.3275 time=562.2s\n","[LOG] total_agg=64 avg_train_loss=1.9719 avg_train_acc=0.2936 test_loss=1.8339 test_acc=0.3269 time=570.0s\n","[LOG] total_agg=65 avg_train_loss=1.9584 avg_train_acc=0.2872 test_loss=1.8294 test_acc=0.3200 time=591.4s\n","[LOG] total_agg=66 avg_train_loss=1.9838 avg_train_acc=0.2937 test_loss=1.8424 test_acc=0.3152 time=599.1s\n","[LOG] total_agg=69 avg_train_loss=1.9643 avg_train_acc=0.2992 test_loss=1.8060 test_acc=0.3380 time=610.8s\n","[LOG] total_agg=69 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.8060 test_acc=0.3380 time=626.5s\n","[LOG] total_agg=71 avg_train_loss=1.9713 avg_train_acc=0.2918 test_loss=1.8414 test_acc=0.3153 time=644.6s\n","[LOG] total_agg=73 avg_train_loss=1.9460 avg_train_acc=0.3115 test_loss=1.8355 test_acc=0.3219 time=654.7s\n","[LOG] total_agg=75 avg_train_loss=1.9568 avg_train_acc=0.3063 test_loss=1.8032 test_acc=0.3373 time=662.6s\n","[LOG] total_agg=76 avg_train_loss=1.9494 avg_train_acc=0.3031 test_loss=1.7949 test_acc=0.3406 time=684.3s\n","[LOG] total_agg=77 avg_train_loss=1.9574 avg_train_acc=0.3082 test_loss=1.7847 test_acc=0.3486 time=695.9s\n","[LOG] total_agg=79 avg_train_loss=1.9460 avg_train_acc=0.3077 test_loss=1.7931 test_acc=0.3378 time=704.1s\n","[LOG] total_agg=81 avg_train_loss=1.9584 avg_train_acc=0.3007 test_loss=1.8053 test_acc=0.3269 time=724.6s\n","[LOG] total_agg=82 avg_train_loss=1.9344 avg_train_acc=0.3121 test_loss=1.7874 test_acc=0.3342 time=737.7s\n","[LOG] total_agg=85 avg_train_loss=1.9262 avg_train_acc=0.3216 test_loss=1.7940 test_acc=0.3244 time=745.4s\n","[LOG] total_agg=86 avg_train_loss=1.9335 avg_train_acc=0.3208 test_loss=1.7903 test_acc=0.3387 time=753.3s\n","[LOG] total_agg=87 avg_train_loss=1.9192 avg_train_acc=0.3253 test_loss=1.7859 test_acc=0.3433 time=774.9s\n","[LOG] total_agg=88 avg_train_loss=1.9349 avg_train_acc=0.3283 test_loss=1.7613 test_acc=0.3599 time=787.0s\n","[LOG] total_agg=88 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.7613 test_acc=0.3599 time=790.8s\n","[LOG] total_agg=91 avg_train_loss=1.9292 avg_train_acc=0.3128 test_loss=1.7647 test_acc=0.3426 time=793.9s\n","[LOG] total_agg=92 avg_train_loss=1.9447 avg_train_acc=0.3172 test_loss=1.7602 test_acc=0.3501 time=813.1s\n","[LOG] total_agg=92 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.7602 test_acc=0.3501 time=831.1s\n","[LOG] total_agg=95 avg_train_loss=1.9152 avg_train_acc=0.3304 test_loss=1.7511 test_acc=0.3554 time=837.3s\n","[LOG] total_agg=97 avg_train_loss=1.9216 avg_train_acc=0.3135 test_loss=1.7673 test_acc=0.3518 time=848.6s\n","[LOG] total_agg=97 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.7673 test_acc=0.3518 time=865.2s\n","[LOG] total_agg=99 avg_train_loss=1.9136 avg_train_acc=0.3255 test_loss=1.7695 test_acc=0.3528 time=876.1s\n","[LOG] total_agg=101 avg_train_loss=1.9275 avg_train_acc=0.3191 test_loss=1.7731 test_acc=0.3453 time=890.9s\n","[LOG] total_agg=102 avg_train_loss=1.9056 avg_train_acc=0.3315 test_loss=1.7642 test_acc=0.3509 time=902.3s\n","[LOG] total_agg=104 avg_train_loss=1.9073 avg_train_acc=0.3320 test_loss=1.7412 test_acc=0.3568 time=916.4s\n","[LOG] total_agg=105 avg_train_loss=1.8853 avg_train_acc=0.3484 test_loss=1.7355 test_acc=0.3585 time=933.6s\n","[LOG] total_agg=107 avg_train_loss=1.9093 avg_train_acc=0.3231 test_loss=1.7342 test_acc=0.3595 time=939.1s\n","[LOG] total_agg=109 avg_train_loss=1.9083 avg_train_acc=0.3226 test_loss=1.7254 test_acc=0.3629 time=964.5s\n","[LOG] total_agg=109 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.7254 test_acc=0.3629 time=979.0s\n","[LOG] total_agg=113 avg_train_loss=1.8996 avg_train_acc=0.3365 test_loss=1.7119 test_acc=0.3696 time=999.8s\n","[LOG] total_agg=114 avg_train_loss=1.8964 avg_train_acc=0.3345 test_loss=1.7045 test_acc=0.3728 time=1011.4s\n","[LOG] total_agg=117 avg_train_loss=1.8928 avg_train_acc=0.3382 test_loss=1.7334 test_acc=0.3672 time=1019.2s\n","[LOG] total_agg=118 avg_train_loss=1.8950 avg_train_acc=0.3297 test_loss=1.7382 test_acc=0.3620 time=1040.8s\n","[LOG] total_agg=119 avg_train_loss=1.8777 avg_train_acc=0.3270 test_loss=1.6990 test_acc=0.3758 time=1048.7s\n","[LOG] total_agg=121 avg_train_loss=1.8822 avg_train_acc=0.3457 test_loss=1.7218 test_acc=0.3649 time=1060.6s\n","[LOG] total_agg=122 avg_train_loss=1.9043 avg_train_acc=0.3353 test_loss=1.7175 test_acc=0.3618 time=1076.1s\n","[LOG] total_agg=124 avg_train_loss=1.8768 avg_train_acc=0.3464 test_loss=1.6976 test_acc=0.3816 time=1087.4s\n","[LOG] total_agg=126 avg_train_loss=1.8756 avg_train_acc=0.3576 test_loss=1.6902 test_acc=0.3815 time=1101.4s\n","[LOG] total_agg=127 avg_train_loss=1.8760 avg_train_acc=0.3459 test_loss=1.7015 test_acc=0.3741 time=1117.0s\n","[LOG] total_agg=129 avg_train_loss=1.8804 avg_train_acc=0.3440 test_loss=1.6848 test_acc=0.3788 time=1128.2s\n","[LOG] total_agg=131 avg_train_loss=1.8803 avg_train_acc=0.3511 test_loss=1.6850 test_acc=0.3803 time=1136.3s\n","[LOG] total_agg=132 avg_train_loss=1.8923 avg_train_acc=0.3436 test_loss=1.6918 test_acc=0.3844 time=1156.7s\n","[LOG] total_agg=133 avg_train_loss=1.8445 avg_train_acc=0.3729 test_loss=1.6655 test_acc=0.4003 time=1169.6s\n","[LOG] total_agg=135 avg_train_loss=1.8659 avg_train_acc=0.3501 test_loss=1.6735 test_acc=0.3832 time=1177.5s\n","[LOG] total_agg=137 avg_train_loss=1.8646 avg_train_acc=0.3529 test_loss=1.7046 test_acc=0.3712 time=1191.6s\n","[LOG] total_agg=138 avg_train_loss=1.8593 avg_train_acc=0.3475 test_loss=1.6732 test_acc=0.3872 time=1208.1s\n","[LOG] total_agg=140 avg_train_loss=1.8593 avg_train_acc=0.3506 test_loss=1.6775 test_acc=0.3857 time=1218.4s\n","[LOG] total_agg=140 avg_train_loss=0.0000 avg_train_acc=0.0000 test_loss=1.6775 test_acc=0.3857 time=1222.2s\n","[LOG] total_agg=143 avg_train_loss=1.8706 avg_train_acc=0.3451 test_loss=1.6566 test_acc=0.3952 time=1233.8s\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1617199195.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1617199195.py\u001b[0m in \u001b[0;36mshould_stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1128573943.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mexp_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mrun_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_single_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             experiment_results[exp_id] = {\n","\u001b[0;32m/tmp/ipython-input-2146696397.py\u001b[0m in \u001b[0;36mrun_single_experiment\u001b[0;34m(exp_id, experiments_dict)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Wait for completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1617199195.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âœ… FedBuff server class defined\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1617199195.py\u001b[0m in \u001b[0;36mmark_stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmark_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Run all 6 experiments sequentially\n","experiment_results = {}\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"STARTING ALL 6 EXPERIMENTS\")\n","print(\"=\"*70)\n","\n","total_start = time.time()\n","\n","for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n","    exp_start = time.time()\n","    try:\n","        run_dir = run_single_experiment(exp_id, experiments)\n","        if run_dir:\n","            experiment_results[exp_id] = {\n","                \"run_dir\": run_dir,\n","                \"status\": \"completed\",\n","                \"duration_min\": (time.time() - exp_start) / 60.0\n","            }\n","            print(f\"âœ… {exp_id} completed in {experiment_results[exp_id]['duration_min']:.2f} minutes\")\n","        else:\n","            experiment_results[exp_id] = {\"status\": \"failed\", \"duration_min\": (time.time() - exp_start) / 60.0}\n","    except Exception as e:\n","        print(f\"âŒ {exp_id} failed: {str(e)}\")\n","        experiment_results[exp_id] = {\"status\": \"error\", \"error\": str(e), \"duration_min\": (time.time() - exp_start) / 60.0}\n","\n","    print(f\"\\n{'â”€'*70}\\n\")\n","\n","total_duration = (time.time() - total_start) / 60.0\n","\n","print(\"=\"*70)\n","print(\"ALL EXPERIMENTS COMPLETED\")\n","print(\"=\"*70)\n","print(f\"Total time: {total_duration:.2f} minutes ({total_duration/60:.2f} hours)\")\n","print(\"\\nResults summary:\")\n","for exp_id, result in experiment_results.items():\n","    if result.get(\"status\") == \"completed\":\n","        print(f\"  {exp_id}: âœ… {result['duration_min']:.2f} min -> {result['run_dir']}\")\n","    else:\n","        print(f\"  {exp_id}: âŒ {result.get('status', 'unknown')}\")\n","print(\"=\"*70)\n"]},{"cell_type":"markdown","metadata":{"id":"JVaw7za7jP_b"},"source":["## 9. Compare All Experiments (After Running)\n","\n","This section creates comparison plots across all 6 experiments.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1EGGvQjzjP_b","executionInfo":{"status":"aborted","timestamp":1764482206529,"user_tz":360,"elapsed":1315560,"user":{"displayName":"Avinash","userId":"15681946276512401675"}}},"outputs":[],"source":["# Compare all experiments (load results from experiment_results)\n","if 'experiment_results' in globals() and len(experiment_results) > 0:\n","    # Load all CSV files\n","    all_data = {}\n","    for exp_id, result in experiment_results.items():\n","        if result.get(\"status\") == \"completed\":\n","            csv_path = result[\"run_dir\"] / \"FedBuff.csv\"\n","            if csv_path.exists():\n","                df = pd.read_csv(csv_path)\n","                df['time_min'] = df['time'] / 60.0\n","                all_data[exp_id] = df\n","                print(f\"âœ… Loaded {exp_id}: {len(df)} rows\")\n","\n","    if len(all_data) > 0:\n","        # Create comparison plots\n","        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n","\n","        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n","\n","        # Plot 1: Accuracy vs Rounds\n","        for i, (exp_id, df) in enumerate(all_data.items()):\n","            axes[0, 0].plot(df['total_agg'], df['test_acc'],\n","                          label=f\"{exp_id} ({experiments[exp_id]['name']})\",\n","                          linewidth=2, color=colors[i % len(colors)])\n","        axes[0, 0].set_xlabel('Round', fontsize=12)\n","        axes[0, 0].set_ylabel('Test Accuracy', fontsize=12)\n","        axes[0, 0].set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n","        axes[0, 0].legend(fontsize=9, loc='lower right')\n","        axes[0, 0].grid(True, alpha=0.3)\n","        axes[0, 0].set_ylim([0, 1.0])\n","\n","        # Plot 2: Accuracy vs Wall Clock Time\n","        for i, (exp_id, df) in enumerate(all_data.items()):\n","            axes[0, 1].plot(df['time_min'], df['test_acc'],\n","                          label=f\"{exp_id}\",\n","                          linewidth=2, color=colors[i % len(colors)])\n","        axes[0, 1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n","        axes[0, 1].set_ylabel('Test Accuracy', fontsize=12)\n","        axes[0, 1].set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n","        axes[0, 1].legend(fontsize=9, loc='lower right')\n","        axes[0, 1].grid(True, alpha=0.3)\n","        axes[0, 1].set_ylim([0, 1.0])\n","\n","        # Plot 3: Best Accuracy by Experiment\n","        best_accs = {exp_id: df['test_acc'].max() for exp_id, df in all_data.items()}\n","        exp_names = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\"\n","                    if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\"\n","                    for exp_id in best_accs.keys()]\n","        axes[1, 0].bar(range(len(best_accs)), list(best_accs.values()),\n","                      color=colors[:len(best_accs)], alpha=0.7, edgecolor='black')\n","        axes[1, 0].set_xticks(range(len(best_accs)))\n","        axes[1, 0].set_xticklabels(exp_names, fontsize=9, rotation=45, ha='right')\n","        axes[1, 0].set_ylabel('Best Test Accuracy', fontsize=12)\n","        axes[1, 0].set_title('Best Test Accuracy by Experiment', fontsize=14, fontweight='bold')\n","        axes[1, 0].grid(True, alpha=0.3, axis='y')\n","        axes[1, 0].set_ylim([0, 1.0])\n","        # Add value labels\n","        for i, (exp_id, acc) in enumerate(best_accs.items()):\n","            axes[1, 0].text(i, acc, f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n","\n","        # Plot 4: Time to reach 50% accuracy\n","        times_to_50 = {}\n","        for exp_id, df in all_data.items():\n","            mask = df['test_acc'] >= 0.5\n","            if mask.any():\n","                first_idx = mask.idxmax()\n","                times_to_50[exp_id] = df.loc[first_idx, 'time_min']\n","            else:\n","                times_to_50[exp_id] = None\n","\n","        valid_times = {k: v for k, v in times_to_50.items() if v is not None}\n","        if len(valid_times) > 0:\n","            exp_names_50 = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\"\n","                           if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\"\n","                           for exp_id in valid_times.keys()]\n","            axes[1, 1].bar(range(len(valid_times)), list(valid_times.values()),\n","                         color=colors[:len(valid_times)], alpha=0.7, edgecolor='black')\n","            axes[1, 1].set_xticks(range(len(valid_times)))\n","            axes[1, 1].set_xticklabels(exp_names_50, fontsize=9, rotation=45, ha='right')\n","            axes[1, 1].set_ylabel('Time to Reach 50% Accuracy (minutes)', fontsize=12)\n","            axes[1, 1].set_title('Convergence Speed: Time to 50% Accuracy', fontsize=14, fontweight='bold')\n","            axes[1, 1].grid(True, alpha=0.3, axis='y')\n","            # Add value labels\n","            for i, (exp_id, t) in enumerate(valid_times.items()):\n","                axes[1, 1].text(i, t, f'{t:.1f}m', ha='center', va='bottom', fontsize=9)\n","\n","        plt.tight_layout()\n","        # Use Google Drive path if in Colab, otherwise local\n","        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n","            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"FedBuff\" / \"comparisons\"\n","        else:\n","            comparison_dir = Path(\"./logs/FedBuff/comparisons\")\n","        comparison_dir.mkdir(parents=True, exist_ok=True)\n","        plt.savefig(comparison_dir / \"all_experiments_comparison.png\", dpi=150, bbox_inches='tight')\n","        print(f\"\\nâœ… Comparison plot saved: {comparison_dir / 'all_experiments_comparison.png'}\")\n","        plt.show()\n","\n","        # Print summary table\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"EXPERIMENT COMPARISON SUMMARY\")\n","        print(\"=\"*80)\n","        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n","        print(\"-\"*80)\n","        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n","            if exp_id in all_data:\n","                df = all_data[exp_id]\n","                cfg = experiments[exp_id]\n","                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n","                      f\"{df['test_acc'].max():<10.4f} {df['test_acc'].iloc[-1]:<11.4f} \"\n","                      f\"{df['time_min'].iloc[-1]:<12.2f}\")\n","        print(\"=\"*80)\n","    else:\n","        print(\"âš ï¸  No completed experiments found. Run experiments first.\")\n","else:\n","    print(\"âš ï¸  No experiment results found. Run the experiments first using Section 8.\")\n"]},{"cell_type":"markdown","metadata":{"id":"4LUUp3DrjP_d"},"source":["## 12. View Results\n"]},{"cell_type":"code","source":["# Load and display results for all 6 experiments\n","if 'experiment_results' in globals() and len(experiment_results) > 0:\n","    print(\"=\"*80)\n","    print(\"FEDBUFF RESULTS - ALL 6 EXPERIMENTS\")\n","    print(\"=\"*80)\n","\n","    all_results = {}\n","\n","    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n","        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n","            run_dir = experiment_results[exp_id][\"run_dir\"]\n","            csv_path = run_dir / \"FedBuff.csv\"\n","\n","            if csv_path.exists():\n","                df = pd.read_csv(csv_path)\n","                all_results[exp_id] = df\n","\n","                exp_config = experiments[exp_id]\n","                print(f\"\\n{'='*80}\")\n","                print(f\"{exp_id}: {exp_config['name']}\")\n","                print(f\"{'='*80}\")\n","                print(f\"  Alpha: {exp_config['partition_alpha']}\")\n","                print(f\"  Stragglers: {exp_config['clients']['struggle_percent']}%\")\n","                print(f\"  Max rounds: {exp_config['train']['max_rounds']}\")\n","                print(f\"\\n  Total rounds completed: {df['total_agg'].max()}\")\n","\n","                final_row = df.iloc[-1]\n","                print(f\"\\n  Final metrics:\")\n","                print(f\"    - Test accuracy: {final_row['test_acc']:.4f}\")\n","                print(f\"    - Train accuracy: {final_row['avg_train_acc']:.4f}\")\n","                print(f\"    - Test loss: {final_row['test_loss']:.4f}\")\n","                print(f\"    - Total time: {final_row['time']:.1f} seconds ({final_row['time']/60:.2f} minutes)\")\n","\n","                best_acc = df['test_acc'].max()\n","                best_round = df.loc[df['test_acc'].idxmax(), 'total_agg']\n","                print(f\"\\n  Best test accuracy: {best_acc:.4f} (round {best_round})\")\n","\n","                print(f\"\\n  Run folder: {run_dir}\")\n","            else:\n","                print(f\"\\n{'='*80}\")\n","                print(f\"{exp_id}: Results file not found\")\n","                print(f\"  Run folder: {run_dir}\")\n","        else:\n","            status = experiment_results.get(exp_id, {}).get(\"status\", \"not run\")\n","            print(f\"\\n{'='*80}\")\n","            print(f\"{exp_id}: {status.upper()}\")\n","            if status == \"error\":\n","                print(f\"  Error: {experiment_results[exp_id].get('error', 'Unknown error')}\")\n","\n","    # Summary table\n","    if len(all_results) > 0:\n","        print(f\"\\n{'='*80}\")\n","        print(\"SUMMARY TABLE\")\n","        print(\"=\"*80)\n","        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Rounds':<8} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n","        print(\"-\"*80)\n","\n","        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n","            if exp_id in all_results:\n","                df = all_results[exp_id]\n","                cfg = experiments[exp_id]\n","                final_row = df.iloc[-1]\n","                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n","                      f\"{df['total_agg'].max():<8} {df['test_acc'].max():<10.4f} {final_row['test_acc']:<11.4f} \"\n","                      f\"{final_row['time']/60:<12.2f}\")\n","\n","        print(\"=\"*80)\n","\n","        # Show first and last rows for each experiment\n","        print(f\"\\n{'='*80}\")\n","        print(\"DETAILED DATA PREVIEW\")\n","        print(\"=\"*80)\n","\n","        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n","            if exp_id in all_results:\n","                df = all_results[exp_id]\n","                exp_config = experiments[exp_id]\n","                print(f\"\\n{exp_id}: {exp_config['name']}\")\n","                print(f\"  First 3 rows:\")\n","                print(df.head(3).to_string(index=False))\n","                print(f\"\\n  Last 3 rows:\")\n","                print(df.tail(3).to_string(index=False))\n","                print()\n","    else:\n","        print(\"\\nâš ï¸  No completed experiments found. Run experiments first using Section 8.\")\n","else:\n","    print(\"âš ï¸  No experiment results found. Run experiments first using Section 8.\")\n","    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")"],"metadata":{"id":"bZbcg1a3_7RQ","executionInfo":{"status":"aborted","timestamp":1764482206534,"user_tz":360,"elapsed":1315564,"user":{"displayName":"Avinash","userId":"15681946276512401675"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oK25QqIQjP_j"},"source":["## 13. Comprehensive Results Visualization\n"]},{"cell_type":"code","source":["# Comprehensive visualization suite for all 6 experiments\n","if 'experiment_results' in globals() and len(experiment_results) > 0:\n","    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n","\n","    all_data = {}\n","    all_participation = {}\n","\n","    # Load all experiment data\n","    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n","        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n","            run_dir = experiment_results[exp_id][\"run_dir\"]\n","            csv_path = run_dir / \"FedBuff.csv\"\n","            participation_path = run_dir / \"FedBuffClientParticipation.csv\"\n","\n","            if csv_path.exists():\n","                df = pd.read_csv(csv_path)\n","                df['time_min'] = df['time'] / 60.0\n","                all_data[exp_id] = {\n","                    'df': df,\n","                    'run_dir': run_dir,\n","                    'config': experiments[exp_id]\n","                }\n","\n","                if participation_path.exists():\n","                    part_df = pd.read_csv(participation_path)\n","                    all_participation[exp_id] = part_df\n","\n","    if len(all_data) > 0:\n","        print(f\"âœ… Loaded data for {len(all_data)} experiments\")\n","        print(\"=\"*80)\n","\n","        # Generate individual plots for each experiment\n","        for exp_id, exp_data in all_data.items():\n","            df = exp_data['df']\n","            run_dir = exp_data['run_dir']\n","            exp_config = exp_data['config']\n","            participation_path = run_dir / \"FedBuffClientParticipation.csv\"\n","\n","            print(f\"\\nðŸ“Š Generating plots for {exp_id}: {exp_config['name']}\")\n","\n","            # Plot 1: Accuracy vs Rounds\n","            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","            axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n","            train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n","            train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n","            if len(train_acc) > 0:\n","                axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n","            axes[0].set_xlabel('Round', fontsize=12)\n","            axes[0].set_ylabel('Accuracy', fontsize=12)\n","            axes[0].set_title(f'{exp_id}: Accuracy vs Rounds', fontsize=14, fontweight='bold')\n","            axes[0].grid(True, alpha=0.3)\n","            axes[0].legend(fontsize=10)\n","            axes[0].set_ylim([0, 1.0])\n","\n","            axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n","            train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n","            train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n","            if len(train_loss) > 0:\n","                axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n","            axes[1].set_xlabel('Round', fontsize=12)\n","            axes[1].set_ylabel('Loss', fontsize=12)\n","            axes[1].set_title(f'{exp_id}: Loss vs Rounds', fontsize=14, fontweight='bold')\n","            axes[1].grid(True, alpha=0.3)\n","            axes[1].legend(fontsize=10)\n","\n","            plt.tight_layout()\n","            plt.savefig(run_dir / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n","            plt.close()\n","\n","            # Plot 2: Accuracy vs Wall Clock Time\n","            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","            axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n","            train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n","            train_time = df[df['avg_train_acc'] > 0]['time_min']\n","            if len(train_acc_time) > 0:\n","                axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n","            axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n","            axes[0].set_ylabel('Accuracy', fontsize=12)\n","            axes[0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n","            axes[0].grid(True, alpha=0.3)\n","            axes[0].legend(fontsize=10)\n","            axes[0].set_ylim([0, 1.0])\n","\n","            axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n","            train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n","            train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n","            if len(train_loss_time) > 0:\n","                axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n","            axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n","            axes[1].set_ylabel('Loss', fontsize=12)\n","            axes[1].set_title(f'{exp_id}: Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n","            axes[1].grid(True, alpha=0.3)\n","            axes[1].legend(fontsize=10)\n","\n","            plt.tight_layout()\n","            plt.savefig(run_dir / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n","            plt.close()\n","\n","            # Plot 3: Convergence Speed Analysis\n","            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","            thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n","            times_to_threshold = []\n","            rounds_to_threshold = []\n","            reached_thresholds = []\n","\n","            for thresh in thresholds:\n","                mask = df['test_acc'] >= thresh\n","                if mask.any():\n","                    first_idx = mask.idxmax()\n","                    times_to_threshold.append(df.loc[first_idx, 'time_min'])\n","                    rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n","                    reached_thresholds.append(thresh)\n","                else:\n","                    break\n","\n","            if len(reached_thresholds) > 0:\n","                axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n","                axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n","                axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n","                axes[0].set_title(f'{exp_id}: Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n","                axes[0].set_xticks(range(len(reached_thresholds)))\n","                axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n","                axes[0].grid(True, alpha=0.3, axis='y')\n","\n","                for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n","                    axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n","\n","                axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n","                axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n","                axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n","                axes[1].set_title(f'{exp_id}: Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n","                axes[1].set_xticks(range(len(reached_thresholds)))\n","                axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n","                axes[1].grid(True, alpha=0.3, axis='y')\n","\n","                for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n","                    axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n","\n","            plt.tight_layout()\n","            plt.savefig(run_dir / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n","            plt.close()\n","\n","            # Plot 4: Training Efficiency\n","            fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n","\n","            if len(df) > 1:\n","                df_sorted = df.sort_values('time_min')\n","                efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n","                efficiency = efficiency.fillna(0)\n","\n","                ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n","                ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n","                ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n","                ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n","                ax.set_title(f'{exp_id}: Training Efficiency', fontsize=14, fontweight='bold')\n","                ax.grid(True, alpha=0.3)\n","\n","                max_eff_idx = efficiency.idxmax()\n","                max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n","                max_eff_val = efficiency.loc[max_eff_idx]\n","                ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n","                ax.legend(fontsize=10)\n","\n","            plt.tight_layout()\n","            plt.savefig(run_dir / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n","            plt.close()\n","\n","            # Plot 5: Client Participation Analysis\n","            if participation_path.exists():\n","                part_df = pd.read_csv(participation_path)\n","\n","                fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","\n","                client_counts = part_df['client_id'].value_counts().sort_index()\n","                axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n","                axes[0, 0].set_xlabel('Client ID', fontsize=11)\n","                axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n","                axes[0, 0].set_title(f'{exp_id}: Client Participation Frequency', fontsize=12, fontweight='bold')\n","                axes[0, 0].grid(True, alpha=0.3, axis='y')\n","\n","                participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n","                axes[0, 1].plot(participation_by_round.index, participation_by_round.values,\n","                               marker='o', markersize=4, linewidth=2, color='coral')\n","                axes[0, 1].set_xlabel('Round', fontsize=11)\n","                axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n","                axes[0, 1].set_title(f'{exp_id}: Client Participation per Round', fontsize=12, fontweight='bold')\n","                axes[0, 1].grid(True, alpha=0.3)\n","\n","                client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n","                axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values,\n","                                marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n","                axes[1, 0].set_xlabel('Round', fontsize=11)\n","                axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n","                axes[1, 0].set_title(f'{exp_id}: Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n","                axes[1, 0].grid(True, alpha=0.3)\n","                axes[1, 0].set_ylim([0, 1.0])\n","\n","                if len(part_df) > 0:\n","                    client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values\n","                                  for cid in sorted(part_df['client_id'].unique())[:10]]\n","                    if len(client_accs) > 0:\n","                        axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n","                        axes[1, 1].set_xlabel('Client ID', fontsize=11)\n","                        axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n","                        axes[1, 1].set_title(f'{exp_id}: Client Accuracy Distribution', fontsize=12, fontweight='bold')\n","                        axes[1, 1].grid(True, alpha=0.3, axis='y')\n","                        axes[1, 1].set_ylim([0, 1.0])\n","\n","                plt.tight_layout()\n","                plt.savefig(run_dir / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n","                plt.close()\n","\n","            # Plot 6: Training Progress Summary\n","            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","\n","            axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n","            train_acc_plot = df[df['avg_train_acc'] > 0]\n","            if len(train_acc_plot) > 0:\n","                axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'],\n","                               'r--', linewidth=2, label='Train')\n","            axes[0, 0].set_xlabel('Round', fontsize=11)\n","            axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n","            axes[0, 0].set_title(f'{exp_id}: Accuracy Trajectory', fontsize=12, fontweight='bold')\n","            axes[0, 0].legend(fontsize=10)\n","            axes[0, 0].grid(True, alpha=0.3)\n","            axes[0, 0].set_ylim([0, 1.0])\n","\n","            axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n","            train_loss_plot = df[df['avg_train_loss'] > 0]\n","            if len(train_loss_plot) > 0:\n","                axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'],\n","                               'r--', linewidth=2, label='Train')\n","            axes[0, 1].set_xlabel('Round', fontsize=11)\n","            axes[0, 1].set_ylabel('Loss', fontsize=11)\n","            axes[0, 1].set_title(f'{exp_id}: Loss Trajectory', fontsize=12, fontweight='bold')\n","            axes[0, 1].legend(fontsize=10)\n","            axes[0, 1].grid(True, alpha=0.3)\n","\n","            axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n","            if len(train_acc_plot) > 0:\n","                train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n","                axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'],\n","                                'r--', linewidth=2, label='Train')\n","            axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n","            axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n","            axes[1, 0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n","            axes[1, 0].legend(fontsize=10)\n","            axes[1, 0].grid(True, alpha=0.3)\n","            axes[1, 0].set_ylim([0, 1.0])\n","\n","            axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n","            axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n","            axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n","            axes[1, 1].set_title(f'{exp_id}: Round Progression Over Time', fontsize=12, fontweight='bold')\n","            axes[1, 1].grid(True, alpha=0.3)\n","\n","            if len(df) > 1:\n","                total_time = df['time_min'].iloc[-1]\n","                total_rounds = df['total_agg'].iloc[-1]\n","                rate = total_rounds / total_time if total_time > 0 else 0\n","                axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min',\n","                               transform=axes[1, 1].transAxes, fontsize=10,\n","                               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n","\n","            plt.tight_layout()\n","            plt.savefig(run_dir / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n","            plt.close()\n","\n","            print(f\"  âœ… All plots saved to {run_dir}\")\n","\n","        # Create comparison plots across all experiments\n","        print(f\"\\nðŸ“Š Creating comparison plots across all experiments...\")\n","\n","        # Comparison Plot 1: Test Accuracy vs Rounds (all experiments)\n","        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n","        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n","            df = exp_data['df']\n","            exp_config = exp_data['config']\n","            label = f\"{exp_id} (Î±={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n","            ax.plot(df['total_agg'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n","        ax.set_xlabel('Round', fontsize=12)\n","        ax.set_ylabel('Test Accuracy', fontsize=12)\n","        ax.set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n","        ax.legend(fontsize=9, loc='lower right')\n","        ax.grid(True, alpha=0.3)\n","        ax.set_ylim([0, 1.0])\n","\n","        # Save comparison plot\n","        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n","            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"FedBuff\" / \"comparisons\"\n","        else:\n","            comparison_dir = Path(\"./logs/FedBuff/comparisons\")\n","        comparison_dir.mkdir(parents=True, exist_ok=True)\n","\n","        plt.tight_layout()\n","        plt.savefig(comparison_dir / \"comparison_accuracy_vs_rounds.png\", dpi=150, bbox_inches='tight')\n","        plt.close()\n","        print(f\"  âœ… Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_rounds.png'}\")\n","\n","        # Comparison Plot 2: Test Accuracy vs Time (all experiments)\n","        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n","        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n","            df = exp_data['df']\n","            exp_config = exp_data['config']\n","            label = f\"{exp_id} (Î±={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n","            ax.plot(df['time_min'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n","        ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n","        ax.set_ylabel('Test Accuracy', fontsize=12)\n","        ax.set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n","        ax.legend(fontsize=9, loc='lower right')\n","        ax.grid(True, alpha=0.3)\n","        ax.set_ylim([0, 1.0])\n","\n","        plt.tight_layout()\n","        plt.savefig(comparison_dir / \"comparison_accuracy_vs_time.png\", dpi=150, bbox_inches='tight')\n","        plt.close()\n","        print(f\"  âœ… Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_time.png'}\")\n","\n","        print(f\"\\nâœ… All visualizations completed!\")\n","        print(f\"   Individual plots saved to each experiment's run folder\")\n","        print(f\"   Comparison plots saved to: {comparison_dir}\")\n","\n","    else:\n","        print(\"âš ï¸  No completed experiments found. Run experiments first using Section 8.\")\n","else:\n","    print(\"âš ï¸  No experiment results found. Run experiments first using Section 8.\")\n","    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")"],"metadata":{"id":"SRJU5SZKAizw","executionInfo":{"status":"aborted","timestamp":1764482206535,"user_tz":360,"elapsed":0,"user":{"displayName":"Avinash","userId":"15681946276512401675"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from IPython.display import Image, display\n","import os\n","\n","# Assuming comparison_dir is defined from the previous cell's execution\n","# If not, re-define it based on the environment\n","if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n","    comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"FedBuff\" / \"comparisons\"\n","else:\n","    comparison_dir = Path(\"./logs/FedBuff/comparisons\")\n","\n","print(\"Displaying Comparison Plots:\")\n","\n","# Display comparison plots\n","comp_plot_acc_rounds = comparison_dir / \"comparison_accuracy_vs_rounds.png\"\n","if os.path.exists(comp_plot_acc_rounds):\n","    print(f\"\\n{'-'*50}\\nTest Accuracy vs Rounds (All Experiments)\")\n","    display(Image(filename=comp_plot_acc_rounds))\n","else:\n","    print(f\"Warning: Comparison plot not found: {comp_plot_acc_rounds}\")\n","\n","comp_plot_acc_time = comparison_dir / \"comparison_accuracy_vs_time.png\"\n","if os.path.exists(comp_plot_acc_time):\n","    print(f\"\\n{'-'*50}\\nTest Accuracy vs Wall Clock Time (All Experiments)\")\n","    display(Image(filename=comp_plot_acc_time))\n","else:\n","    print(f\"Warning: Comparison plot not found: {comp_plot_acc_time}\")\n","\n","# Display individual experiment plots as requested\n","print(\"\\nDisplaying Individual Experiment Plots (Accuracy vs Rounds):\")\n","# Ensure 'all_data' is available, it should be from the previous cell\n","if 'all_data' in globals() and len(all_data) > 0:\n","    for exp_id, exp_data in all_data.items():\n","        run_dir = exp_data['run_dir']\n","        plot_path = run_dir / \"1_accuracy_loss_vs_rounds.png\"\n","        if os.path.exists(plot_path):\n","            print(f\"\\n{'-'*50}\\n{exp_id}: Accuracy & Loss vs Rounds\")\n","            display(Image(filename=plot_path))\n","        else:\n","            print(f\"Warning: Plot not found for {exp_id}: {plot_path}\")\n","else:\n","    print(\"Warning: 'all_data' not found. Please ensure Section 13 has been run.\")"],"metadata":{"id":"EW8Py6yYCzxj","executionInfo":{"status":"aborted","timestamp":1764482206546,"user_tz":360,"elapsed":1315573,"user":{"displayName":"Avinash","userId":"15681946276512401675"}}},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}