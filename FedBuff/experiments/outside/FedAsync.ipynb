{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FedAsync: Complete Self-Contained Notebook\n",
        "\n",
        "This notebook contains all code needed to run FedAsync experiments:\n",
        "- Library installation\n",
        "- Data downloading and loading\n",
        "- All FedAsync implementation\n",
        "- Logging and checkpointing\n",
        "\n",
        "**Run cells sequentially from top to bottom.**\n",
        "\n",
        "## Google Colab Setup\n",
        "This notebook is configured to save all results (logs, checkpoints, models) to Google Drive.\n",
        "Data will be downloaded locally for faster access.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Google Colab Setup (Mount Drive)\n",
        "\n",
        "**Skip this section if running locally.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Google Colab: Mount Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    IN_COLAB = True\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    # Define the target directory for saving results\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/colab/dml_project\"\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "    print(f\"‚úÖ Google Drive mounted\")\n",
        "    print(f\"‚úÖ Output directory set to: {OUTPUT_DIR}\")\n",
        "    print(f\"üìÅ All logs, checkpoints, and results will be saved to Google Drive\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    OUTPUT_DIR = None\n",
        "    print(\"‚ö†Ô∏è  Not running in Google Colab - using local paths\")\n",
        "\n",
        "# Install required packages\n",
        "%pip install torch torchvision pytorch-lightning pyyaml numpy matplotlib pandas -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Silence libraries\n",
        "import os\n",
        "import logging, warnings\n",
        "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
        "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "os.environ[\"LIGHTNING_DISABLE_RICH\"] = \"1\"\n",
        "for name in [\n",
        "    \"pytorch_lightning\", \"lightning\", \"lightning.pytorch\",\n",
        "    \"lightning_fabric\", \"lightning_utilities\", \"torch\", \"torchvision\",\n",
        "]:\n",
        "    logging.getLogger(name).setLevel(logging.ERROR)\n",
        "    logging.getLogger(name).propagate = False\n",
        "logging.getLogger().setLevel(logging.WARNING)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Core imports\n",
        "import time\n",
        "import csv\n",
        "import threading\n",
        "import random\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup paths based on environment\n",
        "if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
        "    # Google Colab: Save everything to Drive except data\n",
        "    BASE_OUTPUT_DIR = Path(OUTPUT_DIR)\n",
        "    DATA_DIR = Path(\"./data\")  # Local for faster access\n",
        "    print(f\"‚úÖ Google Colab mode: Results ‚Üí {BASE_OUTPUT_DIR}\")\n",
        "    print(f\"‚úÖ Data directory: {DATA_DIR} (local)\")\n",
        "else:\n",
        "    # Local execution: Use current directory\n",
        "    BASE_OUTPUT_DIR = Path(\".\")\n",
        "    DATA_DIR = Path(\"./data\")\n",
        "    print(f\"‚úÖ Local mode: Results ‚Üí {BASE_OUTPUT_DIR}\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Utility Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Helper Functions ==========\n",
        "\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    \"\"\"Seed all RNGs for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"Return the first available computation device (CUDA/MPS/CPU).\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device(\"cuda\")\n",
        "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "        return torch.device(\"mps\")\n",
        "    return torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def _device_to_accelerator(device: torch.device) -> str:\n",
        "    \"\"\"Convert torch device to PyTorch Lightning accelerator string.\"\"\"\n",
        "    if device.type == \"cuda\":\n",
        "        return \"gpu\"\n",
        "    if device.type == \"mps\":\n",
        "        return \"mps\"\n",
        "    return \"cpu\"\n",
        "\n",
        "print(\"‚úÖ Helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Model Utilities ==========\n",
        "\n",
        "def build_resnet18(num_classes: int = 10, pretrained: bool = False) -> nn.Module:\n",
        "    \"\"\"Create ResNet-18 adapted for CIFAR-10.\"\"\"\n",
        "    if pretrained:\n",
        "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    else:\n",
        "        m = models.resnet18(weights=None)\n",
        "    # CIFAR-10: 32x32 -> use 3x3 conv, stride 1, no maxpool\n",
        "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    m.maxpool = nn.Identity()\n",
        "    # Replace classifier\n",
        "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "    m.num_classes = num_classes\n",
        "    return m\n",
        "\n",
        "\n",
        "def state_to_list(state: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
        "    \"\"\"Flatten a state_dict to a list of tensors on CPU.\"\"\"\n",
        "    return [t.detach().cpu().clone() for _, t in state.items()]\n",
        "\n",
        "\n",
        "def list_to_state(template: Dict[str, torch.Tensor], arrs: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Rebuild a state_dict from a list of tensors using a template.\"\"\"\n",
        "    out: Dict[str, torch.Tensor] = {}\n",
        "    for (k, v), a in zip(template.items(), arrs):\n",
        "        out[k] = a.to(v.device).type_as(v)\n",
        "    return out\n",
        "\n",
        "print(\"‚úÖ Model utilities defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== Data Loading and Partitioning ==========\n",
        "\n",
        "class DataDistributor:\n",
        "    \"\"\"Data distributor for federated learning with Dirichlet partitioning.\"\"\"\n",
        "    \n",
        "    def __init__(self, dataset_name: str, data_dir: str = \"./data\"):\n",
        "        self.dataset_name = dataset_name.lower()\n",
        "        self.data_dir = data_dir\n",
        "        self.train_dataset, self.test_dataset, self.num_classes = self._load_dataset()\n",
        "        self.partitions = None\n",
        "\n",
        "    def _load_dataset(self) -> Tuple[Any, Any, int]:\n",
        "        \"\"\"Load CIFAR-10 dataset with augmentation.\"\"\"\n",
        "        if self.dataset_name == \"cifar10\":\n",
        "            # Strong data pipeline: augmentation for train, normalization for test\n",
        "            transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=(0.4914, 0.4822, 0.4465),\n",
        "                    std=(0.2470, 0.2435, 0.2616),\n",
        "                ),\n",
        "            ])\n",
        "            transform_test = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=(0.4914, 0.4822, 0.4465),\n",
        "                    std=(0.2470, 0.2435, 0.2616),\n",
        "                ),\n",
        "            ])\n",
        "            train = datasets.CIFAR10(self.data_dir, train=True, download=True, transform=transform_train)\n",
        "            test = datasets.CIFAR10(self.data_dir, train=False, download=True, transform=transform_test)\n",
        "            num_classes = 10\n",
        "        else:\n",
        "            raise ValueError(f\"Dataset '{self.dataset_name}' not supported. Use 'cifar10'.\")\n",
        "        return train, test, num_classes\n",
        "\n",
        "    def distribute_data(self, num_clients: int, alpha: float = 0.5, seed: int = 42):\n",
        "        \"\"\"Partition data using Dirichlet distribution.\"\"\"\n",
        "        np.random.seed(seed)\n",
        "        targets = np.array(self.train_dataset.targets)\n",
        "        self.partitions = {i: [] for i in range(num_clients)}\n",
        "\n",
        "        for cls in range(self.num_classes):\n",
        "            idxs = np.where(targets == cls)[0]\n",
        "            np.random.shuffle(idxs)\n",
        "            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n",
        "            proportions = np.array([p * len(idxs) for p in proportions]).astype(int)\n",
        "\n",
        "            start = 0\n",
        "            for client_id, size in enumerate(proportions):\n",
        "                self.partitions[client_id].extend(idxs[start:start + size])\n",
        "                start += size\n",
        "\n",
        "        for cid in self.partitions:\n",
        "            np.random.shuffle(self.partitions[cid])\n",
        "\n",
        "    def get_client_data(self, client_id: int) -> Subset:\n",
        "        \"\"\"Get data subset for a specific client.\"\"\"\n",
        "        if self.partitions is None:\n",
        "            raise ValueError(\"Data not distributed yet. Call distribute_data() first.\")\n",
        "        indices = self.partitions[client_id]\n",
        "        return Subset(self.train_dataset, indices)\n",
        "\n",
        "print(\"‚úÖ DataDistributor class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. FedAsync Client Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== FedAsync Client ==========\n",
        "\n",
        "def _testloader(root: str, batch_size: int = 256):\n",
        "    \"\"\"Create test dataloader.\"\"\"\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "def _evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = crit(logits, y)\n",
        "            loss_sum += float(loss.item()) * y.size(0)\n",
        "            total += y.size(0)\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "    return loss_sum / max(1, total), correct / max(1, total)\n",
        "\n",
        "\n",
        "class LitCifar(pl.LightningModule):\n",
        "    \"\"\"PyTorch Lightning module for CIFAR-10 training.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_model: nn.Module, lr: float = 1e-3, momentum: float = 0.9, weight_decay: float = 5e-4):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters(ignore=[\"base_model\"])\n",
        "        self.model = base_model\n",
        "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "        # Store optimizer params directly\n",
        "        self._optimizer_lr = lr\n",
        "        self._optimizer_momentum = momentum\n",
        "        self._optimizer_weight_decay = weight_decay\n",
        "        self._train_loss_sum = 0.0\n",
        "        self._train_correct = 0\n",
        "        self._train_total = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, _batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        pred = logits.argmax(1)\n",
        "        self._train_loss_sum += float(loss.item()) * y.size(0)\n",
        "        self._train_correct += (pred == y).sum().item()\n",
        "        self._train_total += y.size(0)\n",
        "        return loss\n",
        "\n",
        "    def on_train_epoch_start(self):\n",
        "        self._train_loss_sum = 0.0\n",
        "        self._train_correct = 0\n",
        "        self._train_total = 0\n",
        "\n",
        "    def get_epoch_metrics(self) -> Tuple[float, float]:\n",
        "        if self._train_total == 0:\n",
        "            return 0.0, 0.0\n",
        "        return self._train_loss_sum / self._train_total, self._train_correct / self._train_total\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.SGD(\n",
        "            self.parameters(),\n",
        "            lr=self._optimizer_lr,\n",
        "            momentum=self._optimizer_momentum,\n",
        "            weight_decay=self._optimizer_weight_decay\n",
        "        )\n",
        "\n",
        "\n",
        "class LocalAsyncClient:\n",
        "    \"\"\"FedAsync client that trains locally and submits updates immediately to server.\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        cid: int,\n",
        "        cfg: dict,\n",
        "        subset: Subset,\n",
        "        work_dir: str = \"./checkpoints/clients\",\n",
        "        base_delay: float = 0.0,\n",
        "        slow: bool = False,\n",
        "        delay_ranges: Optional[tuple] = None,\n",
        "        jitter: float = 0.0,\n",
        "        fix_delay: bool = True,\n",
        "    ):\n",
        "        self.cid = cid\n",
        "        self.cfg = cfg\n",
        "        self.device = get_device()\n",
        "\n",
        "        base = build_resnet18(num_classes=cfg[\"data\"][\"num_classes\"], pretrained=False)\n",
        "        lr = float(cfg[\"clients\"][\"lr\"])\n",
        "        momentum = float(cfg[\"clients\"].get(\"momentum\", 0.9))\n",
        "        weight_decay = float(cfg[\"clients\"].get(\"weight_decay\", 5e-4))\n",
        "        self.lit = LitCifar(base, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
        "\n",
        "        self.loader = DataLoader(subset, batch_size=int(cfg[\"clients\"][\"batch_size\"]),\n",
        "                                 shuffle=True, num_workers=0)\n",
        "\n",
        "        self.base_delay = float(base_delay)\n",
        "        self.slow = bool(slow)\n",
        "        self.delay_ranges = delay_ranges\n",
        "        self.jitter = float(jitter)\n",
        "        self.fix_delay = bool(fix_delay)\n",
        "\n",
        "        if self.fix_delay and self.delay_ranges is not None:\n",
        "            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n",
        "            if self.slow:\n",
        "                self.base_delay = random.uniform(float(a_s), float(b_s))\n",
        "            else:\n",
        "                self.base_delay = random.uniform(float(a_f), float(b_f))\n",
        "\n",
        "        self.accelerator = _device_to_accelerator(self.device)\n",
        "        self.testloader = _testloader(cfg[\"data\"][\"data_dir\"])\n",
        "\n",
        "    def _to_list(self) -> List[torch.Tensor]:\n",
        "        return state_to_list(self.lit.model.state_dict())\n",
        "\n",
        "    def _from_list(self, arrs: List[torch.Tensor]) -> None:\n",
        "        sd = self.lit.model.state_dict()\n",
        "        new_sd = list_to_state(sd, arrs)\n",
        "        self.lit.model.load_state_dict(new_sd, strict=True)\n",
        "        self.lit.to(self.device)\n",
        "\n",
        "    def _sleep_delay(self):\n",
        "        global_d = float(self.cfg.get(\"server_runtime\", {}).get(\"client_delay\", 0.0))\n",
        "        base = self.base_delay\n",
        "\n",
        "        if not self.fix_delay and self.delay_ranges is not None:\n",
        "            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n",
        "            if self.slow:\n",
        "                base = random.uniform(float(a_s), float(b_s))\n",
        "            else:\n",
        "                base = random.uniform(float(a_f), float(b_f))\n",
        "\n",
        "        jit = random.uniform(-self.jitter, self.jitter) if self.jitter > 0.0 else 0.0\n",
        "        delay = max(0.0, global_d + base + jit)\n",
        "        if delay > 0.0:\n",
        "            time.sleep(delay)\n",
        "\n",
        "    def fit_once(self, server) -> bool:\n",
        "        params, version = server.get_global()\n",
        "        self._from_list(params)\n",
        "\n",
        "        self._sleep_delay()\n",
        "\n",
        "        epochs = int(self.cfg[\"clients\"][\"local_epochs\"])\n",
        "        grad_clip = float(self.cfg[\"clients\"].get(\"grad_clip\", 1.0))\n",
        "        trainer = pl.Trainer(\n",
        "            max_epochs=epochs,\n",
        "            accelerator=self.accelerator,\n",
        "            devices=1,\n",
        "            enable_checkpointing=False,\n",
        "            logger=False,\n",
        "            enable_model_summary=False,\n",
        "            num_sanity_val_steps=0,\n",
        "            enable_progress_bar=False,\n",
        "            callbacks=[],\n",
        "            gradient_clip_val=grad_clip,\n",
        "            gradient_clip_algorithm=\"norm\",\n",
        "        )\n",
        "        start = time.time()\n",
        "        trainer.fit(self.lit, train_dataloaders=self.loader)\n",
        "        duration = time.time() - start\n",
        "\n",
        "        train_loss, train_acc = self.lit.get_epoch_metrics()\n",
        "        test_loss, test_acc = _evaluate(self.lit.model, self.testloader, self.device)\n",
        "\n",
        "        new_params = self._to_list()\n",
        "        num_examples = len(self.loader.dataset)\n",
        "\n",
        "        server.submit_update(\n",
        "            client_id=self.cid,\n",
        "            base_version=version,\n",
        "            new_params=new_params,\n",
        "            num_samples=num_examples,\n",
        "            train_time_s=duration,\n",
        "            train_loss=train_loss,\n",
        "            train_acc=train_acc,\n",
        "            test_loss=test_loss,\n",
        "            test_acc=test_acc,\n",
        "        )\n",
        "        return not server.should_stop()\n",
        "\n",
        "print(\"‚úÖ FedAsync client classes defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== FedAsync Server ==========\n",
        "\n",
        "def _testloader_server(root: str, batch_size: int = 256):\n",
        "    \"\"\"Create test dataloader for server evaluation.\"\"\"\n",
        "    tfm = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "\n",
        "def _evaluate_server(model: torch.nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate model on test set.\"\"\"\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss_sum += float(loss.item()) * y.size(0)\n",
        "            total += y.size(0)\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "    return loss_sum / max(1, total), correct / max(1, total)\n",
        "\n",
        "\n",
        "class AsyncFedServer:\n",
        "    \"\"\"FedAsync server with staleness-aware aggregation (immediate updates, no buffering).\"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        global_model: torch.nn.Module,\n",
        "        total_train_samples: int,\n",
        "        c: float = 0.5,\n",
        "        mixing_alpha: float = 1.0,\n",
        "        use_sample_weighing: bool = True,\n",
        "        target_accuracy: float = 0.70,\n",
        "        max_rounds: Optional[int] = None,\n",
        "        eval_interval_s: int = 15,\n",
        "        data_dir: str = \"./data\",\n",
        "        checkpoints_dir: str = \"./checkpoints/FedAsync\",\n",
        "        logs_dir: str = \"./logs/FedAsync\",\n",
        "        global_log_csv: Optional[str] = None,\n",
        "        client_participation_csv: Optional[str] = None,\n",
        "        final_model_path: Optional[str] = None,\n",
        "        resume: bool = True,\n",
        "        device: Optional[torch.device] = None,\n",
        "    ):\n",
        "        self.model = global_model\n",
        "        self.template = {k: v.detach().clone() for k, v in self.model.state_dict().items()}\n",
        "        self.device = device or get_device()\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.total_train_samples = int(total_train_samples)\n",
        "        self.c = float(c)\n",
        "        self.mixing_alpha = float(mixing_alpha)\n",
        "        self.use_sample_weighing = bool(use_sample_weighing)\n",
        "\n",
        "        self.eval_interval_s = int(eval_interval_s)\n",
        "        self.target_accuracy = float(target_accuracy)\n",
        "        self.max_rounds = int(max_rounds) if max_rounds is not None else None\n",
        "\n",
        "        self.data_dir = data_dir\n",
        "        self.ckpt_dir = Path(checkpoints_dir)\n",
        "        self.ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.log_dir = Path(logs_dir)\n",
        "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        self.csv_path = Path(global_log_csv) if global_log_csv else (self.log_dir / \"FedAsync.csv\")\n",
        "        self.participation_csv = Path(client_participation_csv) if client_participation_csv else (self.log_dir / \"FedAsyncClientParticipation.csv\")\n",
        "        self.final_model_path = Path(final_model_path) if final_model_path else Path(\"./results/FedAsyncModel.pt\")\n",
        "        self.final_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if not self.csv_path.exists():\n",
        "            self.csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with self.csv_path.open(\"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\"total_agg\", \"avg_train_loss\", \"avg_train_acc\",\n",
        "                                        \"test_loss\", \"test_acc\", \"time\"])\n",
        "\n",
        "        if not self.participation_csv.exists():\n",
        "            self.participation_csv.parent.mkdir(parents=True, exist_ok=True)\n",
        "            with self.participation_csv.open(\"w\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\n",
        "                    \"client_id\", \"local_train_loss\", \"local_train_acc\",\n",
        "                    \"local_test_loss\", \"local_test_acc\", \"total_agg\"\n",
        "                ])\n",
        "\n",
        "        self._lock = threading.Lock()\n",
        "        self._stop = False\n",
        "        self.t_round = 0\n",
        "        self._log_count = 0\n",
        "        self.testloader = _testloader_server(self.data_dir)\n",
        "        self._train_loss_acc_accum: List[Tuple[float, float, int]] = []\n",
        "        self._start_ts = time.time()\n",
        "\n",
        "        if resume:\n",
        "            self._maybe_resume()\n",
        "\n",
        "    def _ckpt_file(self) -> Path:\n",
        "        return self.ckpt_dir / \"server_last.ckpt\"\n",
        "\n",
        "    def _maybe_resume(self) -> None:\n",
        "        ck = self._ckpt_file()\n",
        "        if ck.exists():\n",
        "            try:\n",
        "                blob = torch.load(ck, map_location=\"cpu\")\n",
        "                state = list_to_state(self.template, blob[\"global_params\"])\n",
        "                self.model.load_state_dict(state, strict=True)\n",
        "                self.t_round = int(blob[\"t_round\"])\n",
        "                print(f\"[resume] Loaded server checkpoint at total_agg={self.t_round}\")\n",
        "            except (RuntimeError, KeyError) as e:\n",
        "                print(f\"[resume] Checkpoint incompatible, starting fresh: {type(e).__name__}\")\n",
        "                ck.unlink()\n",
        "                self.t_round = 0\n",
        "\n",
        "    def _save_ckpt(self) -> None:\n",
        "        sd = state_to_list(self.model.state_dict())\n",
        "        torch.save({\"t_round\": self.t_round, \"global_params\": sd}, self._ckpt_file())\n",
        "\n",
        "    def _save_final_model(self) -> None:\n",
        "        torch.save(self.model.state_dict(), self.final_model_path)\n",
        "\n",
        "    def get_global(self):\n",
        "        with self._lock:\n",
        "            return state_to_list(self.model.state_dict()), self.t_round\n",
        "\n",
        "    def submit_update(\n",
        "        self,\n",
        "        client_id: int,\n",
        "        base_version: int,\n",
        "        new_params: List[torch.Tensor],\n",
        "        num_samples: int,\n",
        "        train_time_s: float,\n",
        "        train_loss: float,\n",
        "        train_acc: float,\n",
        "        test_loss: float,\n",
        "        test_acc: float,\n",
        "    ) -> None:\n",
        "        with self._lock:\n",
        "            if self._stop:\n",
        "                return\n",
        "            if self.max_rounds is not None and self.t_round >= self.max_rounds:\n",
        "                self._stop = True\n",
        "                return\n",
        "\n",
        "            # Log client participation\n",
        "            with self.participation_csv.open(\"a\", newline=\"\") as f:\n",
        "                csv.writer(f).writerow([\n",
        "                    client_id, f\"{train_loss:.6f}\", f\"{train_acc:.6f}\",\n",
        "                    f\"{test_loss:.6f}\", f\"{test_acc:.6f}\", self.t_round\n",
        "                ])\n",
        "\n",
        "            # FedAsync staleness-aware aggregation\n",
        "            staleness = max(0, self.t_round - base_version)\n",
        "            alpha = self.c / float(staleness + 1)  # Staleness weight: c/(staleness+1)\n",
        "            sw = float(num_samples) / float(self.total_train_samples) if self.use_sample_weighing else 1.0\n",
        "            eff = alpha * self.mixing_alpha * sw  # Effective update rate\n",
        "\n",
        "            # Immediate merge (no buffering)\n",
        "            g = state_to_list(self.model.state_dict())\n",
        "            merged = [(1.0 - eff) * gi + eff * ci for gi, ci in zip(g, new_params)]\n",
        "            new_state = list_to_state(self.template, merged)\n",
        "            self.model.load_state_dict(new_state, strict=True)\n",
        "\n",
        "            # Accumulate metrics\n",
        "            self._train_loss_acc_accum.append((float(train_loss), float(train_acc), int(num_samples)))\n",
        "\n",
        "            self.t_round += 1\n",
        "            self._save_ckpt()\n",
        "\n",
        "    def should_stop(self) -> bool:\n",
        "        with self._lock:\n",
        "            return self._stop\n",
        "\n",
        "    def mark_stop(self) -> None:\n",
        "        with self._lock:\n",
        "            self._stop = True\n",
        "            self._save_final_model()\n",
        "            print(f\"[LOG] saved final model -> {self.final_model_path}\")\n",
        "\n",
        "    def _compute_avg_train(self) -> Tuple[float, float]:\n",
        "        if not self._train_loss_acc_accum:\n",
        "            return 0.0, 0.0\n",
        "        loss_sum, acc_sum, n_sum = 0.0, 0.0, 0\n",
        "        for l, a, n in self._train_loss_acc_accum:\n",
        "            loss_sum += l * n\n",
        "            acc_sum += a * n\n",
        "            n_sum += n\n",
        "        return loss_sum / max(1, n_sum), acc_sum / max(1, n_sum)\n",
        "\n",
        "    def _periodic_eval_and_log(self):\n",
        "        test_loss, test_acc = _evaluate_server(self.model, self.testloader, self.device)\n",
        "        avg_train_loss, avg_train_acc = self._compute_avg_train()\n",
        "        now = time.time() - self._start_ts\n",
        "        self._train_loss_acc_accum.clear()\n",
        "\n",
        "        with self.csv_path.open(\"a\", newline=\"\") as f:\n",
        "            csv.writer(f).writerow([\n",
        "                self.t_round, f\"{avg_train_loss:.6f}\", f\"{avg_train_acc:.6f}\",\n",
        "                f\"{test_loss:.6f}\", f\"{test_acc:.6f}\", f\"{now:.3f}\"\n",
        "            ])\n",
        "        print(f\"[LOG] total_agg={self.t_round} \"\n",
        "              f\"avg_train_loss={avg_train_loss:.4f} avg_train_acc={avg_train_acc:.4f} \"\n",
        "              f\"test_loss={test_loss:.4f} test_acc={test_acc:.4f} time={now:.1f}s\")\n",
        "\n",
        "        self._log_count += 1\n",
        "        if self._log_count % 100 == 0:\n",
        "            path = self.ckpt_dir / f\"global_log{self._log_count}_t{self.t_round}.pt\"\n",
        "            torch.save(self.model.state_dict(), path)\n",
        "\n",
        "        if test_acc >= self.target_accuracy:\n",
        "            self._stop = True\n",
        "\n",
        "    def start_eval_timer(self):\n",
        "        def _loop():\n",
        "            next_ts = time.time() + self.eval_interval_s\n",
        "            while True:\n",
        "                now = time.time()\n",
        "                sleep_for = max(0.0, next_ts - now)\n",
        "                time.sleep(sleep_for)\n",
        "                with self._lock:\n",
        "                    if self._stop:\n",
        "                        break\n",
        "                    self._periodic_eval_and_log()\n",
        "                next_ts += self.eval_interval_s\n",
        "        threading.Thread(target=_loop, daemon=True).start()\n",
        "\n",
        "    def wait(self):\n",
        "        try:\n",
        "            while not self.should_stop():\n",
        "                time.sleep(0.2)\n",
        "        finally:\n",
        "            self.mark_stop()\n",
        "\n",
        "print(\"‚úÖ FedAsync server class defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========== All 6 Experiment Configurations ==========\n",
        "\n",
        "# Helper function to get paths (works for both Colab and local)\n",
        "def get_paths(exp_id: str):\n",
        "    \"\"\"Get paths for experiment, using Google Drive if in Colab.\"\"\"\n",
        "    if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
        "        base = BASE_OUTPUT_DIR\n",
        "    else:\n",
        "        base = Path(\".\")\n",
        "    \n",
        "    return {\n",
        "        \"data_dir\": str(DATA_DIR),  # Always local for faster access\n",
        "        \"checkpoints_dir\": str(base / \"checkpoints\" / \"FedAsync\" / exp_id),\n",
        "        \"logs_dir\": str(base / \"logs\" / \"FedAsync\" / exp_id),\n",
        "        \"results_dir\": str(base / \"results\" / \"FedAsync\" / exp_id),\n",
        "    }\n",
        "\n",
        "experiments = {\n",
        "    \"Exp1\": {\n",
        "        \"name\": \"IID (alpha=1000), no stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 0,\n",
        "            \"delay_slow_range\": [0.0, 0.0],\n",
        "            \"delay_fast_range\": [0.0, 0.0],\n",
        "            \"jitter_per_round\": 0.0,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"async\": {\n",
        "            \"c\": 0.5,\n",
        "            \"fedasync_mixing_alpha\": 1.0,\n",
        "            \"use_sample_weighing\": True\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 1000.0,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp1\")\n",
        "    },\n",
        "    \"Exp2\": {\n",
        "        \"name\": \"alpha=0.1, 10% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 10,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"async\": {\n",
        "            \"c\": 0.5,\n",
        "            \"fedasync_mixing_alpha\": 1.0,\n",
        "            \"use_sample_weighing\": True\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp2\")\n",
        "    },\n",
        "    \"Exp3\": {\n",
        "        \"name\": \"alpha=0.1, 20% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 20,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"async\": {\n",
        "            \"c\": 0.5,\n",
        "            \"fedasync_mixing_alpha\": 1.0,\n",
        "            \"use_sample_weighing\": True\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp3\")\n",
        "    },\n",
        "    \"Exp4\": {\n",
        "        \"name\": \"alpha=0.1, 30% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 30,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"async\": {\n",
        "            \"c\": 0.5,\n",
        "            \"fedasync_mixing_alpha\": 1.0,\n",
        "            \"use_sample_weighing\": True\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp4\")\n",
        "    },\n",
        "    \"Exp5\": {\n",
        "        \"name\": \"alpha=0.1, 40% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 40,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"async\": {\n",
        "            \"c\": 0.5,\n",
        "            \"fedasync_mixing_alpha\": 1.0,\n",
        "            \"use_sample_weighing\": True\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp5\")\n",
        "    },\n",
        "    \"Exp6\": {\n",
        "        \"name\": \"alpha=0.1, 50% stragglers\",\n",
        "        \"data\": {\n",
        "            \"dataset\": \"cifar10\",\n",
        "            \"data_dir\": str(DATA_DIR),\n",
        "            \"num_classes\": 10\n",
        "        },\n",
        "        \"clients\": {\n",
        "            \"total\": 20,\n",
        "            \"concurrent\": 5,\n",
        "            \"local_epochs\": 1,\n",
        "            \"batch_size\": 128,\n",
        "            \"lr\": 0.005,\n",
        "            \"momentum\": 0.0,\n",
        "            \"weight_decay\": 0.001,\n",
        "            \"grad_clip\": 5.0,\n",
        "            \"struggle_percent\": 50,\n",
        "            \"delay_slow_range\": [0.8, 2.0],\n",
        "            \"delay_fast_range\": [0.0, 0.2],\n",
        "            \"jitter_per_round\": 0.05,\n",
        "            \"fix_delays_per_client\": True\n",
        "        },\n",
        "        \"async\": {\n",
        "            \"c\": 0.5,\n",
        "            \"fedasync_mixing_alpha\": 1.0,\n",
        "            \"use_sample_weighing\": True\n",
        "        },\n",
        "        \"eval\": {\n",
        "            \"interval_seconds\": 1.0,\n",
        "            \"target_accuracy\": 0.8\n",
        "        },\n",
        "        \"train\": {\n",
        "            \"max_rounds\": 100\n",
        "        },\n",
        "        \"partition_alpha\": 0.1,\n",
        "        \"seed\": 1,\n",
        "        \"server_runtime\": {\n",
        "            \"client_delay\": 0.0\n",
        "        },\n",
        "        \"io\": get_paths(\"Exp6\")\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"‚úÖ All 6 experiment configurations loaded:\")\n",
        "for exp_id, exp_config in experiments.items():\n",
        "    print(f\"  {exp_id}: {exp_config['name']}\")\n",
        "    print(f\"    - Alpha: {exp_config['partition_alpha']}\")\n",
        "    print(f\"    - Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
        "    print(f\"    - Max rounds: {exp_config['train']['max_rounds']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_single_experiment(exp_id: str, experiments_dict: dict):\n",
        "    \"\"\"\n",
        "    Run a single FedAsync experiment.\n",
        "    \n",
        "    Args:\n",
        "        exp_id: Experiment ID (e.g., \"Exp1\", \"Exp2\", ..., \"Exp6\")\n",
        "        experiments_dict: Dictionary containing all experiment configs\n",
        "    \"\"\"\n",
        "    if exp_id not in experiments_dict:\n",
        "        print(f\"‚ùå Error: Experiment {exp_id} not found!\")\n",
        "        return None\n",
        "    \n",
        "    config = experiments_dict[exp_id]\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Running {exp_id}: {config['name']}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Set random seed\n",
        "    seed = int(config.get(\"seed\", 42))\n",
        "    set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    \n",
        "    # Create timestamped run folder\n",
        "    run_dir = Path(config[\"io\"][\"logs_dir\"]) / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    run_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Write COMMIT.txt\n",
        "    commit_hash = \"notebook_run\"\n",
        "    csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n",
        "    with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n",
        "        f.write(f\"{commit_hash},{csv_header}\\n\")\n",
        "    \n",
        "    # Save config to run folder\n",
        "    with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n",
        "        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
        "    \n",
        "    print(f\"‚úÖ Run folder: {run_dir}\")\n",
        "    \n",
        "    # Load and partition data\n",
        "    dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n",
        "    dd.distribute_data(\n",
        "        num_clients=int(config[\"clients\"][\"total\"]),\n",
        "        alpha=float(config[\"partition_alpha\"]),\n",
        "        seed=seed\n",
        "    )\n",
        "    print(f\"‚úÖ Data partitioned: {config['clients']['total']} clients, alpha={config['partition_alpha']}\")\n",
        "    \n",
        "    # Build global model\n",
        "    global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n",
        "    \n",
        "    # Initialize server\n",
        "    server = AsyncFedServer(\n",
        "        global_model=global_model,\n",
        "        total_train_samples=len(dd.train_dataset),\n",
        "        c=float(config[\"async\"][\"c\"]),\n",
        "        mixing_alpha=float(config[\"async\"][\"fedasync_mixing_alpha\"]),\n",
        "        use_sample_weighing=bool(config[\"async\"][\"use_sample_weighing\"]),\n",
        "        target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n",
        "        max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n",
        "        eval_interval_s=int(config[\"eval\"][\"interval_seconds\"]),\n",
        "        data_dir=config[\"data\"][\"data_dir\"],\n",
        "        checkpoints_dir=str(run_dir / \"checkpoints\"),\n",
        "        logs_dir=str(run_dir),\n",
        "        global_log_csv=str(run_dir / \"FedAsync.csv\"),\n",
        "        client_participation_csv=str(run_dir / \"FedAsyncClientParticipation.csv\"),\n",
        "        final_model_path=str(run_dir / \"FedAsyncModel.pt\"),\n",
        "        resume=False,\n",
        "        device=get_device(),\n",
        "    )\n",
        "    print(f\"‚úÖ Server initialized (device: {server.device})\")\n",
        "    print(f\"   c: {server.c}, mixing_alpha: {server.mixing_alpha}\")\n",
        "    \n",
        "    # Setup clients\n",
        "    n = int(config[\"clients\"][\"total\"])\n",
        "    pct = max(0, min(100, int(config[\"clients\"].get(\"struggle_percent\", 0))))\n",
        "    k_slow = (n * pct) // 100\n",
        "    slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
        "    \n",
        "    a_s, b_s = config[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
        "    a_f, b_f = config[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
        "    fix_delays = bool(config[\"clients\"].get(\"fix_delays_per_client\", True))\n",
        "    jitter = float(config[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
        "    \n",
        "    per_client_base_delay: Dict[int, float] = {}\n",
        "    if fix_delays:\n",
        "        for cid in range(n):\n",
        "            if cid in slow_ids:\n",
        "                per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
        "            else:\n",
        "                per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
        "    \n",
        "    clients = []\n",
        "    for cid in range(n):\n",
        "        subset = dd.get_client_data(cid)\n",
        "        base_delay = per_client_base_delay.get(cid, 0.0)\n",
        "        is_slow = cid in slow_ids\n",
        "        clients.append(LocalAsyncClient(\n",
        "            cid=cid,\n",
        "            cfg=config,\n",
        "            subset=subset,\n",
        "            work_dir=str(run_dir / \"checkpoints\" / \"clients\"),\n",
        "            base_delay=base_delay,\n",
        "            slow=is_slow,\n",
        "            delay_ranges=((float(a_s), float(b_s)), (float(a_f), float(b_f))),\n",
        "            jitter=jitter,\n",
        "            fix_delay=fix_delays,\n",
        "        ))\n",
        "    \n",
        "    print(f\"‚úÖ Created {len(clients)} clients ({len(slow_ids)} slow, {n - len(slow_ids)} fast)\")\n",
        "    \n",
        "    # Start experiment\n",
        "    server.start_eval_timer()\n",
        "    sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n",
        "    \n",
        "    def client_loop(client: LocalAsyncClient):\n",
        "        while True:\n",
        "            with sem:\n",
        "                cont = client.fit_once(server)\n",
        "            if not cont:\n",
        "                break\n",
        "            time.sleep(0.05)\n",
        "    \n",
        "    threads = []\n",
        "    for cl in clients:\n",
        "        t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "    \n",
        "    print(f\"‚úÖ Started {len(threads)} client threads\")\n",
        "    print(f\"üöÄ Experiment running... (max {config['train']['max_rounds']} rounds)\")\n",
        "    \n",
        "    # Wait for completion\n",
        "    server.wait()\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "    \n",
        "    print(f\"\\n‚úÖ {exp_id} completed!\")\n",
        "    print(f\"üìÅ Results: {run_dir}\")\n",
        "    \n",
        "    return run_dir\n",
        "\n",
        "print(\"‚úÖ Helper function `run_single_experiment()` defined\")\n",
        "print(\"   Usage: run_dir = run_single_experiment('Exp1', experiments)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Run All 6 Experiments Sequentially\n",
        "\n",
        "This cell runs all experiments one by one. Each experiment saves to its own folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run all 6 experiments sequentially\n",
        "experiment_results = {}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STARTING ALL 6 EXPERIMENTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "total_start = time.time()\n",
        "\n",
        "for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
        "    exp_start = time.time()\n",
        "    try:\n",
        "        run_dir = run_single_experiment(exp_id, experiments)\n",
        "        if run_dir:\n",
        "            experiment_results[exp_id] = {\n",
        "                \"run_dir\": run_dir,\n",
        "                \"status\": \"completed\",\n",
        "                \"duration_min\": (time.time() - exp_start) / 60.0\n",
        "            }\n",
        "            print(f\"‚úÖ {exp_id} completed in {experiment_results[exp_id]['duration_min']:.2f} minutes\")\n",
        "        else:\n",
        "            experiment_results[exp_id] = {\"status\": \"failed\", \"duration_min\": (time.time() - exp_start) / 60.0}\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {exp_id} failed: {str(e)}\")\n",
        "        experiment_results[exp_id] = {\"status\": \"error\", \"error\": str(e), \"duration_min\": (time.time() - exp_start) / 60.0}\n",
        "    \n",
        "    print(f\"\\n{'‚îÄ'*70}\\n\")\n",
        "\n",
        "total_duration = (time.time() - total_start) / 60.0\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ALL EXPERIMENTS COMPLETED\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total time: {total_duration:.2f} minutes ({total_duration/60:.2f} hours)\")\n",
        "print(\"\\nResults summary:\")\n",
        "for exp_id, result in experiment_results.items():\n",
        "    if result.get(\"status\") == \"completed\":\n",
        "        print(f\"  {exp_id}: ‚úÖ {result['duration_min']:.2f} min -> {result['run_dir']}\")\n",
        "    else:\n",
        "        print(f\"  {exp_id}: ‚ùå {result.get('status', 'unknown')}\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compare All Experiments (After Running)\n",
        "\n",
        "This section creates comparison plots across all 6 experiments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all experiments (load results from experiment_results)\n",
        "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
        "    # Load all CSV files\n",
        "    all_data = {}\n",
        "    for exp_id, result in experiment_results.items():\n",
        "        if result.get(\"status\") == \"completed\":\n",
        "            csv_path = result[\"run_dir\"] / \"FedAsync.csv\"\n",
        "            if csv_path.exists():\n",
        "                df = pd.read_csv(csv_path)\n",
        "                df['time_min'] = df['time'] / 60.0\n",
        "                all_data[exp_id] = df\n",
        "                print(f\"‚úÖ Loaded {exp_id}: {len(df)} rows\")\n",
        "    \n",
        "    if len(all_data) > 0:\n",
        "        # Create comparison plots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "        \n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
        "        \n",
        "        # Plot 1: Accuracy vs Rounds\n",
        "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
        "            axes[0, 0].plot(df['total_agg'], df['test_acc'], \n",
        "                          label=f\"{exp_id} ({experiments[exp_id]['name']})\", \n",
        "                          linewidth=2, color=colors[i % len(colors)])\n",
        "        axes[0, 0].set_xlabel('Round', fontsize=12)\n",
        "        axes[0, 0].set_ylabel('Test Accuracy', fontsize=12)\n",
        "        axes[0, 0].set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
        "        axes[0, 0].legend(fontsize=9, loc='lower right')\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        axes[0, 0].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Plot 2: Accuracy vs Wall Clock Time\n",
        "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
        "            axes[0, 1].plot(df['time_min'], df['test_acc'], \n",
        "                          label=f\"{exp_id}\", \n",
        "                          linewidth=2, color=colors[i % len(colors)])\n",
        "        axes[0, 1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "        axes[0, 1].set_ylabel('Test Accuracy', fontsize=12)\n",
        "        axes[0, 1].set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
        "        axes[0, 1].legend(fontsize=9, loc='lower right')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        axes[0, 1].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Plot 3: Best Accuracy by Experiment\n",
        "        best_accs = {exp_id: df['test_acc'].max() for exp_id, df in all_data.items()}\n",
        "        exp_names = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\" \n",
        "                    if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\" \n",
        "                    for exp_id in best_accs.keys()]\n",
        "        axes[1, 0].bar(range(len(best_accs)), list(best_accs.values()), \n",
        "                      color=colors[:len(best_accs)], alpha=0.7, edgecolor='black')\n",
        "        axes[1, 0].set_xticks(range(len(best_accs)))\n",
        "        axes[1, 0].set_xticklabels(exp_names, fontsize=9, rotation=45, ha='right')\n",
        "        axes[1, 0].set_ylabel('Best Test Accuracy', fontsize=12)\n",
        "        axes[1, 0].set_title('Best Test Accuracy by Experiment', fontsize=14, fontweight='bold')\n",
        "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "        axes[1, 0].set_ylim([0, 1.0])\n",
        "        # Add value labels\n",
        "        for i, (exp_id, acc) in enumerate(best_accs.items()):\n",
        "            axes[1, 0].text(i, acc, f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        # Plot 4: Time to reach 50% accuracy\n",
        "        times_to_50 = {}\n",
        "        for exp_id, df in all_data.items():\n",
        "            mask = df['test_acc'] >= 0.5\n",
        "            if mask.any():\n",
        "                first_idx = mask.idxmax()\n",
        "                times_to_50[exp_id] = df.loc[first_idx, 'time_min']\n",
        "            else:\n",
        "                times_to_50[exp_id] = None\n",
        "        \n",
        "        valid_times = {k: v for k, v in times_to_50.items() if v is not None}\n",
        "        if len(valid_times) > 0:\n",
        "            exp_names_50 = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\" \n",
        "                           if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\" \n",
        "                           for exp_id in valid_times.keys()]\n",
        "            axes[1, 1].bar(range(len(valid_times)), list(valid_times.values()), \n",
        "                         color=colors[:len(valid_times)], alpha=0.7, edgecolor='black')\n",
        "            axes[1, 1].set_xticks(range(len(valid_times)))\n",
        "            axes[1, 1].set_xticklabels(exp_names_50, fontsize=9, rotation=45, ha='right')\n",
        "            axes[1, 1].set_ylabel('Time to Reach 50% Accuracy (minutes)', fontsize=12)\n",
        "            axes[1, 1].set_title('Convergence Speed: Time to 50% Accuracy', fontsize=14, fontweight='bold')\n",
        "            axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "            # Add value labels\n",
        "            for i, (exp_id, t) in enumerate(valid_times.items()):\n",
        "                axes[1, 1].text(i, t, f'{t:.1f}m', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        # Use Google Drive path if in Colab, otherwise local\n",
        "        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
        "            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"FedAsync\" / \"comparisons\"\n",
        "        else:\n",
        "            comparison_dir = Path(\"./logs/FedAsync/comparisons\")\n",
        "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(comparison_dir / \"all_experiments_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"\\n‚úÖ Comparison plot saved: {comparison_dir / 'all_experiments_comparison.png'}\")\n",
        "        plt.show()\n",
        "        \n",
        "        # Print summary table\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EXPERIMENT COMPARISON SUMMARY\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
        "        print(\"-\"*80)\n",
        "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
        "            if exp_id in all_data:\n",
        "                df = all_data[exp_id]\n",
        "                cfg = experiments[exp_id]\n",
        "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
        "                      f\"{df['test_acc'].max():<10.4f} {df['test_acc'].iloc[-1]:<11.4f} \"\n",
        "                      f\"{df['time_min'].iloc[-1]:<12.2f}\")\n",
        "        print(\"=\"*80)\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No completed experiments found. Run experiments first.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No experiment results found. Run the experiments first using Section 8.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Comprehensive Results Visualization\n",
        "\n",
        "This section creates detailed plots for a single experiment (use after running Section 8).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive visualization suite for a single experiment\n",
        "# Use this after running experiments - specify the experiment ID and run folder\n",
        "# Example: run_dir = experiment_results['Exp1']['run_dir']\n",
        "\n",
        "# For demonstration, we'll use the last completed experiment\n",
        "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
        "    # Get the last completed experiment\n",
        "    completed_exps = {k: v for k, v in experiment_results.items() if v.get(\"status\") == \"completed\"}\n",
        "    if len(completed_exps) > 0:\n",
        "        # Use the last one\n",
        "        last_exp_id = list(completed_exps.keys())[-1]\n",
        "        run_dir = completed_exps[last_exp_id][\"run_dir\"]\n",
        "        print(f\"üìä Visualizing results for: {last_exp_id}\")\n",
        "        print(f\"üìÅ Run folder: {run_dir}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No completed experiments found. Run experiments first.\")\n",
        "        run_dir = None\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No experiment results found. Run experiments first.\")\n",
        "    run_dir = None\n",
        "\n",
        "if run_dir and Path(run_dir).exists():\n",
        "    csv_path = Path(run_dir) / \"FedAsync.csv\"\n",
        "    participation_path = Path(run_dir) / \"FedAsyncClientParticipation.csv\"\n",
        "    \n",
        "    if csv_path.exists():\n",
        "        df = pd.read_csv(csv_path)\n",
        "        \n",
        "        # Convert time to minutes for better readability\n",
        "        df['time_min'] = df['time'] / 60.0\n",
        "        \n",
        "        # ========== Plot 1: Accuracy vs Rounds ==========\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Test accuracy over rounds\n",
        "        axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
        "        train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
        "        train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n",
        "        if len(train_acc) > 0:\n",
        "            axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
        "        axes[0].set_xlabel('Round', fontsize=12)\n",
        "        axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "        axes[0].set_title('Accuracy vs Rounds', fontsize=14, fontweight='bold')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        axes[0].legend(fontsize=10)\n",
        "        axes[0].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Loss over rounds\n",
        "        axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
        "        train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
        "        train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n",
        "        if len(train_loss) > 0:\n",
        "            axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
        "        axes[1].set_xlabel('Round', fontsize=12)\n",
        "        axes[1].set_ylabel('Loss', fontsize=12)\n",
        "        axes[1].set_title('Loss vs Rounds', fontsize=14, fontweight='bold')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        axes[1].legend(fontsize=10)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(Path(run_dir) / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"‚úÖ Plot 1 saved: 1_accuracy_loss_vs_rounds.png\")\n",
        "        plt.show()\n",
        "        \n",
        "        # ========== Plot 2: Accuracy vs Wall Clock Time ==========\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Test accuracy over time\n",
        "        axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
        "        train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
        "        train_time = df[df['avg_train_acc'] > 0]['time_min']\n",
        "        if len(train_acc_time) > 0:\n",
        "            axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
        "        axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "        axes[0].set_ylabel('Accuracy', fontsize=12)\n",
        "        axes[0].set_title('Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        axes[0].legend(fontsize=10)\n",
        "        axes[0].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Loss over time\n",
        "        axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
        "        train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
        "        train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n",
        "        if len(train_loss_time) > 0:\n",
        "            axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
        "        axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "        axes[1].set_ylabel('Loss', fontsize=12)\n",
        "        axes[1].set_title('Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "        axes[1].legend(fontsize=10)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(Path(run_dir) / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"‚úÖ Plot 2 saved: 2_accuracy_loss_vs_time.png\")\n",
        "        plt.show()\n",
        "        \n",
        "        # ========== Plot 3: Convergence Speed Analysis ==========\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "        \n",
        "        # Time to reach accuracy thresholds\n",
        "        thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
        "        times_to_threshold = []\n",
        "        rounds_to_threshold = []\n",
        "        reached_thresholds = []\n",
        "        \n",
        "        for thresh in thresholds:\n",
        "            mask = df['test_acc'] >= thresh\n",
        "            if mask.any():\n",
        "                first_idx = mask.idxmax()\n",
        "                times_to_threshold.append(df.loc[first_idx, 'time_min'])\n",
        "                rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n",
        "                reached_thresholds.append(thresh)\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        if len(reached_thresholds) > 0:\n",
        "            axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n",
        "            axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n",
        "            axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n",
        "            axes[0].set_title('Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
        "            axes[0].set_xticks(range(len(reached_thresholds)))\n",
        "            axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
        "            axes[0].grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            # Add value labels on bars\n",
        "            for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n",
        "                axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
        "            \n",
        "            axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
        "            axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n",
        "            axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n",
        "            axes[1].set_title('Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
        "            axes[1].set_xticks(range(len(reached_thresholds)))\n",
        "            axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
        "            axes[1].grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            # Add value labels on bars\n",
        "            for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n",
        "                axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(Path(run_dir) / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"‚úÖ Plot 3 saved: 3_convergence_speed.png\")\n",
        "        plt.show()\n",
        "        \n",
        "        # ========== Plot 4: Training Efficiency (Accuracy per Time) ==========\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "        \n",
        "        # Calculate efficiency: accuracy gain per minute\n",
        "        if len(df) > 1:\n",
        "            df_sorted = df.sort_values('time_min')\n",
        "            efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n",
        "            efficiency = efficiency.fillna(0)\n",
        "            \n",
        "            ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n",
        "            ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n",
        "            ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
        "            ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n",
        "            ax.set_title('Training Efficiency: Accuracy Gain Rate Over Time', fontsize=14, fontweight='bold')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            \n",
        "            # Highlight peak efficiency\n",
        "            max_eff_idx = efficiency.idxmax()\n",
        "            max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n",
        "            max_eff_val = efficiency.loc[max_eff_idx]\n",
        "            ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n",
        "            ax.legend(fontsize=10)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(Path(run_dir) / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"‚úÖ Plot 4 saved: 4_training_efficiency.png\")\n",
        "        plt.show()\n",
        "        \n",
        "        # ========== Plot 5: Client Participation Analysis ==========\n",
        "        if participation_path.exists():\n",
        "            part_df = pd.read_csv(participation_path)\n",
        "            \n",
        "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "            \n",
        "            # Client participation frequency\n",
        "            client_counts = part_df['client_id'].value_counts().sort_index()\n",
        "            axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n",
        "            axes[0, 0].set_xlabel('Client ID', fontsize=11)\n",
        "            axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n",
        "            axes[0, 0].set_title('Client Participation Frequency', fontsize=12, fontweight='bold')\n",
        "            axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "            \n",
        "            # Client participation over rounds\n",
        "            participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n",
        "            axes[0, 1].plot(participation_by_round.index, participation_by_round.values, \n",
        "                           marker='o', markersize=4, linewidth=2, color='coral')\n",
        "            axes[0, 1].set_xlabel('Round', fontsize=11)\n",
        "            axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n",
        "            axes[0, 1].set_title('Client Participation per Round', fontsize=12, fontweight='bold')\n",
        "            axes[0, 1].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Average client accuracy over time\n",
        "            client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n",
        "            axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values, \n",
        "                            marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n",
        "            axes[1, 0].set_xlabel('Round', fontsize=11)\n",
        "            axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n",
        "            axes[1, 0].set_title('Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n",
        "            axes[1, 0].grid(True, alpha=0.3)\n",
        "            axes[1, 0].set_ylim([0, 1.0])\n",
        "            \n",
        "            # Client accuracy distribution (box plot)\n",
        "            if len(part_df) > 0:\n",
        "                client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values \n",
        "                              for cid in sorted(part_df['client_id'].unique())[:10]]  # Limit to first 10 clients\n",
        "                if len(client_accs) > 0:\n",
        "                    axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n",
        "                    axes[1, 1].set_xlabel('Client ID', fontsize=11)\n",
        "                    axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
        "                    axes[1, 1].set_title('Client Accuracy Distribution', fontsize=12, fontweight='bold')\n",
        "                    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "                    axes[1, 1].set_ylim([0, 1.0])\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(Path(run_dir) / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n",
        "            print(f\"‚úÖ Plot 5 saved: 5_client_participation.png\")\n",
        "            plt.show()\n",
        "        \n",
        "        # ========== Plot 6: Training Progress Summary ==========\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        \n",
        "        # Accuracy trajectory\n",
        "        axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
        "        train_acc_plot = df[df['avg_train_acc'] > 0]\n",
        "        if len(train_acc_plot) > 0:\n",
        "            axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'], \n",
        "                           'r--', linewidth=2, label='Train')\n",
        "        axes[0, 0].set_xlabel('Round', fontsize=11)\n",
        "        axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
        "        axes[0, 0].set_title('Accuracy Trajectory', fontsize=12, fontweight='bold')\n",
        "        axes[0, 0].legend(fontsize=10)\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "        axes[0, 0].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Loss trajectory\n",
        "        axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n",
        "        train_loss_plot = df[df['avg_train_loss'] > 0]\n",
        "        if len(train_loss_plot) > 0:\n",
        "            axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'], \n",
        "                           'r--', linewidth=2, label='Train')\n",
        "        axes[0, 1].set_xlabel('Round', fontsize=11)\n",
        "        axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
        "        axes[0, 1].set_title('Loss Trajectory', fontsize=12, fontweight='bold')\n",
        "        axes[0, 1].legend(fontsize=10)\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Accuracy vs time\n",
        "        axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
        "        if len(train_acc_plot) > 0:\n",
        "            train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n",
        "            axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'], \n",
        "                            'r--', linewidth=2, label='Train')\n",
        "        axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n",
        "        axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
        "        axes[1, 0].set_title('Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n",
        "        axes[1, 0].legend(fontsize=10)\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "        axes[1, 0].set_ylim([0, 1.0])\n",
        "        \n",
        "        # Round progression over time\n",
        "        axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n",
        "        axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n",
        "        axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n",
        "        axes[1, 1].set_title('Round Progression Over Time', fontsize=12, fontweight='bold')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Add rate annotation\n",
        "        if len(df) > 1:\n",
        "            total_time = df['time_min'].iloc[-1]\n",
        "            total_rounds = df['total_agg'].iloc[-1]\n",
        "            rate = total_rounds / total_time if total_time > 0 else 0\n",
        "            axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min', \n",
        "                           transform=axes[1, 1].transAxes, fontsize=10,\n",
        "                           verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.savefig(Path(run_dir) / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n",
        "        print(f\"‚úÖ Plot 6 saved: 6_training_summary.png\")\n",
        "        plt.show()\n",
        "        \n",
        "        # ========== Print Summary Statistics ==========\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"EXPERIMENT SUMMARY STATISTICS\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Total Rounds: {df['total_agg'].max()}\")\n",
        "        print(f\"Total Time: {df['time_min'].iloc[-1]:.2f} minutes ({df['time'].iloc[-1]:.2f} seconds)\")\n",
        "        print(f\"Final Test Accuracy: {df['test_acc'].iloc[-1]:.4f}\")\n",
        "        print(f\"Best Test Accuracy: {df['test_acc'].max():.4f} (Round {df.loc[df['test_acc'].idxmax(), 'total_agg']})\")\n",
        "        if len(df) > 1:\n",
        "            print(f\"Average Round Time: {(df['time'].iloc[-1] / df['total_agg'].iloc[-1]):.2f} seconds\")\n",
        "            print(f\"Rounds per Minute: {(df['total_agg'].iloc[-1] / df['time_min'].iloc[-1]):.2f}\")\n",
        "        print(\"=\"*60)\n",
        "        \n",
        "    else:\n",
        "        print(\"Results file not found yet.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No experiment results found. Run experiments first using Section 8.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
