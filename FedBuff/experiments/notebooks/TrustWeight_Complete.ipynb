{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrustWeight: Complete Self-Contained Notebook\n",
    "\n",
    "This notebook contains all code needed to run TrustWeight experiments:\n",
    "- Library installation\n",
    "- Data downloading and loading\n",
    "- All TrustWeight implementation\n",
    "- Logging and checkpointing\n",
    "\n",
    "**Run cells sequentially from top to bottom.**\n",
    "\n",
    "## Google Colab Setup\n",
    "This notebook is configured to save all results (logs, checkpoints, models) to Google Drive.\n",
    "Data will be downloaded locally for faster access.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Google Colab Setup (Mount Drive)\n",
    "\n",
    "**Skip this section if running locally.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Colab: Mount Google Drive\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    # Mount Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "    # Define the target directory for saving results\n",
    "    OUTPUT_DIR = \"/content/drive/MyDrive/colab/dml_project\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"âœ… Google Drive mounted\")\n",
    "    print(f\"âœ… Output directory set to: {OUTPUT_DIR}\")\n",
    "    print(f\"ðŸ“ All logs, checkpoints, and results will be saved to Google Drive\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    OUTPUT_DIR = None\n",
    "    print(\"âš ï¸  Not running in Google Colab - using local paths\")\n",
    "\n",
    "# Install required packages\n",
    "%pip install torch torchvision pytorch-lightning pyyaml numpy matplotlib pandas -q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence libraries\n",
    "import os\n",
    "import logging, warnings\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "os.environ[\"LIGHTNING_DISABLE_RICH\"] = \"1\"\n",
    "for name in [\n",
    "    \"pytorch_lightning\", \"lightning\", \"lightning.pytorch\",\n",
    "    \"lightning_fabric\", \"lightning_utilities\", \"torch\", \"torchvision\",\n",
    "]:\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "    logging.getLogger(name).propagate = False\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Core imports\n",
    "import time\n",
    "import csv\n",
    "import threading\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup paths based on environment\n",
    "if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "    # Google Colab: Save everything to Drive except data\n",
    "    BASE_OUTPUT_DIR = Path(OUTPUT_DIR)\n",
    "    DATA_DIR = Path(\"./data\")  # Local for faster access\n",
    "    print(f\"âœ… Google Colab mode: Results â†’ {BASE_OUTPUT_DIR}\")\n",
    "    print(f\"âœ… Data directory: {DATA_DIR} (local)\")\n",
    "else:\n",
    "    # Local execution: Use current directory\n",
    "    BASE_OUTPUT_DIR = Path(\".\")\n",
    "    DATA_DIR = Path(\"./data\")\n",
    "    print(f\"âœ… Local mode: Results â†’ {BASE_OUTPUT_DIR}\")\n",
    "\n",
    "print(\"âœ… Libraries imported\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utility Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Helper Functions ==========\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed all RNGs for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    \"\"\"Return the first available computation device (CUDA/MPS/CPU).\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def _device_to_accelerator(device: torch.device) -> str:\n",
    "    \"\"\"Convert torch device to PyTorch Lightning accelerator string.\"\"\"\n",
    "    if device.type == \"cuda\":\n",
    "        return \"gpu\"\n",
    "    if device.type == \"mps\":\n",
    "        return \"mps\"\n",
    "    return \"cpu\"\n",
    "\n",
    "print(\"âœ… Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Model Utilities ==========\n",
    "\n",
    "def build_resnet18(num_classes: int = 10, pretrained: bool = False) -> nn.Module:\n",
    "    \"\"\"Create ResNet-18 adapted for CIFAR-10.\"\"\"\n",
    "    if pretrained:\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        m = models.resnet18(weights=None)\n",
    "    # CIFAR-10: 32x32 -> use 3x3 conv, stride 1, no maxpool\n",
    "    m.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    m.maxpool = nn.Identity()\n",
    "    # Replace classifier\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    m.num_classes = num_classes\n",
    "    return m\n",
    "\n",
    "\n",
    "def state_to_list(state: Dict[str, torch.Tensor]) -> List[torch.Tensor]:\n",
    "    \"\"\"Flatten a state_dict to a list of tensors on CPU.\"\"\"\n",
    "    return [t.detach().cpu().clone() for _, t in state.items()]\n",
    "\n",
    "\n",
    "def list_to_state(template: Dict[str, torch.Tensor], arrs: List[torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Rebuild a state_dict from a list of tensors using a template.\"\"\"\n",
    "    out: Dict[str, torch.Tensor] = {}\n",
    "    for (k, v), a in zip(template.items(), arrs):\n",
    "        out[k] = a.to(v.device).type_as(v)\n",
    "    return out\n",
    "\n",
    "print(\"âœ… Model utilities defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Data Loading and Partitioning ==========\n",
    "\n",
    "class DataDistributor:\n",
    "    \"\"\"Data distributor for federated learning with Dirichlet partitioning.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name: str, data_dir: str = \"./data\"):\n",
    "        self.dataset_name = dataset_name.lower()\n",
    "        self.data_dir = data_dir\n",
    "        self.train_dataset, self.test_dataset, self.num_classes = self._load_dataset()\n",
    "        self.partitions = None\n",
    "\n",
    "    def _load_dataset(self) -> Tuple[Any, Any, int]:\n",
    "        \"\"\"Load CIFAR-10 dataset with augmentation.\"\"\"\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            # Strong data pipeline: augmentation for train, normalization for test\n",
    "            transform_train = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.4914, 0.4822, 0.4465),\n",
    "                    std=(0.2470, 0.2435, 0.2616),\n",
    "                ),\n",
    "            ])\n",
    "            transform_test = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.4914, 0.4822, 0.4465),\n",
    "                    std=(0.2470, 0.2435, 0.2616),\n",
    "                ),\n",
    "            ])\n",
    "            train = datasets.CIFAR10(self.data_dir, train=True, download=True, transform=transform_train)\n",
    "            test = datasets.CIFAR10(self.data_dir, train=False, download=True, transform=transform_test)\n",
    "            num_classes = 10\n",
    "        else:\n",
    "            raise ValueError(f\"Dataset '{self.dataset_name}' not supported. Use 'cifar10'.\")\n",
    "        return train, test, num_classes\n",
    "\n",
    "    def distribute_data(self, num_clients: int, alpha: float = 0.5, seed: int = 42):\n",
    "        \"\"\"Partition data using Dirichlet distribution.\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        targets = np.array(self.train_dataset.targets)\n",
    "        self.partitions = {i: [] for i in range(num_clients)}\n",
    "\n",
    "        for cls in range(self.num_classes):\n",
    "            idxs = np.where(targets == cls)[0]\n",
    "            np.random.shuffle(idxs)\n",
    "            proportions = np.random.dirichlet(alpha=np.repeat(alpha, num_clients))\n",
    "            proportions = np.array([p * len(idxs) for p in proportions]).astype(int)\n",
    "\n",
    "            start = 0\n",
    "            for client_id, size in enumerate(proportions):\n",
    "                self.partitions[client_id].extend(idxs[start:start + size])\n",
    "                start += size\n",
    "\n",
    "        for cid in self.partitions:\n",
    "            np.random.shuffle(self.partitions[cid])\n",
    "\n",
    "    def get_client_data(self, client_id: int) -> Subset:\n",
    "        \"\"\"Get data subset for a specific client.\"\"\"\n",
    "        if self.partitions is None:\n",
    "            raise ValueError(\"Data not distributed yet. Call distribute_data() first.\")\n",
    "        indices = self.partitions[client_id]\n",
    "        return Subset(self.train_dataset, indices)\n",
    "\n",
    "print(\"âœ… DataDistributor class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TrustWeight Client Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TrustWeight Client ==========\n",
    "\n",
    "def _testloader(root: str, batch_size: int = 256):\n",
    "    \"\"\"Create test dataloader.\"\"\"\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=tfm)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "def _evaluate(model: nn.Module, loader: DataLoader, device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate model on test set.\"\"\"\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    total, correct, loss_sum = 0, 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = crit(logits, y)\n",
    "            loss_sum += float(loss.item()) * y.size(0)\n",
    "            total += y.size(0)\n",
    "            correct += (logits.argmax(1) == y).sum().item()\n",
    "    return loss_sum / max(1, total), correct / max(1, total)\n",
    "\n",
    "\n",
    "class LitCifar(pl.LightningModule):\n",
    "    \"\"\"PyTorch Lightning module for CIFAR-10 training.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_model: nn.Module, lr: float = 1e-3, momentum: float = 0.9, weight_decay: float = 5e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"base_model\"])\n",
    "        self.model = base_model\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        # Store optimizer params directly\n",
    "        self._optimizer_lr = lr\n",
    "        self._optimizer_momentum = momentum\n",
    "        self._optimizer_weight_decay = weight_decay\n",
    "        self._train_loss_sum = 0.0\n",
    "        self._train_correct = 0\n",
    "        self._train_total = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, _batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        pred = logits.argmax(1)\n",
    "        self._train_loss_sum += float(loss.item()) * y.size(0)\n",
    "        self._train_correct += (pred == y).sum().item()\n",
    "        self._train_total += y.size(0)\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self._train_loss_sum = 0.0\n",
    "        self._train_correct = 0\n",
    "        self._train_total = 0\n",
    "\n",
    "    def get_epoch_metrics(self) -> Tuple[float, float]:\n",
    "        if self._train_total == 0:\n",
    "            return 0.0, 0.0\n",
    "        return self._train_loss_sum / self._train_total, self._train_correct / self._train_total\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self._optimizer_lr,\n",
    "            momentum=self._optimizer_momentum,\n",
    "            weight_decay=self._optimizer_weight_decay\n",
    "        )\n",
    "\n",
    "\n",
    "class AsyncClient:\n",
    "    \"\"\"TrustWeight client that trains locally and submits updates to server.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        cid: int,\n",
    "        cfg: dict,\n",
    "        subset: Subset,\n",
    "        work_dir: str = \"./checkpoints/clients\",\n",
    "        base_delay: float = 0.0,\n",
    "        slow: bool = False,\n",
    "        delay_ranges: Optional[tuple] = None,\n",
    "        jitter: float = 0.0,\n",
    "        fix_delay: bool = True,\n",
    "    ):\n",
    "        self.cid = cid\n",
    "        self.cfg = cfg\n",
    "        self.device = get_device()\n",
    "\n",
    "        base = build_resnet18(num_classes=cfg[\"data\"][\"num_classes\"], pretrained=False)\n",
    "        lr = float(cfg[\"clients\"][\"lr\"])\n",
    "        momentum = float(cfg[\"clients\"].get(\"momentum\", 0.9))\n",
    "        weight_decay = float(cfg[\"clients\"].get(\"weight_decay\", 5e-4))\n",
    "        self.lit = LitCifar(base, lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "\n",
    "        self.loader = DataLoader(subset, batch_size=int(cfg[\"clients\"][\"batch_size\"]),\n",
    "                                 shuffle=True, num_workers=0)\n",
    "\n",
    "        self.base_delay = float(base_delay)\n",
    "        self.slow = bool(slow)\n",
    "        self.delay_ranges = delay_ranges\n",
    "        self.jitter = float(jitter)\n",
    "        self.fix_delay = bool(fix_delay)\n",
    "\n",
    "        if self.fix_delay and self.delay_ranges is not None:\n",
    "            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n",
    "            if self.slow:\n",
    "                self.base_delay = random.uniform(float(a_s), float(b_s))\n",
    "            else:\n",
    "                self.base_delay = random.uniform(float(a_f), float(b_f))\n",
    "\n",
    "        self.accelerator = _device_to_accelerator(self.device)\n",
    "        self.testloader = _testloader(cfg[\"data\"][\"data_dir\"])\n",
    "\n",
    "    def _to_list(self) -> List[torch.Tensor]:\n",
    "        return state_to_list(self.lit.model.state_dict())\n",
    "\n",
    "    def _from_list(self, arrs: List[torch.Tensor]) -> None:\n",
    "        sd = self.lit.model.state_dict()\n",
    "        new_sd = list_to_state(sd, arrs)\n",
    "        self.lit.model.load_state_dict(new_sd, strict=True)\n",
    "        self.lit.to(self.device)\n",
    "\n",
    "    def _sleep_delay(self):\n",
    "        global_d = float(self.cfg.get(\"server_runtime\", {}).get(\"client_delay\", 0.0))\n",
    "        base = self.base_delay\n",
    "\n",
    "        if not self.fix_delay and self.delay_ranges is not None:\n",
    "            (a_s, b_s), (a_f, b_f) = self.delay_ranges\n",
    "            if self.slow:\n",
    "                base = random.uniform(float(a_s), float(b_s))\n",
    "            else:\n",
    "                base = random.uniform(float(a_f), float(b_f))\n",
    "\n",
    "        jit = random.uniform(-self.jitter, self.jitter) if self.jitter > 0.0 else 0.0\n",
    "        delay = max(0.0, global_d + base + jit)\n",
    "        if delay > 0.0:\n",
    "            time.sleep(delay)\n",
    "\n",
    "    def fit_once(self, server) -> bool:\n",
    "        params, version = server.get_global()\n",
    "        self._from_list(params)\n",
    "\n",
    "        self._sleep_delay()\n",
    "\n",
    "        epochs = int(self.cfg[\"clients\"][\"local_epochs\"])\n",
    "        grad_clip = float(self.cfg[\"clients\"].get(\"grad_clip\", 1.0))\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=epochs,\n",
    "            accelerator=self.accelerator,\n",
    "            devices=1,\n",
    "            enable_checkpointing=False,\n",
    "            logger=False,\n",
    "            enable_model_summary=False,\n",
    "            num_sanity_val_steps=0,\n",
    "            enable_progress_bar=False,\n",
    "            callbacks=[],\n",
    "            gradient_clip_val=grad_clip,\n",
    "            gradient_clip_algorithm=\"norm\",\n",
    "        )\n",
    "        start = time.time()\n",
    "        trainer.fit(self.lit, train_dataloaders=self.loader)\n",
    "        duration = time.time() - start\n",
    "\n",
    "        train_loss, train_acc = self.lit.get_epoch_metrics()\n",
    "        test_loss, test_acc = _evaluate(self.lit.model, self.testloader, self.device)\n",
    "\n",
    "        new_params = self._to_list()\n",
    "        num_examples = len(self.loader.dataset)\n",
    "\n",
    "        server.submit_update(\n",
    "            client_id=self.cid,\n",
    "            base_version=version,\n",
    "            new_params=new_params,\n",
    "            num_samples=num_examples,\n",
    "            train_time_s=duration,\n",
    "            train_loss=train_loss,\n",
    "            train_acc=train_acc,\n",
    "            test_loss=test_loss,\n",
    "            test_acc=test_acc,\n",
    "        )\n",
    "        return not server.should_stop()\n",
    "\n",
    "print(\"âœ… TrustWeight client classes defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== TrustWeight Server Implementation ==========\n",
    "# Adapted from TrustWeight/server.py for notebook use\n",
    "\n",
    "from collections import OrderedDict as ODType\n",
    "from dataclasses import dataclass\n",
    "\n",
    "def _testloader(root: str, batch_size: int = 256) -> DataLoader:\n",
    "    \"\"\"Create test dataloader.\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    ds = datasets.CIFAR10(root=root, train=False, download=True, transform=transform)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "def _flatten_state(state: ODType[str, torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"Flatten a state_dict into a 1D tensor.\"\"\"\n",
    "    return torch.cat([p.reshape(-1) for p in state.values()])\n",
    "\n",
    "def _flatten_state_by_template(\n",
    "    state: Dict[str, torch.Tensor], template: ODType[str, torch.Tensor]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Flatten state according to template key ordering.\"\"\"\n",
    "    return torch.cat([state[k].reshape(-1) for k in template.keys()])\n",
    "\n",
    "def _vector_to_state(\n",
    "    vec: torch.Tensor, template: ODType[str, torch.Tensor]\n",
    ") -> ODType[str, torch.Tensor]:\n",
    "    \"\"\"Convert flattened vector back to state_dict.\"\"\"\n",
    "    new_state: ODType[str, torch.Tensor] = type(template)()\n",
    "    offset = 0\n",
    "    for k, t in template.items():\n",
    "        numel = t.numel()\n",
    "        new_state[k] = vec[offset : offset + numel].view_as(t).clone()\n",
    "        offset += numel\n",
    "    assert offset == vec.numel()\n",
    "    return new_state\n",
    "\n",
    "@dataclass\n",
    "class ClientUpdateState:\n",
    "    \"\"\"State for a client update.\"\"\"\n",
    "    client_id: int\n",
    "    base_version: int\n",
    "    new_params: ODType[str, torch.Tensor]\n",
    "    num_samples: int\n",
    "    train_time_s: float\n",
    "    delta_loss: float\n",
    "    loss_before: float\n",
    "    loss_after: float\n",
    "    train_acc: float\n",
    "    test_loss: float\n",
    "    test_acc: float\n",
    "    arrival_ts: float\n",
    "\n",
    "class AsyncServer:\n",
    "    \"\"\"Central server maintaining global model and asynchronous buffer.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        global_model: torch.nn.Module,\n",
    "        total_train_samples: int,\n",
    "        buffer_size: int = 5,\n",
    "        buffer_timeout_s: float = 5.0,\n",
    "        use_sample_weighing: bool = True,\n",
    "        target_accuracy: float = 0.8,\n",
    "        max_rounds: Optional[int] = None,\n",
    "        eval_interval_s: int = 15,\n",
    "        data_dir: str = \"./data\",\n",
    "        checkpoints_dir: str = \"./checkpoints/TrustWeight\",\n",
    "        logs_dir: str = \"./logs/TrustWeight\",\n",
    "        global_log_csv: Optional[str] = None,\n",
    "        client_participation_csv: Optional[str] = None,\n",
    "        final_model_path: Optional[str] = None,\n",
    "        resume: bool = True,\n",
    "        device: Optional[torch.device] = None,\n",
    "        eta: float = 1.0,\n",
    "    ):\n",
    "        self.device = device or get_device()\n",
    "        \n",
    "        # Extract num_classes from global_model\n",
    "        # Try to get it from model attribute, or infer from fc layer\n",
    "        if hasattr(global_model, 'num_classes'):\n",
    "            self.num_classes = global_model.num_classes\n",
    "        elif hasattr(global_model, 'fc'):\n",
    "            self.num_classes = global_model.fc.out_features\n",
    "        else:\n",
    "            # Default for CIFAR-10\n",
    "            self.num_classes = 10\n",
    "        \n",
    "        # data / evaluation\n",
    "        self.testloader = _testloader(data_dir, batch_size=256)\n",
    "        \n",
    "        # global model and version history\n",
    "        self._template_state: ODType[str, torch.Tensor] = ODType(\n",
    "            (k, v.detach().cpu().clone()) for k, v in global_model.state_dict().items()\n",
    "        )\n",
    "        self._global_state: ODType[str, torch.Tensor] = ODType(\n",
    "            (k, v.clone()) for k, v in self._template_state.items()\n",
    "        )\n",
    "        \n",
    "        self._model_versions: List[ODType[str, torch.Tensor]] = [\n",
    "            ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "        ]\n",
    "        self._version: int = 0\n",
    "        \n",
    "        # strategy encapsulating all math - use eta from config\n",
    "        dim = _flatten_state(self._global_state).numel()\n",
    "        strategy_cfg = TrustWeightedConfig(eta=eta)\n",
    "        self.strategy = TrustWeightedAsyncStrategy(dim=dim, cfg=strategy_cfg)\n",
    "        \n",
    "        # async buffer\n",
    "        self.buffer: List[ClientUpdateState] = []\n",
    "        self.buffer_size: int = buffer_size\n",
    "        self.buffer_timeout_s: float = buffer_timeout_s\n",
    "        self._last_flush_ts: float = time.time()\n",
    "        \n",
    "        # logging / control\n",
    "        self.global_log_path = Path(global_log_csv) if global_log_csv else Path(logs_dir) / \"TrustWeight.csv\"\n",
    "        self.global_log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self._init_global_log()\n",
    "        \n",
    "        self.client_log_path = Path(client_participation_csv) if client_participation_csv else Path(logs_dir) / \"TrustWeightClientParticipation.csv\"\n",
    "        self.client_log_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self._init_client_log()\n",
    "        \n",
    "        self.eval_interval_s: float = eval_interval_s\n",
    "        self.target_accuracy: float = target_accuracy\n",
    "        self.max_rounds: int = max_rounds if max_rounds is not None else 1000\n",
    "        self.update_clip_norm: float = 10.0  # Default clipping norm\n",
    "        \n",
    "        self._num_aggregations: int = 0\n",
    "        self._stop: bool = False\n",
    "        self._stop_reason: str = \"\"\n",
    "        self._lock = threading.Lock()\n",
    "        self._agg_lock = threading.Lock()\n",
    "        self._start_ts: float = time.time()\n",
    "        \n",
    "        # Evaluation timer (optional, for periodic status)\n",
    "        self._eval_thread = None\n",
    "    \n",
    "    def _init_global_log(self) -> None:\n",
    "        \"\"\"Initialize the global training CSV.\"\"\"\n",
    "        if self.global_log_path.exists():\n",
    "            return\n",
    "        with self.global_log_path.open(\"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"total_agg\", \"avg_train_loss\", \"avg_train_acc\",\n",
    "                \"test_loss\", \"test_acc\", \"time\",\n",
    "            ])\n",
    "    \n",
    "    def _append_global_log(\n",
    "        self,\n",
    "        total_agg: int,\n",
    "        avg_train_loss: float,\n",
    "        avg_train_acc: float,\n",
    "        test_loss: float,\n",
    "        test_acc: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Append a single aggregation row to the global CSV.\"\"\"\n",
    "        ts = time.time() - self._start_ts\n",
    "        with self.global_log_path.open(\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                int(total_agg),\n",
    "                float(avg_train_loss),\n",
    "                float(avg_train_acc),\n",
    "                float(test_loss),\n",
    "                float(test_acc),\n",
    "                ts,\n",
    "            ])\n",
    "    \n",
    "    def _init_client_log(self) -> None:\n",
    "        \"\"\"Initialize the client participation CSV.\"\"\"\n",
    "        if self.client_log_path.exists():\n",
    "            return\n",
    "        with self.client_log_path.open(\"w\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                \"client_id\", \"local_train_loss\", \"local_train_acc\",\n",
    "                \"local_test_loss\", \"local_test_acc\", \"total_agg\", \"staleness\",\n",
    "            ])\n",
    "    \n",
    "    def _append_client_participation_log(\n",
    "        self,\n",
    "        total_agg: int,\n",
    "        updates: List[ClientUpdateState],\n",
    "        staleness_list: List[float],\n",
    "    ) -> None:\n",
    "        \"\"\"Append one row per client update.\"\"\"\n",
    "        with self.client_log_path.open(\"a\", newline=\"\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            for u, tau_i in zip(updates, staleness_list):\n",
    "                writer.writerow([\n",
    "                    int(u.client_id),\n",
    "                    float(u.loss_after),\n",
    "                    float(u.train_acc),\n",
    "                    float(u.test_loss),\n",
    "                    float(u.test_acc),\n",
    "                    int(total_agg),\n",
    "                    float(tau_i),\n",
    "                ])\n",
    "    \n",
    "    def should_stop(self) -> bool:\n",
    "        with self._lock:\n",
    "            return self._stop\n",
    "    \n",
    "    def mark_stop(self, reason: str = \"\") -> None:\n",
    "        with self._lock:\n",
    "            if not self._stop:\n",
    "                self._stop = True\n",
    "                if reason:\n",
    "                    self._stop_reason = reason\n",
    "                print(f\"[Server] Stopping: {self._stop_reason}\")\n",
    "    \n",
    "    def get_global_model(self) -> Tuple[int, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"Return (version, state_dict) of the current global model.\"\"\"\n",
    "        with self._lock:\n",
    "            version = self._version\n",
    "            state = ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "        return version, state\n",
    "    \n",
    "    def _make_model_from_state(self, state: Dict[str, torch.Tensor], num_classes: int) -> torch.nn.Module:\n",
    "        model = build_resnet18(num_classes=num_classes)\n",
    "        model.load_state_dict(state)\n",
    "        return model.to(self.device)\n",
    "    \n",
    "    def _evaluate_global(self, num_classes: int) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate the current global model on the test set.\"\"\"\n",
    "        model = self._make_model_from_state(self._global_state, num_classes)\n",
    "        model.eval()\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        total_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_examples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.testloader:\n",
    "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                total_loss += loss.item() * xb.size(0)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                total_correct += (preds == yb).sum().item()\n",
    "                total_examples += xb.size(0)\n",
    "        if total_examples == 0:\n",
    "            return 0.0, 0.0\n",
    "        return total_loss / total_examples, total_correct / total_examples\n",
    "    \n",
    "    def _flush_buffer_if_needed(self) -> None:\n",
    "        now = time.time()\n",
    "        should_flush = False\n",
    "        if len(self.buffer) >= self.buffer_size:\n",
    "            should_flush = True\n",
    "        elif (now - self._last_flush_ts) >= self.buffer_timeout_s and self.buffer:\n",
    "            should_flush = True\n",
    "        \n",
    "        if not should_flush:\n",
    "            return\n",
    "        \n",
    "        # copy buffer locally under lock then release for heavy work\n",
    "        with self._lock:\n",
    "            buffer_copy = list(self.buffer)\n",
    "            self.buffer.clear()\n",
    "            self._last_flush_ts = now\n",
    "        \n",
    "        # Serialize aggregations to avoid version races\n",
    "        with self._agg_lock:\n",
    "            self._aggregate(buffer_copy)\n",
    "    \n",
    "    def _aggregate(self, updates: List[ClientUpdateState]) -> None:\n",
    "        \"\"\"Aggregate a batch of client updates and log to CSVs.\"\"\"\n",
    "        if not updates:\n",
    "            return\n",
    "        \n",
    "        # Snapshot of current global parameters and version history\n",
    "        with self._lock:\n",
    "            global_vec = _flatten_state_by_template(self._global_state, self._template_state)\n",
    "            version_now = self._version\n",
    "            model_versions = list(self._model_versions)\n",
    "        \n",
    "        # Construct per-update vectors and metadata for the strategy\n",
    "        update_vectors: List[Dict[str, torch.Tensor]] = []\n",
    "        staleness_list: List[float] = []\n",
    "        valid_updates: List[ClientUpdateState] = []\n",
    "        \n",
    "        for u in updates:\n",
    "            base_state = model_versions[u.base_version]\n",
    "            base_vec = _flatten_state_by_template(base_state, self._template_state)\n",
    "            new_vec = _flatten_state_by_template(u.new_params, self._template_state)\n",
    "            ui = new_vec - base_vec\n",
    "            \n",
    "            # Skip bad updates\n",
    "            if not torch.isfinite(ui).all():\n",
    "                print(f\"[Server] Dropping client {u.client_id} update due to NaN/Inf values\")\n",
    "                continue\n",
    "            if self.update_clip_norm > 0:\n",
    "                norm = torch.norm(ui)\n",
    "                if torch.isfinite(norm) and norm.item() > self.update_clip_norm:\n",
    "                    ui = ui * (self.update_clip_norm / (norm + 1e-12))\n",
    "            \n",
    "            # Ï„_i = current-server-version - base_version\n",
    "            tau_i = float(max(0, version_now - u.base_version))\n",
    "            staleness_list.append(tau_i)\n",
    "            \n",
    "            delta_loss = float(u.delta_loss)\n",
    "            update_vectors.append({\n",
    "                \"u\": ui,\n",
    "                \"tau\": torch.tensor(tau_i, dtype=torch.float32),\n",
    "                \"num_samples\": torch.tensor(float(u.num_samples), dtype=torch.float32),\n",
    "                \"delta_loss\": torch.tensor(delta_loss, dtype=torch.float32),\n",
    "            })\n",
    "            valid_updates.append(u)\n",
    "        \n",
    "        if not update_vectors:\n",
    "            print(\"[Server] Buffer flush skipped: no valid updates after filtering.\")\n",
    "            return\n",
    "        \n",
    "        # Run the trust-weighted aggregation strategy\n",
    "        new_global_vec, agg_metrics = self.strategy.aggregate(global_vec, update_vectors)\n",
    "        \n",
    "        # Map back into parameter state_dict form\n",
    "        new_state = _vector_to_state(new_global_vec, self._template_state)\n",
    "        \n",
    "        # Compute average local train metrics\n",
    "        avg_train_loss = sum(u.loss_after for u in valid_updates) / len(valid_updates)\n",
    "        avg_train_acc = sum(u.train_acc for u in valid_updates) / len(valid_updates)\n",
    "        \n",
    "        # Commit the new global model\n",
    "        with self._lock:\n",
    "            self._global_state = ODType((k, v.clone()) for k, v in new_state.items())\n",
    "            self._model_versions.append(\n",
    "                ODType((k, v.clone()) for k, v in self._global_state.items())\n",
    "            )\n",
    "            self._version = len(self._model_versions) - 1\n",
    "            self._num_aggregations += 1\n",
    "            total_agg = self._num_aggregations\n",
    "        \n",
    "        # Evaluate updated global model\n",
    "        test_loss, test_acc = self._evaluate_global(self.num_classes)\n",
    "        \n",
    "        # Log metrics\n",
    "        self._append_global_log(\n",
    "            total_agg=total_agg,\n",
    "            avg_train_loss=avg_train_loss,\n",
    "            avg_train_acc=avg_train_acc,\n",
    "            test_loss=test_loss,\n",
    "            test_acc=test_acc,\n",
    "        )\n",
    "        self._append_client_participation_log(\n",
    "            total_agg=total_agg,\n",
    "            updates=valid_updates,\n",
    "            staleness_list=staleness_list,\n",
    "        )\n",
    "        \n",
    "        print(\n",
    "            f\"[Server] Aggregated {len(valid_updates)} updates -> agg={total_agg} \"\n",
    "            f\"(avg_tau={agg_metrics.get('avg_tau', 0.0):.3f}, \"\n",
    "            f\"test_loss={test_loss:.4f}, test_acc={test_acc:.4f})\"\n",
    "        )\n",
    "        \n",
    "        # Stopping conditions\n",
    "        if test_acc >= self.target_accuracy:\n",
    "            self.mark_stop(f\"target accuracy {test_acc:.4f} reached\")\n",
    "        if total_agg >= self.max_rounds:\n",
    "            self.mark_stop(\"max aggregation rounds reached\")\n",
    "    \n",
    "    def submit_update(\n",
    "        self,\n",
    "        client_id: int,\n",
    "        base_version: int,\n",
    "        new_params: Dict[str, torch.Tensor],\n",
    "        num_samples: int,\n",
    "        train_time_s: float,\n",
    "        delta_loss: float,\n",
    "        loss_before: float,\n",
    "        loss_after: float,\n",
    "        train_acc: float,\n",
    "        test_loss: float,\n",
    "        test_acc: float,\n",
    "    ) -> None:\n",
    "        \"\"\"Entry point called by clients after local training.\"\"\"\n",
    "        cu = ClientUpdateState(\n",
    "            client_id=client_id,\n",
    "            base_version=base_version,\n",
    "            new_params=new_params,\n",
    "            num_samples=num_samples,\n",
    "            train_time_s=float(train_time_s),\n",
    "            delta_loss=float(delta_loss),\n",
    "            loss_before=float(loss_before),\n",
    "            loss_after=float(loss_after),\n",
    "            train_acc=float(train_acc),\n",
    "            test_loss=float(test_loss),\n",
    "            test_acc=float(test_acc),\n",
    "            arrival_ts=time.time(),\n",
    "        )\n",
    "        with self._lock:\n",
    "            self.buffer.append(cu)\n",
    "        self._flush_buffer_if_needed()\n",
    "    \n",
    "    def start_eval_timer(self):\n",
    "        \"\"\"Start periodic evaluation timer.\"\"\"\n",
    "        def _loop():\n",
    "            next_ts = time.time() + self.eval_interval_s\n",
    "            while True:\n",
    "                now = time.time()\n",
    "                sleep_for = max(0.0, next_ts - now)\n",
    "                time.sleep(sleep_for)\n",
    "                with self._lock:\n",
    "                    if self._stop:\n",
    "                        break\n",
    "                next_ts += self.eval_interval_s\n",
    "        self._eval_thread = threading.Thread(target=_loop, daemon=True)\n",
    "        self._eval_thread.start()\n",
    "    \n",
    "    def wait(self) -> None:\n",
    "        \"\"\"Block until training is finished.\"\"\"\n",
    "        try:\n",
    "            while not self.should_stop():\n",
    "                time.sleep(0.2)\n",
    "        finally:\n",
    "            self.mark_stop(self._stop_reason or \"training finished\")\n",
    "\n",
    "print(\"âœ… TrustWeight server implementation defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== All 6 Experiment Configurations ==========\n",
    "\n",
    "# Helper function to get paths (works for both Colab and local)\n",
    "def get_paths(exp_id: str):\n",
    "    \"\"\"Get paths for experiment, using Google Drive if in Colab.\"\"\"\n",
    "    if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "        base = BASE_OUTPUT_DIR\n",
    "    else:\n",
    "        base = Path(\".\")\n",
    "    \n",
    "    return {\n",
    "        \"data_dir\": str(DATA_DIR),  # Always local for faster access\n",
    "        \"checkpoints_dir\": str(base / \"checkpoints\" / \"TrustWeight\" / exp_id),\n",
    "        \"logs_dir\": str(base / \"logs\" / \"TrustWeight\" / exp_id),\n",
    "        \"results_dir\": str(base / \"results\" / \"TrustWeight\" / exp_id),\n",
    "    }\n",
    "\n",
    "experiments = {\n",
    "    \"Exp1\": {\n",
    "        \"name\": \"IID (alpha=1000), no stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),  # Use DATA_DIR variable\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 0,\n",
    "            \"delay_slow_range\": [0.0, 0.0],\n",
    "            \"delay_fast_range\": [0.0, 0.0],\n",
    "            \"jitter_per_round\": 0.0,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 1000.0,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp1\")\n",
    "    },\n",
    "    \"Exp2\": {\n",
    "        \"name\": \"alpha=0.1, 10% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 10,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp2\")\n",
    "    },\n",
    "    \"Exp3\": {\n",
    "        \"name\": \"alpha=0.1, 20% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 20,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp3\")\n",
    "    },\n",
    "    \"Exp4\": {\n",
    "        \"name\": \"alpha=0.1, 30% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 30,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp4\")\n",
    "    },\n",
    "    \"Exp5\": {\n",
    "        \"name\": \"alpha=0.1, 40% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 40,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp5\")\n",
    "    },\n",
    "    \"Exp6\": {\n",
    "        \"name\": \"alpha=0.1, 50% stragglers\",\n",
    "        \"data\": {\n",
    "            \"dataset\": \"cifar10\",\n",
    "            \"data_dir\": str(DATA_DIR),\n",
    "            \"num_classes\": 10\n",
    "        },\n",
    "        \"clients\": {\n",
    "            \"total\": 20,\n",
    "            \"concurrent\": 5,\n",
    "            \"local_epochs\": 1,\n",
    "            \"batch_size\": 128,\n",
    "            \"lr\": 0.005,\n",
    "            \"momentum\": 0.0,\n",
    "            \"weight_decay\": 0.001,\n",
    "            \"grad_clip\": 5.0,\n",
    "            \"struggle_percent\": 50,\n",
    "            \"delay_slow_range\": [0.8, 2.0],\n",
    "            \"delay_fast_range\": [0.0, 0.2],\n",
    "            \"jitter_per_round\": 0.05,\n",
    "            \"fix_delays_per_client\": True\n",
    "        },\n",
    "        \"trustweight\": {\n",
    "            \"buffer_size\": 5,\n",
    "            \"buffer_timeout_s\": 0.0,\n",
    "            \"use_sample_weighing\": True,\n",
    "            \"eta\": 0.5\n",
    "        },\n",
    "        \"eval\": {\n",
    "            \"interval_seconds\": 1.0,\n",
    "            \"target_accuracy\": 0.8\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"max_rounds\": 100\n",
    "        },\n",
    "        \"partition_alpha\": 0.1,\n",
    "        \"seed\": 1,\n",
    "        \"server_runtime\": {\n",
    "            \"client_delay\": 0.0\n",
    "        },\n",
    "        \"io\": get_paths(\"Exp6\")\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… All 6 experiment configurations loaded:\")\n",
    "for exp_id, exp_config in experiments.items():\n",
    "    print(f\"  {exp_id}: {exp_config['name']}\")\n",
    "    print(f\"    - Alpha: {exp_config['partition_alpha']}\")\n",
    "    print(f\"    - Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
    "    print(f\"    - Max rounds: {exp_config['train']['max_rounds']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Single Experiment (Helper Function)\n",
    "\n",
    "Use this function to run a single experiment by ID (Exp1-Exp6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_experiment(exp_id: str, experiments_dict: dict):\n",
    "    \"\"\"\n",
    "    Run a single TrustWeight experiment.\n",
    "    \n",
    "    Args:\n",
    "        exp_id: Experiment ID (e.g., \"Exp1\", \"Exp2\", ..., \"Exp6\")\n",
    "        experiments_dict: Dictionary containing all experiment configs\n",
    "    \"\"\"\n",
    "    if exp_id not in experiments_dict:\n",
    "        print(f\"âŒ Error: Experiment {exp_id} not found!\")\n",
    "        return None\n",
    "    \n",
    "    config = experiments_dict[exp_id]\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Running {exp_id}: {config['name']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Set random seed\n",
    "    seed = int(config.get(\"seed\", 42))\n",
    "    set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Create timestamped run folder\n",
    "    run_dir = Path(config[\"io\"][\"logs_dir\"]) / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Write COMMIT.txt\n",
    "    commit_hash = \"notebook_run\"\n",
    "    csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n",
    "    with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n",
    "        f.write(f\"{commit_hash},{csv_header}\\n\")\n",
    "    \n",
    "    # Save config to run folder\n",
    "    with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n",
    "        yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    print(f\"âœ… Run folder: {run_dir}\")\n",
    "    \n",
    "    # Load and partition data\n",
    "    dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n",
    "    dd.distribute_data(\n",
    "        num_clients=int(config[\"clients\"][\"total\"]),\n",
    "        alpha=float(config[\"partition_alpha\"]),\n",
    "        seed=seed\n",
    "    )\n",
    "    print(f\"âœ… Data partitioned: {config['clients']['total']} clients, alpha={config['partition_alpha']}\")\n",
    "    \n",
    "    # Build global model\n",
    "    global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n",
    "    \n",
    "    # Initialize server\n",
    "    server = AsyncServer(\n",
    "        global_model=global_model,\n",
    "        total_train_samples=len(dd.train_dataset),\n",
    "        buffer_size=int(config[\"trustweight\"][\"buffer_size\"]),\n",
    "        buffer_timeout_s=float(config[\"trustweight\"][\"buffer_timeout_s\"]),\n",
    "        use_sample_weighing=bool(config[\"trustweight\"][\"use_sample_weighing\"]),\n",
    "        target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n",
    "        max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n",
    "        eval_interval_s=int(config[\"eval\"][\"interval_seconds\"]),\n",
    "        data_dir=config[\"data\"][\"data_dir\"],\n",
    "        checkpoints_dir=str(run_dir / \"checkpoints\"),\n",
    "        logs_dir=str(run_dir),\n",
    "        global_log_csv=str(run_dir / \"TrustWeight.csv\"),\n",
    "        client_participation_csv=str(run_dir / \"TrustWeightClientParticipation.csv\"),\n",
    "        final_model_path=str(run_dir / \"TrustWeightModel.pt\"),\n",
    "        resume=False,\n",
    "        device=get_device(),\n",
    "        eta=float(config[\"trustweight\"].get(\"eta\", 0.5)),\n",
    "    )\n",
    "    print(f\"âœ… Server initialized (device: {server.device})\")\n",
    "    \n",
    "    # Setup clients\n",
    "    n = int(config[\"clients\"][\"total\"])\n",
    "    pct = max(0, min(100, int(config[\"clients\"].get(\"struggle_percent\", 0))))\n",
    "    k_slow = (n * pct) // 100\n",
    "    slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
    "    \n",
    "    a_s, b_s = config[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
    "    a_f, b_f = config[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
    "    fix_delays = bool(config[\"clients\"].get(\"fix_delays_per_client\", True))\n",
    "    jitter = float(config[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
    "    \n",
    "    per_client_base_delay: Dict[int, float] = {}\n",
    "    if fix_delays:\n",
    "        for cid in range(n):\n",
    "            if cid in slow_ids:\n",
    "                per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
    "            else:\n",
    "                per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
    "    \n",
    "    clients = []\n",
    "    for cid in range(n):\n",
    "        indices = dd.partitions[cid] if cid in dd.partitions else []\n",
    "        base_delay = per_client_base_delay.get(cid, 0.0)\n",
    "        is_slow = cid in slow_ids\n",
    "        clients.append(AsyncClient(\n",
    "            cid=cid,\n",
    "            indices=indices,\n",
    "            cfg=config,\n",
    "        ),\n",
    "            base_delay=base_delay,\n",
    "            slow=is_slow,\n",
    "            delay_ranges=((float(a_s), float(b_s)), (float(a_f), float(b_f))),\n",
    "            jitter=jitter,\n",
    "            fix_delay=fix_delays,\n",
    "        ))\n",
    "    \n",
    "    print(f\"âœ… Created {len(clients)} clients ({len(slow_ids)} slow, {n - len(slow_ids)} fast)\")\n",
    "    \n",
    "    # Start experiment\n",
    "    server.start_eval_timer()\n",
    "    sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n",
    "    \n",
    "    def client_loop(client: AsyncClient):\n",
    "        while True:\n",
    "            with sem:\n",
    "                cont = client.run_once(server)\n",
    "            if not cont:\n",
    "                break\n",
    "            time.sleep(0.05)\n",
    "    \n",
    "    threads = []\n",
    "    for cl in clients:\n",
    "        t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    \n",
    "    print(f\"âœ… Started {len(threads)} client threads\")\n",
    "    print(f\"ðŸš€ Experiment running... (max {config['train']['max_rounds']} rounds)\")\n",
    "    \n",
    "    # Wait for completion\n",
    "    server.wait()\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    print(f\"\\nâœ… {exp_id} completed!\")\n",
    "    print(f\"ðŸ“ Results: {run_dir}\")\n",
    "    \n",
    "    return run_dir\n",
    "\n",
    "print(\"âœ… Helper function `run_single_experiment()` defined\")\n",
    "print(\"   Usage: run_dir = run_single_experiment('Exp1', experiments)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run All 6 Experiments Sequentially\n",
    "\n",
    "This cell runs all experiments one by one. Each experiment saves to its own folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all 6 experiments sequentially\n",
    "experiment_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ALL 6 EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "    exp_start = time.time()\n",
    "    try:\n",
    "        run_dir = run_single_experiment(exp_id, experiments)\n",
    "        if run_dir:\n",
    "            experiment_results[exp_id] = {\n",
    "                \"run_dir\": run_dir,\n",
    "                \"status\": \"completed\",\n",
    "                \"duration_min\": (time.time() - exp_start) / 60.0\n",
    "            }\n",
    "            print(f\"âœ… {exp_id} completed in {experiment_results[exp_id]['duration_min']:.2f} minutes\")\n",
    "        else:\n",
    "            experiment_results[exp_id] = {\"status\": \"failed\", \"duration_min\": (time.time() - exp_start) / 60.0}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {exp_id} failed: {str(e)}\")\n",
    "        experiment_results[exp_id] = {\"status\": \"error\", \"error\": str(e), \"duration_min\": (time.time() - exp_start) / 60.0}\n",
    "    \n",
    "    print(f\"\\n{'â”€'*70}\\n\")\n",
    "\n",
    "total_duration = (time.time() - total_start) / 60.0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ALL EXPERIMENTS COMPLETED\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total time: {total_duration:.2f} minutes ({total_duration/60:.2f} hours)\")\n",
    "print(\"\\nResults summary:\")\n",
    "for exp_id, result in experiment_results.items():\n",
    "    if result.get(\"status\") == \"completed\":\n",
    "        print(f\"  {exp_id}: âœ… {result['duration_min']:.2f} min -> {result['run_dir']}\")\n",
    "    else:\n",
    "        print(f\"  {exp_id}: âŒ {result.get('status', 'unknown')}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare All Experiments (After Running)\n",
    "\n",
    "This section creates comparison plots across all 6 experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all experiments (load results from experiment_results)\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    # Load all CSV files\n",
    "    all_data = {}\n",
    "    for exp_id, result in experiment_results.items():\n",
    "        if result.get(\"status\") == \"completed\":\n",
    "            csv_path = result[\"run_dir\"] / \"TrustWeight.csv\"\n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['time_min'] = df['time'] / 60.0\n",
    "                all_data[exp_id] = df\n",
    "                print(f\"âœ… Loaded {exp_id}: {len(df)} rows\")\n",
    "    \n",
    "    if len(all_data) > 0:\n",
    "        # Create comparison plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "        \n",
    "        # Plot 1: Accuracy vs Rounds\n",
    "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
    "            axes[0, 0].plot(df['total_agg'], df['test_acc'], \n",
    "                          label=f\"{exp_id} ({experiments[exp_id]['name']})\", \n",
    "                          linewidth=2, color=colors[i % len(colors)])\n",
    "        axes[0, 0].set_xlabel('Round', fontsize=12)\n",
    "        axes[0, 0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "        axes[0, 0].set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].legend(fontsize=9, loc='lower right')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].set_ylim([0, 1.0])\n",
    "        \n",
    "        # Plot 2: Accuracy vs Wall Clock Time\n",
    "        for i, (exp_id, df) in enumerate(all_data.items()):\n",
    "            axes[0, 1].plot(df['time_min'], df['test_acc'], \n",
    "                          label=f\"{exp_id}\", \n",
    "                          linewidth=2, color=colors[i % len(colors)])\n",
    "        axes[0, 1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "        axes[0, 1].set_ylabel('Test Accuracy', fontsize=12)\n",
    "        axes[0, 1].set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].legend(fontsize=9, loc='lower right')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_ylim([0, 1.0])\n",
    "        \n",
    "        # Plot 3: Best Accuracy by Experiment\n",
    "        best_accs = {exp_id: df['test_acc'].max() for exp_id, df in all_data.items()}\n",
    "        exp_names = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\" \n",
    "                    if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\" \n",
    "                    for exp_id in best_accs.keys()]\n",
    "        axes[1, 0].bar(range(len(best_accs)), list(best_accs.values()), \n",
    "                      color=colors[:len(best_accs)], alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_xticks(range(len(best_accs)))\n",
    "        axes[1, 0].set_xticklabels(exp_names, fontsize=9, rotation=45, ha='right')\n",
    "        axes[1, 0].set_ylabel('Best Test Accuracy', fontsize=12)\n",
    "        axes[1, 0].set_title('Best Test Accuracy by Experiment', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "        axes[1, 0].set_ylim([0, 1.0])\n",
    "        # Add value labels\n",
    "        for i, (exp_id, acc) in enumerate(best_accs.items()):\n",
    "            axes[1, 0].text(i, acc, f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # Plot 4: Time to reach 50% accuracy\n",
    "        times_to_50 = {}\n",
    "        for exp_id, df in all_data.items():\n",
    "            mask = df['test_acc'] >= 0.5\n",
    "            if mask.any():\n",
    "                first_idx = mask.idxmax()\n",
    "                times_to_50[exp_id] = df.loc[first_idx, 'time_min']\n",
    "            else:\n",
    "                times_to_50[exp_id] = None\n",
    "        \n",
    "        valid_times = {k: v for k, v in times_to_50.items() if v is not None}\n",
    "        if len(valid_times) > 0:\n",
    "            exp_names_50 = [f\"{exp_id}\\n({experiments[exp_id]['clients']['struggle_percent']}% stragglers)\" \n",
    "                           if exp_id != 'Exp1' else f\"{exp_id}\\n(IID)\" \n",
    "                           for exp_id in valid_times.keys()]\n",
    "            axes[1, 1].bar(range(len(valid_times)), list(valid_times.values()), \n",
    "                         color=colors[:len(valid_times)], alpha=0.7, edgecolor='black')\n",
    "            axes[1, 1].set_xticks(range(len(valid_times)))\n",
    "            axes[1, 1].set_xticklabels(exp_names_50, fontsize=9, rotation=45, ha='right')\n",
    "            axes[1, 1].set_ylabel('Time to Reach 50% Accuracy (minutes)', fontsize=12)\n",
    "            axes[1, 1].set_title('Convergence Speed: Time to 50% Accuracy', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "            # Add value labels\n",
    "            for i, (exp_id, t) in enumerate(valid_times.items()):\n",
    "                axes[1, 1].text(i, t, f'{t:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        # Use Google Drive path if in Colab, otherwise local\n",
    "        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"TrustWeight\" / \"comparisons\"\n",
    "        else:\n",
    "            comparison_dir = Path(\"./logs/TrustWeight/comparisons\")\n",
    "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(comparison_dir / \"all_experiments_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "        print(f\"\\nâœ… Comparison plot saved: {comparison_dir / 'all_experiments_comparison.png'}\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary table\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"EXPERIMENT COMPARISON SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_data:\n",
    "                df = all_data[exp_id]\n",
    "                cfg = experiments[exp_id]\n",
    "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
    "                      f\"{df['test_acc'].max():<10.4f} {df['test_acc'].iloc[-1]:<11.4f} \"\n",
    "                      f\"{df['time_min'].iloc[-1]:<12.2f}\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(\"âš ï¸  No completed experiments found. Run experiments first.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No experiment results found. Run the experiments first using Section 8.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download CIFAR-10 dataset\n",
    "# IMPORTANT: Make sure you've run Section 7 first to set the 'config' variable!\n",
    "if 'config' not in globals():\n",
    "    raise NameError(\n",
    "        \"âŒ 'config' is not defined!\\n\"\n",
    "        \"Please run Section 7 first to select an experiment (Exp1-Exp6).\\n\"\n",
    "        \"Or use Section 8 (automated runner) which handles everything automatically.\"\n",
    "    )\n",
    "\n",
    "print(\"Downloading CIFAR-10 dataset...\")\n",
    "dd = DataDistributor(dataset_name=config[\"data\"][\"dataset\"], data_dir=config[\"data\"][\"data_dir\"])\n",
    "print(f\"âœ… Dataset loaded: {len(dd.train_dataset)} training samples, {len(dd.test_dataset)} test samples\")\n",
    "print(f\"âœ… Number of classes: {dd.num_classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which experiment to run manually (Exp1, Exp2, Exp3, Exp4, Exp5, or Exp6)\n",
    "# This sets the 'config' variable for the manual cells below\n",
    "EXP_ID = \"Exp1\"  # Change this to Exp2, Exp3, etc. to run different experiments\n",
    "\n",
    "if EXP_ID in experiments:\n",
    "    config = experiments[EXP_ID]\n",
    "    print(f\"âœ… Selected experiment: {EXP_ID} - {config['name']}\")\n",
    "    print(f\"   Alpha: {config['partition_alpha']}\")\n",
    "    print(f\"   Stragglers: {config['clients']['struggle_percent']}%\")\n",
    "    print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
    "else:\n",
    "    raise ValueError(f\"Experiment {EXP_ID} not found! Use Exp1-Exp6.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which experiment to run manually (Exp1, Exp2, Exp3, Exp4, Exp5, or Exp6)\n",
    "# This sets the 'config' variable for the manual cells below\n",
    "EXP_ID = \"Exp1\"  # Change this to Exp2, Exp3, etc. to run different experiments\n",
    "\n",
    "if EXP_ID in experiments:\n",
    "    config = experiments[EXP_ID]\n",
    "    print(f\"âœ… Selected experiment: {EXP_ID} - {config['name']}\")\n",
    "    print(f\"   Alpha: {config['partition_alpha']}\")\n",
    "    print(f\"   Stragglers: {config['clients']['struggle_percent']}%\")\n",
    "    print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
    "else:\n",
    "    raise ValueError(f\"Experiment {EXP_ID} not found! Use Exp1-Exp6.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Partition Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition data among clients\n",
    "print(f\"Partitioning data with alpha={config['partition_alpha']}...\")\n",
    "dd.distribute_data(\n",
    "    num_clients=int(config[\"clients\"][\"total\"]),\n",
    "    alpha=float(config[\"partition_alpha\"]),\n",
    "    seed=int(config[\"seed\"])\n",
    ")\n",
    "print(f\"âœ… Data partitioned among {config['clients']['total']} clients\")\n",
    "for i in range(min(3, config['clients']['total'])):\n",
    "    print(f\"   Client {i}: {len(dd.get_client_data(i))} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Setup Folders and Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamped run folder\n",
    "run_dir = Path(\"logs\") / \"TrustWeight\" / f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write COMMIT.txt (simulated git hash for notebook)\n",
    "commit_hash = \"notebook_run\"\n",
    "csv_header = \"total_agg,avg_train_loss,avg_train_acc,test_loss,test_acc,time\"\n",
    "with (run_dir / \"COMMIT.txt\").open(\"w\") as f:\n",
    "    f.write(f\"{commit_hash},{csv_header}\\n\")\n",
    "\n",
    "# Save config to run folder\n",
    "with (run_dir / \"CONFIG.yaml\").open(\"w\") as f:\n",
    "    yaml.dump(config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "print(f\"âœ… Run folder created: {run_dir}\")\n",
    "print(f\"âœ… COMMIT.txt and CONFIG.yaml saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Initialize Server and Clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "seed = int(config.get(\"seed\", 42))\n",
    "set_seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Build global model\n",
    "global_model = build_resnet18(num_classes=config[\"data\"][\"num_classes\"], pretrained=False)\n",
    "print(f\"âœ… Global model created: ResNet-18\")\n",
    "\n",
    "# Initialize server\n",
    "server = AsyncServer(\n",
    "    global_model=global_model,\n",
    "    total_train_samples=len(dd.train_dataset),\n",
    "    buffer_size=int(config[\"trustweight\"][\"buffer_size\"]),\n",
    "    buffer_timeout_s=float(config[\"trustweight\"][\"buffer_timeout_s\"]),\n",
    "    use_sample_weighing=bool(config[\"trustweight\"][\"use_sample_weighing\"]),\n",
    "    target_accuracy=float(config[\"eval\"][\"target_accuracy\"]),\n",
    "    max_rounds=int(config[\"train\"][\"max_rounds\"]) if \"max_rounds\" in config[\"train\"] else None,\n",
    "    eval_interval_s=int(config[\"eval\"][\"interval_seconds\"]),\n",
    "    data_dir=config[\"data\"][\"data_dir\"],\n",
    "    checkpoints_dir=str(run_dir / \"checkpoints\"),\n",
    "    logs_dir=str(run_dir),\n",
    "    global_log_csv=str(run_dir / \"TrustWeight.csv\"),\n",
    "    client_participation_csv=str(run_dir / \"TrustWeightClientParticipation.csv\"),\n",
    "    final_model_path=str(run_dir / \"TrustWeightModel.pt\"),\n",
    "    resume=False,  # Start fresh in notebook\n",
    "    device=get_device(),\n",
    "    eta=float(config[\"trustweight\"].get(\"eta\", 0.5)),\n",
    ")\n",
    "print(f\"âœ… Server initialized\")\n",
    "print(f\"   Device: {server.device}\")\n",
    "print(f\"   Buffer size: {server.buffer_size}\")\n",
    "print(f\"   Eta: {server.eta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup client delays (for heterogeneity simulation)\n",
    "n = int(config[\"clients\"][\"total\"])\n",
    "pct = max(0, min(100, int(config[\"clients\"].get(\"struggle_percent\", 0))))\n",
    "k_slow = (n * pct) // 100\n",
    "slow_ids = set(random.sample(range(n), k_slow)) if k_slow > 0 else set()\n",
    "\n",
    "a_s, b_s = config[\"clients\"].get(\"delay_slow_range\", [0.8, 2.0])\n",
    "a_f, b_f = config[\"clients\"].get(\"delay_fast_range\", [0.0, 0.2])\n",
    "fix_delays = bool(config[\"clients\"].get(\"fix_delays_per_client\", True))\n",
    "jitter = float(config[\"clients\"].get(\"jitter_per_round\", 0.0))\n",
    "\n",
    "per_client_base_delay: Dict[int, float] = {}\n",
    "if fix_delays:\n",
    "    for cid in range(n):\n",
    "        if cid in slow_ids:\n",
    "            per_client_base_delay[cid] = random.uniform(float(a_s), float(b_s))\n",
    "        else:\n",
    "            per_client_base_delay[cid] = random.uniform(float(a_f), float(b_f))\n",
    "\n",
    "# Create clients\n",
    "clients = []\n",
    "for cid in range(n):\n",
    "    indices = dd.partitions[cid] if cid in dd.partitions else []\n",
    "    base_delay = per_client_base_delay.get(cid, 0.0)\n",
    "    is_slow = cid in slow_ids\n",
    "    clients.append(AsyncClient(\n",
    "            cid=cid,\n",
    "            indices=indices,\n",
    "            cfg=config,\n",
    "        ),\n",
    "        base_delay=base_delay,\n",
    "        slow=is_slow,\n",
    "        delay_ranges=((float(a_s), float(b_s)), (float(a_f), float(b_f))),\n",
    "        jitter=jitter,\n",
    "        fix_delay=fix_delays,\n",
    "    ))\n",
    "\n",
    "print(f\"âœ… Created {len(clients)} clients\")\n",
    "print(f\"   Slow clients: {len(slow_ids)}\")\n",
    "print(f\"   Fast clients: {n - len(slow_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results for all 6 experiments\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    print(\"=\"*80)\n",
    "    print(\"FEDBUFF RESULTS - ALL 6 EXPERIMENTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n",
    "            run_dir = experiment_results[exp_id][\"run_dir\"]\n",
    "            csv_path = run_dir / \"TrustWeight.csv\"\n",
    "            \n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                all_results[exp_id] = df\n",
    "                \n",
    "                exp_config = experiments[exp_id]\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"{exp_id}: {exp_config['name']}\")\n",
    "                print(f\"{'='*80}\")\n",
    "                print(f\"  Alpha: {exp_config['partition_alpha']}\")\n",
    "                print(f\"  Stragglers: {exp_config['clients']['struggle_percent']}%\")\n",
    "                print(f\"  Max rounds: {exp_config['train']['max_rounds']}\")\n",
    "                print(f\"\\n  Total rounds completed: {df['total_agg'].max()}\")\n",
    "                \n",
    "                final_row = df.iloc[-1]\n",
    "                print(f\"\\n  Final metrics:\")\n",
    "                print(f\"    - Test accuracy: {final_row['test_acc']:.4f}\")\n",
    "                print(f\"    - Train accuracy: {final_row['avg_train_acc']:.4f}\")\n",
    "                print(f\"    - Test loss: {final_row['test_loss']:.4f}\")\n",
    "                print(f\"    - Total time: {final_row['time']:.1f} seconds ({final_row['time']/60:.2f} minutes)\")\n",
    "                \n",
    "                best_acc = df['test_acc'].max()\n",
    "                best_round = df.loc[df['test_acc'].idxmax(), 'total_agg']\n",
    "                print(f\"\\n  Best test accuracy: {best_acc:.4f} (round {best_round})\")\n",
    "                \n",
    "                print(f\"\\n  Run folder: {run_dir}\")\n",
    "            else:\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"{exp_id}: Results file not found\")\n",
    "                print(f\"  Run folder: {run_dir}\")\n",
    "        else:\n",
    "            status = experiment_results.get(exp_id, {}).get(\"status\", \"not run\")\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"{exp_id}: {status.upper()}\")\n",
    "            if status == \"error\":\n",
    "                print(f\"  Error: {experiment_results[exp_id].get('error', 'Unknown error')}\")\n",
    "    \n",
    "    # Summary table\n",
    "    if len(all_results) > 0:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"SUMMARY TABLE\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Exp':<6} {'Alpha':<8} {'Stragglers':<12} {'Rounds':<8} {'Best Acc':<10} {'Final Acc':<11} {'Time (min)':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_results:\n",
    "                df = all_results[exp_id]\n",
    "                cfg = experiments[exp_id]\n",
    "                final_row = df.iloc[-1]\n",
    "                print(f\"{exp_id:<6} {cfg['partition_alpha']:<8.1f} {cfg['clients']['struggle_percent']:<12} \"\n",
    "                      f\"{df['total_agg'].max():<8} {df['test_acc'].max():<10.4f} {final_row['test_acc']:<11.4f} \"\n",
    "                      f\"{final_row['time']/60:<12.2f}\")\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show first and last rows for each experiment\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DETAILED DATA PREVIEW\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "            if exp_id in all_results:\n",
    "                df = all_results[exp_id]\n",
    "                exp_config = experiments[exp_id]\n",
    "                print(f\"\\n{exp_id}: {exp_config['name']}\")\n",
    "                print(f\"  First 3 rows:\")\n",
    "                print(df.head(3).to_string(index=False))\n",
    "                print(f\"\\n  Last 3 rows:\")\n",
    "                print(df.tail(3).to_string(index=False))\n",
    "                print()\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  No completed experiments found. Run experiments first using Section 8.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No experiment results found. Run experiments first using Section 8.\")\n",
    "    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Run TrustWeight Experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization suite for all 6 experiments\n",
    "if 'experiment_results' in globals() and len(experiment_results) > 0:\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']\n",
    "    \n",
    "    all_data = {}\n",
    "    all_participation = {}\n",
    "    \n",
    "    # Load all experiment data\n",
    "    for exp_id in [\"Exp1\", \"Exp2\", \"Exp3\", \"Exp4\", \"Exp5\", \"Exp6\"]:\n",
    "        if exp_id in experiment_results and experiment_results[exp_id].get(\"status\") == \"completed\":\n",
    "            run_dir = experiment_results[exp_id][\"run_dir\"]\n",
    "            csv_path = run_dir / \"TrustWeight.csv\"\n",
    "            participation_path = run_dir / \"TrustWeightClientParticipation.csv\"\n",
    "            \n",
    "            if csv_path.exists():\n",
    "                df = pd.read_csv(csv_path)\n",
    "                df['time_min'] = df['time'] / 60.0\n",
    "                all_data[exp_id] = {\n",
    "                    'df': df,\n",
    "                    'run_dir': run_dir,\n",
    "                    'config': experiments[exp_id]\n",
    "                }\n",
    "                \n",
    "                if participation_path.exists():\n",
    "                    part_df = pd.read_csv(participation_path)\n",
    "                    all_participation[exp_id] = part_df\n",
    "    \n",
    "    if len(all_data) > 0:\n",
    "        print(f\"âœ… Loaded data for {len(all_data)} experiments\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Generate individual plots for each experiment\n",
    "        for exp_id, exp_data in all_data.items():\n",
    "            df = exp_data['df']\n",
    "            run_dir = exp_data['run_dir']\n",
    "            exp_config = exp_data['config']\n",
    "            participation_path = run_dir / \"TrustWeightClientParticipation.csv\"\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Generating plots for {exp_id}: {exp_config['name']}\")\n",
    "            \n",
    "            # Plot 1: Accuracy vs Rounds\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "            train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "            train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n",
    "            if len(train_acc) > 0:\n",
    "                axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "            axes[0].set_xlabel('Round', fontsize=12)\n",
    "            axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "            axes[0].set_title(f'{exp_id}: Accuracy vs Rounds', fontsize=14, fontweight='bold')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].legend(fontsize=10)\n",
    "            axes[0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "            train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "            train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n",
    "            if len(train_loss) > 0:\n",
    "                axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "            axes[1].set_xlabel('Round', fontsize=12)\n",
    "            axes[1].set_ylabel('Loss', fontsize=12)\n",
    "            axes[1].set_title(f'{exp_id}: Loss vs Rounds', fontsize=14, fontweight='bold')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            axes[1].legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 2: Accuracy vs Wall Clock Time\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "            train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "            train_time = df[df['avg_train_acc'] > 0]['time_min']\n",
    "            if len(train_acc_time) > 0:\n",
    "                axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "            axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "            axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "            axes[0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            axes[0].legend(fontsize=10)\n",
    "            axes[0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "            train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "            train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n",
    "            if len(train_loss_time) > 0:\n",
    "                axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "            axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "            axes[1].set_ylabel('Loss', fontsize=12)\n",
    "            axes[1].set_title(f'{exp_id}: Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            axes[1].legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 3: Convergence Speed Analysis\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "            \n",
    "            thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "            times_to_threshold = []\n",
    "            rounds_to_threshold = []\n",
    "            reached_thresholds = []\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                mask = df['test_acc'] >= thresh\n",
    "                if mask.any():\n",
    "                    first_idx = mask.idxmax()\n",
    "                    times_to_threshold.append(df.loc[first_idx, 'time_min'])\n",
    "                    rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n",
    "                    reached_thresholds.append(thresh)\n",
    "                else:\n",
    "                    break\n",
    "            \n",
    "            if len(reached_thresholds) > 0:\n",
    "                axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "                axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "                axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n",
    "                axes[0].set_title(f'{exp_id}: Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "                axes[0].set_xticks(range(len(reached_thresholds)))\n",
    "                axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "                axes[0].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n",
    "                    axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "                \n",
    "                axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "                axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "                axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n",
    "                axes[1].set_title(f'{exp_id}: Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "                axes[1].set_xticks(range(len(reached_thresholds)))\n",
    "                axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "                axes[1].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n",
    "                    axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 4: Training Efficiency\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "            \n",
    "            if len(df) > 1:\n",
    "                df_sorted = df.sort_values('time_min')\n",
    "                efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n",
    "                efficiency = efficiency.fillna(0)\n",
    "                \n",
    "                ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n",
    "                ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n",
    "                ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "                ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n",
    "                ax.set_title(f'{exp_id}: Training Efficiency', fontsize=14, fontweight='bold')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                max_eff_idx = efficiency.idxmax()\n",
    "                max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n",
    "                max_eff_val = efficiency.loc[max_eff_idx]\n",
    "                ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n",
    "                ax.legend(fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot 5: Client Participation Analysis\n",
    "            if participation_path.exists():\n",
    "                part_df = pd.read_csv(participation_path)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "                \n",
    "                client_counts = part_df['client_id'].value_counts().sort_index()\n",
    "                axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n",
    "                axes[0, 0].set_xlabel('Client ID', fontsize=11)\n",
    "                axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n",
    "                axes[0, 0].set_title(f'{exp_id}: Client Participation Frequency', fontsize=12, fontweight='bold')\n",
    "                axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "                \n",
    "                participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n",
    "                axes[0, 1].plot(participation_by_round.index, participation_by_round.values, \n",
    "                               marker='o', markersize=4, linewidth=2, color='coral')\n",
    "                axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "                axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n",
    "                axes[0, 1].set_title(f'{exp_id}: Client Participation per Round', fontsize=12, fontweight='bold')\n",
    "                axes[0, 1].grid(True, alpha=0.3)\n",
    "                \n",
    "                client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n",
    "                axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values, \n",
    "                                marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n",
    "                axes[1, 0].set_xlabel('Round', fontsize=11)\n",
    "                axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n",
    "                axes[1, 0].set_title(f'{exp_id}: Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n",
    "                axes[1, 0].grid(True, alpha=0.3)\n",
    "                axes[1, 0].set_ylim([0, 1.0])\n",
    "                \n",
    "                if len(part_df) > 0:\n",
    "                    client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values \n",
    "                                  for cid in sorted(part_df['client_id'].unique())[:10]]\n",
    "                    if len(client_accs) > 0:\n",
    "                        axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n",
    "                        axes[1, 1].set_xlabel('Client ID', fontsize=11)\n",
    "                        axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
    "                        axes[1, 1].set_title(f'{exp_id}: Client Accuracy Distribution', fontsize=12, fontweight='bold')\n",
    "                        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "                        axes[1, 1].set_ylim([0, 1.0])\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(run_dir / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            # Plot 6: Training Progress Summary\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            \n",
    "            axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "            train_acc_plot = df[df['avg_train_acc'] > 0]\n",
    "            if len(train_acc_plot) > 0:\n",
    "                axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'], \n",
    "                               'r--', linewidth=2, label='Train')\n",
    "            axes[0, 0].set_xlabel('Round', fontsize=11)\n",
    "            axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "            axes[0, 0].set_title(f'{exp_id}: Accuracy Trajectory', fontsize=12, fontweight='bold')\n",
    "            axes[0, 0].legend(fontsize=10)\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n",
    "            train_loss_plot = df[df['avg_train_loss'] > 0]\n",
    "            if len(train_loss_plot) > 0:\n",
    "                axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'], \n",
    "                               'r--', linewidth=2, label='Train')\n",
    "            axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "            axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
    "            axes[0, 1].set_title(f'{exp_id}: Loss Trajectory', fontsize=12, fontweight='bold')\n",
    "            axes[0, 1].legend(fontsize=10)\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "            if len(train_acc_plot) > 0:\n",
    "                train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n",
    "                axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'], \n",
    "                                'r--', linewidth=2, label='Train')\n",
    "            axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n",
    "            axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "            axes[1, 0].set_title(f'{exp_id}: Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n",
    "            axes[1, 0].legend(fontsize=10)\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].set_ylim([0, 1.0])\n",
    "            \n",
    "            axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n",
    "            axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n",
    "            axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n",
    "            axes[1, 1].set_title(f'{exp_id}: Round Progression Over Time', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "            \n",
    "            if len(df) > 1:\n",
    "                total_time = df['time_min'].iloc[-1]\n",
    "                total_rounds = df['total_agg'].iloc[-1]\n",
    "                rate = total_rounds / total_time if total_time > 0 else 0\n",
    "                axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min', \n",
    "                               transform=axes[1, 1].transAxes, fontsize=10,\n",
    "                               verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(run_dir / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            print(f\"  âœ… All plots saved to {run_dir}\")\n",
    "        \n",
    "        # Create comparison plots across all experiments\n",
    "        print(f\"\\nðŸ“Š Creating comparison plots across all experiments...\")\n",
    "        \n",
    "        # Comparison Plot 1: Test Accuracy vs Rounds (all experiments)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n",
    "            df = exp_data['df']\n",
    "            exp_config = exp_data['config']\n",
    "            label = f\"{exp_id} (Î±={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n",
    "            ax.plot(df['total_agg'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n",
    "        ax.set_xlabel('Round', fontsize=12)\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "        ax.set_title('Test Accuracy vs Rounds (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([0, 1.0])\n",
    "        \n",
    "        # Save comparison plot\n",
    "        if 'IN_COLAB' in globals() and IN_COLAB and OUTPUT_DIR:\n",
    "            comparison_dir = BASE_OUTPUT_DIR / \"logs\" / \"TrustWeight\" / \"comparisons\"\n",
    "        else:\n",
    "            comparison_dir = Path(\"./logs/TrustWeight/comparisons\")\n",
    "        comparison_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(comparison_dir / \"comparison_accuracy_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  âœ… Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_rounds.png'}\")\n",
    "        \n",
    "        # Comparison Plot 2: Test Accuracy vs Time (all experiments)\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "        for i, (exp_id, exp_data) in enumerate(all_data.items()):\n",
    "            df = exp_data['df']\n",
    "            exp_config = exp_data['config']\n",
    "            label = f\"{exp_id} (Î±={exp_config['partition_alpha']}, {exp_config['clients']['struggle_percent']}% stragglers)\"\n",
    "            ax.plot(df['time_min'], df['test_acc'], label=label, linewidth=2, color=colors[i % len(colors)])\n",
    "        ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "        ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "        ax.set_title('Test Accuracy vs Wall Clock Time (All Experiments)', fontsize=14, fontweight='bold')\n",
    "        ax.legend(fontsize=9, loc='lower right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim([0, 1.0])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(comparison_dir / \"comparison_accuracy_vs_time.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"  âœ… Comparison plot saved: {comparison_dir / 'comparison_accuracy_vs_time.png'}\")\n",
    "        \n",
    "        print(f\"\\nâœ… All visualizations completed!\")\n",
    "        print(f\"   Individual plots saved to each experiment's run folder\")\n",
    "        print(f\"   Comparison plots saved to: {comparison_dir}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸  No completed experiments found. Run experiments first using Section 8.\")\n",
    "else:\n",
    "    print(\"âš ï¸  No experiment results found. Run experiments first using Section 8.\")\n",
    "    print(\"   The 'experiment_results' dictionary will be created after running Section 8.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start evaluation timer\n",
    "server.start_eval_timer()\n",
    "print(\"âœ… Evaluation timer started\")\n",
    "\n",
    "# Concurrency gate\n",
    "sem = threading.Semaphore(int(config[\"clients\"][\"concurrent\"]))\n",
    "\n",
    "def client_loop(client: AsyncClient):\n",
    "    \"\"\"Client training loop.\"\"\"\n",
    "    while True:\n",
    "        with sem:\n",
    "            cont = client.run_once(server)\n",
    "        if not cont:\n",
    "            break\n",
    "        time.sleep(0.05)\n",
    "\n",
    "# Launch client threads\n",
    "threads = []\n",
    "for cl in clients:\n",
    "    t = threading.Thread(target=client_loop, args=(cl,), daemon=False)\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "print(f\"âœ… Started {len(threads)} client threads\")\n",
    "print(f\"\\nðŸš€ TrustWeight experiment running...\")\n",
    "print(f\"   Max rounds: {config['train']['max_rounds']}\")\n",
    "print(f\"   Target accuracy: {config['eval']['target_accuracy']}\")\n",
    "print(f\"\\nResults will be saved to: {run_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for completion\n",
    "server.wait()\n",
    "for t in threads:\n",
    "    t.join()\n",
    "\n",
    "print(\"\\nâœ… Experiment completed!\")\n",
    "print(f\"\\nðŸ“ Results saved to: {run_dir}\")\n",
    "print(f\"   - TrustWeight.csv: Global metrics\")\n",
    "print(f\"   - TrustWeightClientParticipation.csv: Client participation\")\n",
    "print(f\"   - TrustWeightModel.pt: Final model\")\n",
    "print(f\"   - CONFIG.yaml: Configuration used\")\n",
    "print(f\"   - COMMIT.txt: Run metadata\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. View Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "csv_path = run_dir / \"TrustWeight.csv\"\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"=== TrustWeight Results ===\")\n",
    "    print(f\"\\nTotal rounds: {df['total_agg'].max()}\")\n",
    "    print(f\"\\nFinal metrics:\")\n",
    "    final_row = df.iloc[-1]\n",
    "    print(f\"  - Test accuracy: {final_row['test_acc']:.4f}\")\n",
    "    print(f\"  - Train accuracy: {final_row['avg_train_acc']:.4f}\")\n",
    "    print(f\"  - Test loss: {final_row['test_loss']:.4f}\")\n",
    "    print(f\"  - Total time: {final_row['time']:.1f} seconds\")\n",
    "    \n",
    "    print(f\"\\nBest test accuracy: {df['test_acc'].max():.4f} (round {df.loc[df['test_acc'].idxmax(), 'total_agg']})\")\n",
    "    \n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\nLast 5 rows:\")\n",
    "    print(df.tail())\n",
    "else:\n",
    "    print(\"Results file not found yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Comprehensive Results Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization suite\n",
    "csv_path = run_dir / \"TrustWeight.csv\"\n",
    "participation_path = run_dir / \"TrustWeightClientParticipation.csv\"\n",
    "\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert time to minutes for better readability\n",
    "    df['time_min'] = df['time'] / 60.0\n",
    "    \n",
    "    # ========== Plot 1: Accuracy vs Rounds ==========\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Test accuracy over rounds\n",
    "    axes[0].plot(df['total_agg'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "    train_acc = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "    train_rounds = df[df['avg_train_acc'] > 0]['total_agg']\n",
    "    if len(train_acc) > 0:\n",
    "        axes[0].plot(train_rounds, train_acc, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "    axes[0].set_xlabel('Round', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Accuracy vs Rounds', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # Loss over rounds\n",
    "    axes[1].plot(df['total_agg'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "    train_loss = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "    train_rounds_loss = df[df['avg_train_loss'] > 0]['total_agg']\n",
    "    if len(train_loss) > 0:\n",
    "        axes[1].plot(train_rounds_loss, train_loss, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "    axes[1].set_xlabel('Round', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Loss vs Rounds', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"1_accuracy_loss_vs_rounds.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… Plot 1 saved: 1_accuracy_loss_vs_rounds.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Plot 2: Accuracy vs Wall Clock Time ==========\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Test accuracy over time\n",
    "    axes[0].plot(df['time_min'], df['test_acc'], label='Test Accuracy', marker='o', markersize=4, linewidth=2)\n",
    "    train_acc_time = df[df['avg_train_acc'] > 0]['avg_train_acc']\n",
    "    train_time = df[df['avg_train_acc'] > 0]['time_min']\n",
    "    if len(train_acc_time) > 0:\n",
    "        axes[0].plot(train_time, train_acc_time, label='Train Accuracy', marker='s', markersize=4, linewidth=2, color='orange')\n",
    "    axes[0].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Accuracy vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend(fontsize=10)\n",
    "    axes[0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # Loss over time\n",
    "    axes[1].plot(df['time_min'], df['test_loss'], label='Test Loss', marker='o', markersize=4, linewidth=2, color='red')\n",
    "    train_loss_time = df[df['avg_train_loss'] > 0]['avg_train_loss']\n",
    "    train_time_loss = df[df['avg_train_loss'] > 0]['time_min']\n",
    "    if len(train_loss_time) > 0:\n",
    "        axes[1].plot(train_time_loss, train_loss_time, label='Train Loss', marker='s', markersize=4, linewidth=2, color='purple')\n",
    "    axes[1].set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Loss vs Wall Clock Time', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"2_accuracy_loss_vs_time.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… Plot 2 saved: 2_accuracy_loss_vs_time.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Plot 3: Convergence Speed Analysis ==========\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Time to reach accuracy thresholds\n",
    "    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "    times_to_threshold = []\n",
    "    rounds_to_threshold = []\n",
    "    reached_thresholds = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        mask = df['test_acc'] >= thresh\n",
    "        if mask.any():\n",
    "            first_idx = mask.idxmax()\n",
    "            times_to_threshold.append(df.loc[first_idx, 'time_min'])\n",
    "            rounds_to_threshold.append(df.loc[first_idx, 'total_agg'])\n",
    "            reached_thresholds.append(thresh)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if len(reached_thresholds) > 0:\n",
    "        axes[0].bar(range(len(reached_thresholds)), times_to_threshold, color='skyblue', alpha=0.7, edgecolor='navy')\n",
    "        axes[0].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "        axes[0].set_ylabel('Time to Reach (minutes)', fontsize=12)\n",
    "        axes[0].set_title('Time to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xticks(range(len(reached_thresholds)))\n",
    "        axes[0].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "        axes[0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (t, v) in enumerate(zip(reached_thresholds, times_to_threshold)):\n",
    "            axes[0].text(i, v, f'{v:.1f}m', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        axes[1].bar(range(len(reached_thresholds)), rounds_to_threshold, color='lightcoral', alpha=0.7, edgecolor='darkred')\n",
    "        axes[1].set_xlabel('Accuracy Threshold', fontsize=12)\n",
    "        axes[1].set_ylabel('Rounds to Reach', fontsize=12)\n",
    "        axes[1].set_title('Rounds to Reach Accuracy Thresholds', fontsize=14, fontweight='bold')\n",
    "        axes[1].set_xticks(range(len(reached_thresholds)))\n",
    "        axes[1].set_xticklabels([f'{t:.1f}' for t in reached_thresholds], fontsize=10)\n",
    "        axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (t, v) in enumerate(zip(reached_thresholds, rounds_to_threshold)):\n",
    "            axes[1].text(i, v, f'{int(v)}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"3_convergence_speed.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… Plot 3 saved: 3_convergence_speed.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Plot 4: Training Efficiency (Accuracy per Time) ==========\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Calculate efficiency: accuracy gain per minute\n",
    "    if len(df) > 1:\n",
    "        df_sorted = df.sort_values('time_min')\n",
    "        efficiency = df_sorted['test_acc'].diff() / df_sorted['time_min'].diff()\n",
    "        efficiency = efficiency.fillna(0)\n",
    "        \n",
    "        ax.plot(df_sorted['time_min'], efficiency, marker='o', markersize=3, linewidth=2, color='green')\n",
    "        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.3)\n",
    "        ax.set_xlabel('Wall Clock Time (minutes)', fontsize=12)\n",
    "        ax.set_ylabel('Accuracy Gain per Minute', fontsize=12)\n",
    "        ax.set_title('Training Efficiency: Accuracy Gain Rate Over Time', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight peak efficiency\n",
    "        max_eff_idx = efficiency.idxmax()\n",
    "        max_eff_time = df_sorted.loc[max_eff_idx, 'time_min']\n",
    "        max_eff_val = efficiency.loc[max_eff_idx]\n",
    "        ax.plot(max_eff_time, max_eff_val, 'r*', markersize=15, label=f'Peak: {max_eff_val:.4f}/min')\n",
    "        ax.legend(fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"4_training_efficiency.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… Plot 4 saved: 4_training_efficiency.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Plot 5: Client Participation Analysis ==========\n",
    "    if participation_path.exists():\n",
    "        part_df = pd.read_csv(participation_path)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Client participation frequency\n",
    "        client_counts = part_df['client_id'].value_counts().sort_index()\n",
    "        axes[0, 0].bar(client_counts.index, client_counts.values, color='steelblue', alpha=0.7, edgecolor='navy')\n",
    "        axes[0, 0].set_xlabel('Client ID', fontsize=11)\n",
    "        axes[0, 0].set_ylabel('Number of Updates', fontsize=11)\n",
    "        axes[0, 0].set_title('Client Participation Frequency', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Client participation over rounds\n",
    "        participation_by_round = part_df.groupby('total_agg')['client_id'].count()\n",
    "        axes[0, 1].plot(participation_by_round.index, participation_by_round.values, \n",
    "                       marker='o', markersize=4, linewidth=2, color='coral')\n",
    "        axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "        axes[0, 1].set_ylabel('Number of Clients', fontsize=11)\n",
    "        axes[0, 1].set_title('Client Participation per Round', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Average client accuracy over time\n",
    "        client_acc_by_round = part_df.groupby('total_agg')['local_test_acc'].mean()\n",
    "        axes[1, 0].plot(client_acc_by_round.index, client_acc_by_round.values, \n",
    "                        marker='s', markersize=4, linewidth=2, color='mediumseagreen')\n",
    "        axes[1, 0].set_xlabel('Round', fontsize=11)\n",
    "        axes[1, 0].set_ylabel('Average Client Test Accuracy', fontsize=11)\n",
    "        axes[1, 0].set_title('Average Client Accuracy Over Rounds', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        axes[1, 0].set_ylim([0, 1.0])\n",
    "        \n",
    "        # Client accuracy distribution (box plot)\n",
    "        if len(part_df) > 0:\n",
    "            client_accs = [part_df[part_df['client_id'] == cid]['local_test_acc'].values \n",
    "                          for cid in sorted(part_df['client_id'].unique())[:10]]  # Limit to first 10 clients for readability\n",
    "            if len(client_accs) > 0:\n",
    "                axes[1, 1].boxplot(client_accs, labels=[f'C{i}' for i in range(len(client_accs))])\n",
    "                axes[1, 1].set_xlabel('Client ID', fontsize=11)\n",
    "                axes[1, 1].set_ylabel('Test Accuracy', fontsize=11)\n",
    "                axes[1, 1].set_title('Client Accuracy Distribution', fontsize=12, fontweight='bold')\n",
    "                axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "                axes[1, 1].set_ylim([0, 1.0])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(run_dir / \"5_client_participation.png\", dpi=150, bbox_inches='tight')\n",
    "        print(f\"âœ… Plot 5 saved: 5_client_participation.png\")\n",
    "        plt.show()\n",
    "    \n",
    "    # ========== Plot 6: Training Progress Summary ==========\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Accuracy trajectory\n",
    "    axes[0, 0].plot(df['total_agg'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "    train_acc_plot = df[df['avg_train_acc'] > 0]\n",
    "    if len(train_acc_plot) > 0:\n",
    "        axes[0, 0].plot(train_acc_plot['total_agg'], train_acc_plot['avg_train_acc'], \n",
    "                       'r--', linewidth=2, label='Train')\n",
    "    axes[0, 0].set_xlabel('Round', fontsize=11)\n",
    "    axes[0, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[0, 0].set_title('Accuracy Trajectory', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].legend(fontsize=10)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # Loss trajectory\n",
    "    axes[0, 1].plot(df['total_agg'], df['test_loss'], 'b-', linewidth=2, label='Test')\n",
    "    train_loss_plot = df[df['avg_train_loss'] > 0]\n",
    "    if len(train_loss_plot) > 0:\n",
    "        axes[0, 1].plot(train_loss_plot['total_agg'], train_loss_plot['avg_train_loss'], \n",
    "                       'r--', linewidth=2, label='Train')\n",
    "    axes[0, 1].set_xlabel('Round', fontsize=11)\n",
    "    axes[0, 1].set_ylabel('Loss', fontsize=11)\n",
    "    axes[0, 1].set_title('Loss Trajectory', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=10)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy vs time\n",
    "    axes[1, 0].plot(df['time_min'], df['test_acc'], 'b-', linewidth=2, label='Test')\n",
    "    if len(train_acc_plot) > 0:\n",
    "        train_time_acc = df[df['avg_train_acc'] > 0]['time_min']\n",
    "        axes[1, 0].plot(train_time_acc, train_acc_plot['avg_train_acc'], \n",
    "                        'r--', linewidth=2, label='Train')\n",
    "    axes[1, 0].set_xlabel('Time (minutes)', fontsize=11)\n",
    "    axes[1, 0].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[1, 0].set_title('Accuracy vs Wall Clock Time', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].legend(fontsize=10)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].set_ylim([0, 1.0])\n",
    "    \n",
    "    # Round progression over time\n",
    "    axes[1, 1].plot(df['time_min'], df['total_agg'], 'g-', linewidth=2, marker='o', markersize=3)\n",
    "    axes[1, 1].set_xlabel('Time (minutes)', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Total Rounds', fontsize=11)\n",
    "    axes[1, 1].set_title('Round Progression Over Time', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add rate annotation\n",
    "    if len(df) > 1:\n",
    "        total_time = df['time_min'].iloc[-1]\n",
    "        total_rounds = df['total_agg'].iloc[-1]\n",
    "        rate = total_rounds / total_time if total_time > 0 else 0\n",
    "        axes[1, 1].text(0.05, 0.95, f'Rate: {rate:.2f} rounds/min', \n",
    "                       transform=axes[1, 1].transAxes, fontsize=10,\n",
    "                       verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(run_dir / \"6_training_summary.png\", dpi=150, bbox_inches='tight')\n",
    "    print(f\"âœ… Plot 6 saved: 6_training_summary.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # ========== Print Summary Statistics ==========\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Rounds: {df['total_agg'].max()}\")\n",
    "    print(f\"Total Time: {df['time_min'].iloc[-1]:.2f} minutes ({df['time'].iloc[-1]:.2f} seconds)\")\n",
    "    print(f\"Final Test Accuracy: {df['test_acc'].iloc[-1]:.4f}\")\n",
    "    print(f\"Best Test Accuracy: {df['test_acc'].max():.4f} (Round {df.loc[df['test_acc'].idxmax(), 'total_agg']})\")\n",
    "    if len(df) > 1:\n",
    "        print(f\"Average Round Time: {(df['time'].iloc[-1] / df['total_agg'].iloc[-1]):.2f} seconds\")\n",
    "        print(f\"Rounds per Minute: {(df['total_agg'].iloc[-1] / df['time_min'].iloc[-1]):.2f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    print(\"Results file not found yet.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
